## 🔖 Articles
	- Top Python Resources for Beginners | HackerNoon](https://omnivore.app/me/top-python-resources-for-beginners-hacker-noon-18bc48d1b49)
	  site:: [hackernoon.com](https://hackernoon.com/top-python-resources-for-beginners-yi6h3yh0)
	  author:: Cyril Michino
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 25th, 2020]]
		- ### Content
			- \#\# Too Long; Didn't Read
			  
			  We have compiled a bull-pack of recommended resources for anyone looking to learn Python Programming. We have tailored these resources to be suitable for Python developers of all levels. These resources will be most-applicable to those still early on the learning curve or already familiar with the basics but not really advanced. The Learning Pathway is what is your path from beginner to advanced and what career focus areas are available with Python. Here are some of the common pathways for Python Developers and some of their corresponding libraries/frameworks.
			  
			  ---
			  
			  \#\#\# People Mentioned
			  
			  [![Mention Thumbnail](https://proxy-prod.omnivore-image-cache.app/0x0,sGT2Fc1SJ4haMu3PAWM7Rk7QfjHj6sIugj-MMUyD3_Vs/https://hackernoon.imgix.net/avatars/robot-a3.png?auto=format&fit=max&w=256)](https://hackernoon.com/u/edwardclark)
			  
			  \#\#\# Companies Mentioned
			  
			  [![Mention Thumbnail](https://proxy-prod.omnivore-image-cache.app/0x0,sAazRe7xcrzZDtijAWMEzsv07UbQtZnYzudxg-KRwyuE/http://logo.bigpicture.io/logo/amazon.com)](https://hackernoon.com/company/amazon)
			  
			  Amazon
			  
			  [![Mention Thumbnail](https://proxy-prod.omnivore-image-cache.app/0x0,slZcJBHct3i0I2QlDGCC0JRuw7EpHz9cAzP2oMfqPQ2U/http://logo.bigpicture.io/logo/google.com)](https://hackernoon.com/company/google)
			  
			  Google
			  
			  ![featured image - Top Python Resources for Beginners](https://proxy-prod.omnivore-image-cache.app/0x0,sEzX6UMcI4ynsubtTjuMAzt9hv70iWTk_iwhFbWeupzM/https://images.unsplash.com/photo-1555952494-efd681c7e3f9?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjEwMDk2Mn0)
			  
			  ![Cyril Michino HackerNoon profile picture](https://proxy-prod.omnivore-image-cache.app/0x0,so7m4dFbIAA85oe2NS6kPzFMM-tDz00warG1h75jCCiQ/https://hackernoon.imgix.net/images/avatars/mH4IKEvLjgTDcYImgSg0CeFWy7p2.jpg?auto=format&fit=max&w=3840)
			  
			  ---
			  
			  \#\#\# [@cyrilmichino](https://hackernoon.com/u/cyrilmichino)
			  
			  **Cyril Michino**
			  
			  ---
			  
			  Founder at Chaptr Global | Pioneering income-share agreem...
			  
			  ---
			  
			  Receive Stories from @cyrilmichino
			  
			  New to Python? Or are you already a seasoned developer looking to boost and advance your Python knowledge? We have compiled a bull-pack of recommended resources for anyone looking to learn Python Programming. We have tailored these resources to be suitable for Python developers of all levels but the resources listed will be most-applicable to those still early on the learning curve or already familiar with the basics but not really advanced.
			  
			  ![image](https://proxy-prod.omnivore-image-cache.app/0x0,scXnYeiCpZ6Jq9qMd7A8Nd1JMS_KdQa_pJF2LK845pq4/https://hackernoon.imgix.net/photos/mH4IKEvLjgTDcYImgSg0CeFWy7p2-gnr3yzv?w=1200&q=75&auto=format)
			  
			  \#\# The Learning Pathway
			  
			  First, let us explore the ideal Python Pathway — what is your path from beginner to advanced and what career focus areas are available with Python.
			  
			  1. **Learn Python Fundamentals:** As with any language, first, you need to grasp the basics, from Python syntax to the Python PEP-8 conventions and know how to use the language. Here you will explore variables, expressions, operators, conditionals, loops, and Python data structures. After learning Python fundamentals, you should be set to start practicing with mini scripting projects.
			  2. **Become Advanced with Python:** Here you will now seek to add utility to your Python knowledge by learning how Python interacts with Web Data (Web Scraping ability is one of the biggest outcomes here), how to use Python for Object-Oriented Programming, and finally how to integrate Python with Databases (SQL Databases, NoSQL Databases, and APIs)
			  3. **Focus on a Career Pathway:** Now, you are already advanced in the language and it is time to explore and focus on a career pathway. This will involve delving into Python libraries and frameworks that are specific to a particular career path. Here are some of the common pathways for Python Developers and some of their corresponding libraries/frameworks:
			  * **Web Development** — Django and Flask Frameworks are the most popular for web development tasks and projects with Python
			  * **Data Science** — Learn how to conduct Data Sourcing, Data Analysis and Machine Learning (ML) with Python. Libraries to learn include: Numpy, Matplotlib, Pandas, SciPy and SciKit Learn for ML among others.
			  * **Machine Learning and Artificial Intelligence** — A popular career pathway as Artificial Intelligence keep gaining ground in our day-to-day lives. Some top libraries to learn are SciKit Learn, Tensorflow, Keras, Natural Language Toolkit (NLTK), PyTorch, Theano, etc.
			  * **Graphical User Interface (GUI) Programming** — With GUI libraries such as Tkinter, wxPython, PyQT, and Kivy you can create graphical applications with Python.
			  * **Game Development** — While not so many developers end up in Game Development, it is probably one of the most interesting pathways to explore. To build games, PyGame is the library to learn.
			  
			  Additionally, it is recommended that you explore **Version Control** ([Github](https://github.com/?ref=hackernoon.com) is a perfect choice), **Python Data Structure Algorithms** (efficiency matters always) and **DevOps** (practices combining software development with IT operations in companies).
			  
			  There are also tons of [pre-installed Python Libraries](https://docs.python.org/3/library/?ref=hackernoon.com) and [community-developed Packages](https://pypi.org/?ref=hackernoon.com), not necessarily pegged to a career pathway, worth learning to fulfil certain tasks or just for fun (cause why not?). My personal favorite — Selenium for browser automation.
			  
			  **_Any career pathway or recommended library that you think we left out? Drop a comment at the end of this article!_**
			  
			  \#\# Step 1: LEARN
			  
			  In this section, I have detailed all resources that you need to learn Python. You could use these resources to teach yourself Python from scratch or supplement any Python Course:
			  
			  \#\# Integrated Development Environment (IDEs)
			  
			  Your first step before even learning the language should be choosing an IDE and setting it up. This is where you’ll be writing and executing your Python code. Here are the top IDEs to consider:
			  
			  1. [**Visual Studio Code**](https://code.visualstudio.com/?ref=hackernoon.com) — The most popular IDE not only for Python but for numerous programming languages. It goes without saying, VS Code is what I use from day-to-day
			  2. [**Jupyter Notebooks**](https://jupyter.org/?ref=hackernoon.com) — If your target is taking up a Data Science pathway, then look no further than Jupyter. This has become an industry-standard tool for Data Scientists
			  3. Other top options worth considering are: [PyCharm](https://www.jetbrains.com/pycharm/?ref=hackernoon.com), [Atom](https://atom.io/?ref=hackernoon.com), and [Spyder](https://www.spyder-ide.org/?ref=hackernoon.com)
			  
			  You can download and install your chosen IDE independently, however, I recommend using [Anaconda](https://www.anaconda.com/distribution/?ref=hackernoon.com) which comes packaged with VSCode, Jupyter, and Spyder, even Rstudio for Data Science targets.
			  
			  \#\# Test Runner
			  
			  As a beginner, you probably want to delve into learning the basics before getting into testing. Nevertheless, test running is quite important in making sure all elements of your program are working as designed. In Python, we use the following libraries/frameworks to conduct tests:
			  
			  1. [**Unit Test**](https://docs.python.org/3/library/unittest.html?ref=hackernoon.com) — Built into Python, this is extremely popular across the Python community
			  2. [**Nose**](https://pypi.org/project/nose/?ref=hackernoon.com) — This framework is wrapped around unittest to add more functionality for tests
			  3. [**Pytest**](https://docs.pytest.org/en/latest/?ref=hackernoon.com) — A lot of Pythonistas are now using py.test over unittest given its simple syntax
			  
			  \#\# Debugger
			  
			  As you learn Python and start practicing, bugs will be quite common and identifying them fast is an issue for many. Not any more! Today, all you have to do is copy-paste your code on [Python Tutor](http://www.pythontutor.com/?ref=hackernoon.com) to visualize its execution and help you pinpoint the bug or issue. Some alternative debuggers are [The Python Debugger/PDB](https://docs.python.org/3/library/pdb.html?ref=hackernoon.com) (A Python Module) and [Online GDB](https://www.onlinegdb.com/online%5Fpython%5Fdebugger?ref=hackernoon.com).
			  
			  \#\# Recommended Python Learning Guides
			  
			  **Learn Python from well-structured websites covering at least all fundamentals:**
			  
			  1. [Official Python Documentation](https://docs.python.org/?ref=hackernoon.com) by Python.org — The Ultimate Python Resource
			  2. [The W3 Schools Python Guide](https://www.w3schools.com/python/?ref=hackernoon.com) — My Personal Favorite for Beginners
			  3. [The HitchHiker’s Python Guide](https://docs.python-guide.org/?ref=hackernoon.com) — Best Practice Handbook
			  
			  **Learn by reading detailed Python Books or PDF Publications:**
			  
			  1. Python for Everybody by Charles R. Severance \[[Download Free PDF](http://do1.dr-chuck.com/pythonlearn/EN%5Fus/pythonlearn.pdf?ref=hackernoon.com)\] \[[Access as HTML Site](https://www.py4e.com/html3/?ref=hackernoon.com)\]
			  2. Think Python — How to Think Like a Computer Scientist _by Allen B. Downey_ \[[Buy on Amazon](https://www.amazon.com/gp/product/1491939362/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1491939362&linkCode=as2&tag=chaptrglobal-20&linkId=33de58ed61bad4eb411f2687f34ffbd1&ref=hackernoon.com)\]
			  3. Learn Python the Hard Way _by Zed Shaw_ \[[Buy on Amazon](https://www.amazon.com/gp/product/0134692888/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0134692888&linkCode=as2&tag=chaptrglobal-20&linkId=5884d701799893cfd59450966299063a&ref=hackernoon.com)\]
			  4. Python Crash Course — A Hands-On Project-based Introduction to Programming _by Eric Matthes_ \[[Buy on Amazon](https://www.amazon.com/gp/product/1593279280/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1593279280&linkCode=as2&tag=chaptrglobal-20&linkId=0f040aee98d0ac33be98f0b5c44d134f&ref=hackernoon.com)\]
			  5. Python Cookbook _by David Beazley and Brian K.Jones_ \[[Buy on Amazon](https://www.amazon.com/gp/product/1449340377/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1449340377&linkCode=as2&tag=chaptrglobal-20&linkId=b6a8a98baf4f3b31c54a0095b833dd0f&ref=hackernoon.com)\]
			  6. Python Tricks — A Buffet of Awesome Python Features _by Dan Bader_ \[[Buy on Amazon](https://www.amazon.com/gp/product/1775093301/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1775093301&linkCode=as2&tag=chaptrglobal-20&linkId=1d3c9b02ed8b5933fb2e3e0488ff3cf8&ref=hackernoon.com)\]
			  7. Effective Python — 90 Specific Ways to Write Better Python _by Brett Slatkin_ \[[Buy on Amazon](https://www.amazon.com/gp/product/0134853989/ref=as%5Fli%5Ftl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0134853989&linkCode=as2&tag=chaptrglobal-20&linkId=0608c13c1bbcec65fb19b5860078a71b&ref=hackernoon.com)\]
			  
			  Finally, to learn the Python Code Style Guide — **PEP-8 Conventions** — and start writing Pythonic Code. Visit [the official style guide documentation at Python.org](https://www.python.org/dev/peps/pep-0008/?ref=hackernoon.com)
			  
			  \#\# Gauge Your Python Knowledge
			  
			  Already versed with Python and would like to test your knowledge? We recommend using common interview questions to gauge your knowledge. After all, these questions give you an overview of what the industry needs you to know:
			  
			  1. [Guru99](https://www.guru99.com/python-interview-questions-answers.html?ref=hackernoon.com) — Top 40 Python Interview Questions & Answers
			  2. [EduReka](https://www.edureka.co/blog/interview-questions/python-interview-questions/?ref=hackernoon.com) — Top 100 Python Interview Questions You Must Prepare In 2020
			  3. [DataFlair](https://data-flair.training/blogs/top-python-interview-questions-answer/?ref=hackernoon.com) — 150+ Python Interview Questions and Answers for Freshers
			  
			  \#\# Step 2: PRACTICE
			  
			  It goes without saying — **Do Not Learn Coding Without Practice!** But, the big problem is actually finding projects suited for your level that would best test and let you put your skill into practice. We’ve therefore split these projects into beginner, intermediate, and advanced levels:
			  
			  Most of these projects were inspired by [Data Flair — Python Project Ideas for 2020](https://data-flair.training/blogs/python-project-ideas/?ref=hackernoon.com). To understand the project deliverable, you can simply make a Google Search or **visit the Data Flair link to get a short description**.
			  
			  The starred projects require a Graphical User Interface to be fulfilled while the projects with “Rec.” are those that I recommend trying out for ultimate practice and fun engagement. I have also sought to attempt all these projects and make them available on the [Chaptr Github Account](https://github.com/chaptr-global/?ref=hackernoon.com). _3 project repositories will be added each week for the next 10 weeks. Contributions are open!_
			  
			  **_Have you already done any of these projects? Or, do you have other project ideas worth mentioning? Drop a comment at the end of the article with your Github Repo link or Project Idea._**
			  
			  \#\# Step 3: ENGAGE
			  
			  Alas! You are no random beginner anymore, you are pretty confident with your skills, and you have probably started populating your Github portfolio with some inspiring Python projects. You have also started mastering additional libraries to shape your career pathway. It is time to continue your Python engagement and keep developing your competencies. Here are some ways to achieve this:
			  
			  \#\# Get Certified — Python Certifications
			  
			  I am not usually a fan of certifications, but in most countries, employers are still blinded by these traditional standards of proving competence. However, on a positive note, credible certifications do test your competence well and best help you know whether you are set for a job in the industry. Below are the two organizations best suited for Python competence certification:
			  
			  [**The Python Institute**](https://pythoninstitute.org/certification/?ref=hackernoon.com) (by the Open Education Development Group) offers 4 certification exams at 3 competency levels:
			  
			  * **Entry** — Certified Entry-Level Python Programmer Certification
			  * **Associate** — Certified Associate in Python Programming Certification
			  * **Professional** — Certified Professional in Python Programming 1 Certification and Certified Professional in Python Programming 2 Certification
			  
			  [**Microsoft Inc.**](https://www.microsoft.com/en-us/learning/browse-all-certifications.aspx?ref=hackernoon.com) (The company needs no introduction) offers mostly online exams for certification across all continents. You could explore the following certifications:
			  
			  * [MTA](https://www.microsoft.com/en-us/learning/mta-summary-certification.aspx?ref=hackernoon.com) — Microsoft Technology Associate
			  * [MCSA](https://www.microsoft.com/en-us/learning/mcsa-web-applications-certification.aspx?ref=hackernoon.com) — Microsoft Certified Solutions Associate
			  * [MCSD](https://www.microsoft.com/en-us/learning/mcsd-app-builder-certification.aspx?ref=hackernoon.com) — Microsoft Certified Solutions Developer
			  
			  Beware that Microsoft is undertaking [a major “certpocalypse”](https://www.microsoft.com/en-us/learning/community-blog-post.aspx?BlogId=8&Id=375282&ref=hackernoon.com) (Certificate Apocalypse), and some of these certification exams will no longer be on offer next year, but once certified, whether or not a program is retired, the certification will be valid and active for 2 years as usual.
			  
			  \#\# Keep Learning — Top Python Blogs
			  
			  One of the best ways to keep up your engagement with Python will be through Python Blogs and Vlogs. Below are blogs you should definitely look at. They are all pretty consistent on posting and have tons of Python content at everyone’s disposal:
			  
			  1. Love Python — [http://love-python.blogspot.com/](https://love-python.blogspot.com/?ref=hackernoon.com)
			  2. Planet Python — [https://planetpython.org/](https://planetpython.org/?ref=hackernoon.com)
			  3. The Mouse Vs. The Python — [https://www.blog.pythonlibrary.org/](https://www.blog.pythonlibrary.org/?ref=hackernoon.com)
			  4. Real Python — [https://www.fullstackpython.com/blog.html](https://www.fullstackpython.com/blog.html?ref=hackernoon.com)
			  5. Full Stack Python — [https://realpython.com/](https://realpython.com/?ref=hackernoon.com)
			  
			  \#\# Stay Connected — Popular Python Communities
			  
			  Finally, it time for you to engage further with the global Python community and even start contributing. Some of the popular communities for Pythonistas are:
			  
			  1. [**r/Python**](https://www.reddit.com/r/Python/?ref=hackernoon.com) — the official Reddit for Python News, Content, and Resources
			  2. [**Python Weekly**](https://www.pythonweekly.com/?ref=hackernoon.com)\- Free weekly Python Newsletter to subscribe to
			  3. [**PySlackers**](https://pyslackers.com/web?ref=hackernoon.com) — An open Slack Channel for Python Enthusiasts
			  
			  \#\# Conclusion and Next Steps
			  
			  Now, a Python Developer you are! So, what career pathway do you want to take? Data Science, Machine Learning & AI, Game Development, Web Development, etc. Any libraries from the Python Packages that interest you? You can now easily chart your path based on your interests and career goals.
			  
			  **_Do you have ideas of interesting Python resources that have been left out? Have you used and gained from any of the resources above? Drop a comment below and let others learn what worked for you or what other resources could be of help._**
			  
			  I will also be releasing detailed resources on the Data Science Career Pathway as well as other popular Python career pathways. 
			  
			  **Subscribe or follow this publication to get notified.** 
			  
			  To learn about Chaptr Global and our emerging technology programs, visit [our website homepage.](https://chaptrglobal.com/?ref=hackernoon.com)
			  
			  _Previously published at https://chaptrglobal.com/python-resources/_
			  
			  [![NEAR Protocol](https://proxy-prod.omnivore-image-cache.app/0x0,sWdqXARD_UWGfcG17O6Wffz3HQAq1spyyJyX5ZKxz-IQ/https://hackernoon.imgix.net/images/img-cg23atb.jpeg?auto=format&fit=max&w=3840) ](https://nearcon.app/?utm%5Fsource=hackernoon&utm%5Fmedium=category-page&utm%5Fcampaign=nearcon)
			  
			  \#\#\#\# Comments
			  
			  \#\#\#\# TOPICS
			  
			  \#\#\#\# RELATED STORIES
			  
			  \#\#\#\# Mentioned in this story
			  
			  \#\#\#\#\# companies
			  
			  \#\#\#\#\# People
			  
			  [![Mention Thumbnail](https://proxy-prod.omnivore-image-cache.app/0x0,sIslY-fGjyEt2s6flbsAksr4Xs9aL2NGW0DfVRJ2H80I/https://hackernoon.imgix.net/avatars/robot-a3.png?auto=format&fit=max&w=128)](https://hackernoon.com/u/edwardclark)
	- Every Complex DataFrame Manipulation, Explained & Visualized Intuitively - KDnuggets](https://omnivore.app/me/every-complex-data-frame-manipulation-explained-visualized-intui-18bc48d1a38)
	  collapsed:: true
	  site:: [KDnuggets](https://www.kdnuggets.com/2020/11/dataframe-manipulation-explained-visualized.html)
	  labels:: [[pandas]]
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- [![Advertisement](https://proxy-prod.omnivore-image-cache.app/728x90,swaZXoROTSSFP0D_H_OM4gKk1oY9IhY0GXbhzdEAUMG8/https://s0.2mdn.net/simgad/13255037943377488672)](https://adclick.g.doubleclick.net/pcs/click?xai=AKAOjss5bAEDpDPoAtiYgV9%5FBrPhMwGmrCLSuYj04JhW1-zlMOwkaNa%5FygeavpS2PKvIT7zvMSl5Y6yj1E1WdXPQ0Hq2YEoXRyyhkhlgUh20N6vPbxS6sp9YA9WQdRftN5pOyN%5FT%5Fc58r5p1wC571bgbFXQ&sai=AMfl-YR9bVJ3b0RKU-5uQ7VOGG7niBUw0H5jvmPGodXA%5F%5Fiu4q-2I-zMhgaYS6OQw5%5FZjvKH5YCuPhhJOl-o0tQ&sig=Cg0ArKJSzDFnAR4E5jkC&cry=1&fbs%5Faeid=[gw%5Ffbsaeid]&urlfix=1&adurl=https://www.sas.com/gms/redirect.jsp%3Fdetail%3DPLN4206%5F1930035836%26dclid%3D%25edclid!)
			  
			  Most Data Scientists might hail the power of Pandas for data preparation, but many may not be capable of leveraging all that power. Manipulating data frames can quickly become a complex task, so eight of these techniques within Pandas are presented with an explanation, visualization, code, and tricks to remember how to do it.
			  
			  ---
			  
			  
			  **By [Andre Ye](https://www.linkedin.com/in/andre-ye-501746150/), Cofounder at Critiq, Editor & Top Writer at Medium**.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sTRh39yzNhoTxvKoszSUm9pLUiQ-YZkJzzLgqw63VBOI/https://miro.medium.com/max/2961/1*1p1WoRK7p1TIJbLT8zZw3A.png)
			  
			  Pandas offers a wide range of DataFrame manipulations, but many of them are complex and may not seem approachable. This article presents 8 essential DataFrame manipulation methods that cover almost all of the manipulation functions a data scientist would need to know. Each method will include an explanation, visualization, code, and tricks to remember it.
			  
			  \#\#\# Pivot
			  
			  Pivoting a table creates a new ‘pivoted table’ that projects existing columns in the data as elements of a new table, being the index, column, and the values. The columns in the initial DataFrame that will become the index and the columns are displayed as unique values, and combinations of these two columns will be displayed as the value. This means that pivots cannot handle duplicate values.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s91zYdEItJUUlEt2o0mHNdRbHDQcRHGUb1_AvhVRONwg/https://miro.medium.com/max/875/1*FGqEaHi4rm2RwFqeHsxOyw.png)
			  
			  The code to pivot a DataFrame named _df_ is as follows:
			  
			  df.pivot(index='foo', columns='bar', values='baz')
			  
			  _To memorize_: A pivot is — outside the realm of data manipulation — a turn around some sort of object. In sports, one can ‘pivot’ around their foot to spin: pivots in pandas are similar. The state of the original DataFrame is pivoted around central elements of a DataFrame into a new one. Some elements very literally pivot in that they are rotated or transformed (like column ‘_bar_’).
			  
			  \#\#\# Melt
			  
			  Melting can be thought of as an ‘unpivot,’ in that it converts matrix-based data (has two dimensions) into list-based data (columns represent values and rows indicate unique data points), whereas pivots do the opposite. Consider a two-dimensional matrix with one dimension ‘_B_’ and ‘_C_’ (column names), with the other dimension ‘_a’_, ‘_b_’, and ‘_c_’ (row indices).
			  
			  We select an ID, one of the dimensions, and a column/columns to contain values. The column(s) that contain values are transformed into two columns: one for the variable (the name of the value column) and another for the value (the number contained in it).
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s7Reh6EyKXd6WwFbciLJeLdmWB0XONI0PlEKB85Lc9dM/https://miro.medium.com/max/875/1*EvpoxUh_ycXCCeryflf_ZA.png)
			  
			  The result is every combination of the ID column’s values (_a, b, c_) and the value columns (_B, C_), with its corresponding value, organized in list format.
			  
			  The melt operation can be performed like such on DataFrame _df_:
			  
			  df.melt(id_vars=['A'], value_vars=['B','C'])
			  
			  _To memorize_: Melting something like a candle is to turn a solidified and composite object into several much smaller, individual elements (wax droplets). Melting a two-dimensional DataFrame unpacks its solidified structure and records its pieces as individual entries in a list.
			  
			  \#\#\# Explode
			  
			  Exploding is a helpful method to get rid of lists in the data. When a column is exploded, all lists inside of it are listed as new rows under the same index (to prevent this, simply call _.reset\_index()_ afterward). Non-list items like strings or numbers are not affected, and empty lists are NaN values (you can cleanse these using _.dropna()_).
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s9EGV2IuFjTfSJztlNXj0e-sjhV2QmPNZjdsw3iVUFso/https://miro.medium.com/max/875/1*WIgtU7yIwVhvQgugmCdGSA.png)
			  
			  Exploding a column ‘_A_’ in DataFrame _df_ is very simple:
			  
			  _To remember_: Exploding something releases all its internal contents — exploding a list separates its elements.
			  
			  \#\#\# Stack
			  
			  Stacking takes a DataFrame of any size and ‘stacks’ the columns as subindices of existing indices. Hence, the resulting DataFrame has only one column and two levels of indices.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sF9t9UOLbwKCNOPNKEkyxLnFuqZ2lkro7861IxLib_B0/https://miro.medium.com/max/875/1*2xcdye2CPT_jRYFEMGqkdw.png)
			  
			  Stacking a table named _df_ is as simple as _df.stack()_.
			  
			  In order to access the value of, say, the dog’s height, simply call an index-based retrieval twice, like _df.loc\[‘dog’\].loc\[‘height’\]_.
			  
			  _To remember_: Visually, _stack_ takes the two-dimensionality of a table and stacks the columns into multi-level indices.
			  
			  \#\#\# Unstack
			  
			  Unstacking takes a multi-index DataFrame and unstacks it, converting the indices in a specified level into columns of a new DataFrame with its corresponding values. Calling a stack followed by an unstack on a table will not change it (excusing the existence of a ‘_0_’).
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sinsVFhGCiw1tKSNtlt2B1ExS0kTT7zp1dsGCfYSeQRc/https://miro.medium.com/max/875/1*gj_4TxXqERXubC867Sn-qQ.png)
			  
			  A parameter in unstacking is its level. In list indexing, an index of -1 will return the last element; this is the same with levels. A level of -1 indicates that the last index level (the one rightmost) will be unstacked. As a further example, when the level is set to 0 (the first index level), values in it become columns and the following index level (the second) becomes the transformed DataFrame’s index.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sRW8kv1ONcOzDwOm1c9a6uSuvOhx443AmDpPbqXmQNzw/https://miro.medium.com/max/875/1*OKm3r6rzcjEKquznIf03og.png)
			  
			  Unstacking can be performed the same as stacking, but with the level parameter: _df.unstack(level=-1)_.
			  
			  _To remember_: Unstack means “to undo a stack.”
			  
			  \#\#\# Merge
			  
			  To merge two DataFrames is to combine them column-wise (horizontally) among a shared ‘key’. This key allows for the tables to be combined, even if they are not ordered similarly. The finished merged DataFrame will add suffixes _\_x_ and _\_y_ to value columns by default.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sqkLQGX2dKepx5Fb1g1cfazN6IgcT3S67UwB7sPzmQuw/https://miro.medium.com/max/1250/1*RTzkUWUQTaAU348YQLJJ_Q.png)
			  
			  In order to merge two DataFrames _df1_ and _df2_ (where _df1_ contains the _leftkey_ and _df2_ contains the _rightkey_), call:
			  
			  df1.merge(df2, left_on='leftkey', right_on='rightkey')
			  
			  Merges are not functions of pandas but are attached to a DataFrame. It is always assumed that the DataFrame in which the merge is being attached to is the ‘left table’, and the DataFrame called as a parameter in the function is the ‘right table’, with corresponding keys.
			  
			  The merge function performs by default what is called an inner join: if each of the DataFrames has a key not listed in the other’s, it is not included in the merged DataFrame. On the other hand, if a key is listed twice in the same DataFrame, every combination of values for the same keys is listed in the merged table. For example, if _df1_ has 3 values for key _foo_ and _df2_ had 2 values for the same key, there would be 6 entries with _leftkey=foo_ and _rightkey=foo_ in the final DataFrame.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sXtYZ9Af5qEIS7L7MosbRWdrSkUKFzRtWoIyjOLUvCik/https://miro.medium.com/max/1250/1*efCmFOV08ctxUCO3dIleoQ.png)
			  
			  _To remember_: You merge DataFrames like you merge lanes when driving — horizontally. Imagine each of the columns as one lane on the highway; in order to merge, they must combine horizontally.
			  
			  \#\#\# Join
			  
			  Joins are generally preferred over merge because it has a cleaner syntax and a wider range of possibilities in joining two DataFrames horizontally. The syntax of a join is as follows:
			  
			  df1.join(other=df2, on='common_key', how='join_method')
			  
			  When using joins, the common key column (analogous to _right\_on_ and _left\_on_ in merge) must be named the same name. The how parameter is a string referring to one of four methods _join_ can combine two DataFrames:
			  
			  * ‘_left_’: Include all elements of _df1_, accompanied with elements of _df2_ only if their key is a key of _df1_. Otherwise, the missing portion of the merged DataFrame for _df2_ will be marked as NaN.
			  * ‘_right_’: ‘_left_’, but on the other DataFrame. Include all elements of _df2_, accompanied with elements of _df1_ only if their key is a key of _df2_.
			  * ‘_outer_’: Include all elements from both DataFrames, even if a key is not present in the other’s — missing elements are marked as NaN.
			  * ‘_inner_’: Include only elements whose keys are present in both DataFrame keys (intersection). Default for merge.
			  
			  _To remember_: If you’ve worked with SQL, the word ‘join’ should immediately be associated with column-wise addition. If not, ‘join’ and ‘merge’ have very similar meanings definition-wise.
			  
			  \#\#\# Concat
			  
			  Whereas merges and joins work horizontally, concatenations, or concats for short, attach DataFrames row-wise (vertically). Consider, for example, two DataFrames _df1_ and _df2_ with the same column names, concatenated using _pandas.concat(\[df1, df2\])_:
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sqte-8ZDoTA3iQmkuBPjoHPaWJoM8X6z10KPb1is_T94/https://miro.medium.com/max/875/1*sc3wG3m8RgU3EAIkB42j7w.png)
			  
			  Although you can use concat for column-wise joining by turning the axis parameter to _1_, it would be easier just to use join.
			  
			  Note that concat is a pandas function and not one of a DataFrame. Hence, it takes in a list of DataFrames to be concatenated.
			  
			  If a DataFrame has a column not included in the other, by default, it will be included, with missing values listed as NaN. To prevent this, add an additional parameter, _join=’inner’_, which will only concatenate columns both DataFrames have in common.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s51liedL9zIRHD6ZPtvEpIexwSzQhnwzQfp5GRDsUb-Q/https://miro.medium.com/max/875/1*MOE_LKKAzNGwybP5jAJktA.png)
			  
			  _To remember_: In lists and strings, additional items can be concatenated. Concatenation is the appendage of additional elements to an existing body, not the adding of new information (as is column-wise joining). Since each index/row is an individual item, concatenation adds additional items to a DataFrame, which can be thought of as a list of rows.
			  
			  Append is another method to combine two DataFrames, but it performs the same functionality as concat and is less efficient and versatile.
			  
			  Sometimes built-in functions aren’t enough.
			  
			  Although these functions cover a wide range of what you may need to manipulate your data for, sometimes the data manipulation required is too complex for one or even a series of functions to perform. Explore complex data manipulation methods like parser functions, iterative projection, efficient parsing, and more [here](https://medium.com/analytics-vidhya/tips-tricks-techniques-to-take-your-data-wrangling-skills-to-the-next-level-c912c23b20cb).
			  
			  [Original](https://medium.com/analytics-vidhya/every-dataframe-manipulation-explained-visualized-intuitively-dbeea7a5529e). Reposted with permission.
			  
			  **Related:**
			  
			  * [Exploratory Data Analysis on Steroids](https://www.kdnuggets.com/2020/07/exploratory-data-analysis-steroids.html)
			  * [How to Prepare Your Data](https://www.kdnuggets.com/2020/06/how-prepare-your-data.html)
			  * [Introduction to Pandas for Data Science](https://www.kdnuggets.com/2020/06/introduction-pandas-data-science.html)
	- Python programming language: Now you can take NSA's free course for beginners | ZDNET](https://omnivore.app/me/python-programming-language-now-you-can-take-nsa-s-free-course-f-18bc48d1955)
	  collapsed:: true
	  site:: [ZDNET](https://www.zdnet.com/article/python-programming-language-now-you-can-take-nsas-free-course-for-beginners/)
	  author:: Liam Tung
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Feb 11th, 2020]]
		- ### Content
			- ![programming-languages-python-is-now-more-5dca916e415e540001491fc7-1-nov-12-2019-12-52-39-poster.jpg](https://proxy-prod.omnivore-image-cache.app/1449x0,sImCJU0TAHqB4hYreeYMsys86HoOEzE8h6-DeGqNfZho/https://www.zdnet.com/a/img/resize/7d3fefa6354e4b40058080c073086a10b26fa5d4/2019/11/12/1bc0fc87-2bfe-4ea0-b112-0b698f028c8e/programming-languages-python-is-now-more-5dca916e415e540001491fc7-1-nov-12-2019-12-52-39-poster.jpg?auto=webp&fit=crop&frame=1&height=814.5&width=1449)
			  
			  Developers already have numerous options from the [likes of Microsoft](https://www.zdnet.com/article/microsoft-we-want-you-to-learn-python-programming-language-for-free/) [and Google](https://www.zdnet.com/article/google-reveals-new-python-programming-language-course-scholarships-for-2500/) for learning how to code in the popular Python programming language. But now budding Python developers can read up on the National Security Agency's own Python training materials. 
			  
			  Software engineer Christopher Swenson filed a Freedom of Information Act (FOIA) request with the NSA for access to its Python training materials and received a lightly redacted 400-page printout of the agency's COMP 3321 Python training course.
			  
			  \#\#\# Developer
			  
			  Swenson [has since scanned the documents](https://twitter.com/chris%5Fswenson/status/1225836060938125313), ran OCR on the text to make it searchable, and hosted it on Digital Oceans Spaces. The material has also been [uploaded to the Internet Archive](https://archive.org/details/comp3321/page/n89/mode/2up/search/device). 
			  
			  There doesn't look to be anything controversial in the documents, which contains course material sessions that would take between 45 and 90 minutes to complete in a class setting. The COMP 3321 course can be completed over a "full-time, two-week block" with 10 modules covered per week. 
			  
			  **MUST READ:** [**Programming languages: Go, Scala and Ruby most wanted, Python and JavaScript most used**](https://www.zdnet.com/article/programming-languages-go-scala-and-ruby-most-wanted-python-and-javascript-most-used/)
			  
			  The NSA also suggests that the material could be taught at a more "leisurely pace, for instance during a weekly brown bag lunch" over several months or even over a three-day workshop. 
			  
			  The course offers a quick introduction to Python, its creator Guido van Rossum, and what the language is suitable for, such as automating tasks, creating a web application or doing advanced mathematical research. It also explains why Python has become so popular among beginning developers and data scientists. 
			  
			  **SEE:** [**How to build a successful developer career**](https://www.techrepublic.com/resource-library/whitepapers/how-to-build-a-successful-developer-career/?ftag=CMG-01-10aaa1b) **(free PDF)**
			  
			  "If you don't know any programming languages yet, Python is a good place to start. If you already know a different language, it's easy to pick Python on the side. Python isn't entirely free of frustration and confusion, but hopefully you can avoid those parts until long after you get some good use out of Python," writes the NSA. 
			  
			  Students use version 4.4.0 of the Anaconda3 Python distribution and can run Python in the command line or through a Jupyter notebook from the browser. 
			  
			  **MUST READ:** [**Python is eating the world: How one developer's side project became the hottest programming language on the planet**](https://www.techrepublic.com/article/python-is-eating-the-world-how-one-developers-side-project-became-the-hottest-programming-language-on-the-planet/)
			  
			  Swenson told ZDNet that it was "mostly just curiosity" that motivated him to ask the NSA about its Python training material.   
			  He also said the NSA had excluded some course material, but that he'll keep trying to get more from the agency.   
			  "The response was OK. I can't say I'm really disappointed with getting almost 400 pages of material," Swenson said.
			  
			  "There were some course materials they held back in their entirety, though. I'll continue sending more FOIA requests and appeals to find out what more I can get of similar materials."
			  
			  Python developer Kushal Das has [pulled out some interesting details from the material](https://kushaldas.in/posts/python-course-inside-of-nsa-via-a-foia-request.html). He found that the NSA has an internal Python package index, that its GitLab instance is [gitlab.coi.nsa.ic.gov](http://gitlab.coi.nsa.ic.gov/), and that it has a Jupyter gallery that runs over HTTPS. NSA also offers git installation instructions for CentOS, Red Hat Enterprise Linux, Ubuntu, and Windows, but not Debian. 
			  
			  \#\#\# More on Python and programming languages
			  
			  * [**Programming languages: Java developers flock to Kotlin and ditch Oracle JDK for OpenJDK**](https://www.zdnet.com/article/programming-languages-java-developers-flock-to-kotlin-and-ditch-oracle-jdk-for-openjdk/)
			  * [**Looking to hire a '10x developer'? You can try, but it probably won't boost productivity**](https://www.zdnet.com/article/looking-to-hire-a-10x-developer-you-can-try-but-it-probably-wont-boost-productivity/)
			  * **[Programming language popularity: Apple's Objective-C tumbles down the rankings](https://www.zdnet.com/article/programming-language-popularity-apples-objective-c-tumbles-down-the-rankings/)**
			  * **[Programming languages: Go and Python are what developers most want to learn](https://www.zdnet.com/article/go-and-python-are-the-programming-languages-developers-most-want-to-learn/)**
			  * [**Know Python language and up for a 'hardcore' coding test? Get in touch, says Tesla**](https://www.zdnet.com/article/tesla-know-python-language-and-up-for-a-hardcore-coding-test-get-in-touch/)
			  * **[Java or C++, Full stack or Front end: The programming languages and developer jobs that pay you the most](https://www.zdnet.com/article/java-c-or-javascript-full-stack-or-front-end-here-are-the-software-developer-jobs-that-pay-the-most/)**
			  * [**Developers love Rust programming language: Here's why**](https://www.zdnet.com/article/developers-love-rust-programming-language-heres-why/)
			  * **[Google reveals new Python programming language course: Scholarships for 2,500](https://www.zdnet.com/article/google-reveals-new-python-programming-language-course-scholarships-for-2500/)**
			  * **[Microsoft boosts programming language Python's popular VS Code extension](https://www.zdnet.com/article/microsoft-boosts-programming-language-pythons-popular-vs-code-extension/)**
			  * **[Programming language of 2019? Python beaten by trusty old C](https://www.zdnet.com/article/programming-language-of-2019-python-beaten-by-trusty-old-c/)**
			  * [**Programming language Python's popular extension for Visual Studio Code revamped**](https://www.zdnet.com/article/programming-language-pythons-popular-extension-for-visual-studio-code-revamped/)
			  * **[Programming language Python 2.7 code is now frozen: Last release coming in April](https://www.zdnet.com/article/programming-language-python-2-7-code-is-now-frozen-last-release-coming-in-april/)**
			  * **[Netflix: Our Metaflow Python library for faster data science is now open source](https://www.zdnet.com/article/netflix-our-metaflow-python-library-for-faster-data-science-is-now-open-source/)**
			  * **[Microsoft: We're creating a new Rust-like programming language for secure coding](https://www.zdnet.com/article/microsoft-were-creating-a-new-rust-based-programming-language-for-secure-coding/)**
			  * [**Tech jobs: Python programming language and AWS skills demand has exploded**](https://www.zdnet.com/article/tech-jobs-python-programming-language-and-aws-skills-demand-has-exploded/)
			  * [**Python programming language creator retires, saying: 'It's been an amazing ride'**](https://www.zdnet.com/article/python-programming-language-creator-retires-saying-its-been-an-amazing-ride/)
			  * [**Programming languages: How Instagram's taming a multimillion-line Python monster**](https://www.zdnet.com/article/programming-languages-how-instagrams-taming-a-multimillion-line-python-monster/)
			  * **[Salesforce: Why we ditched Python for Google's Go language in Einstein Analytics](https://www.zdnet.com/article/salesforce-why-we-ditched-python-for-googles-go-language-in-einstein-analytics/)**
			  * **[Google: Take our free Kotlin programming language courses to build Android apps](https://www.zdnet.com/article/google-take-our-free-kotlin-programming-language-courses-to-build-android-apps/)**
			  * [**Facebook: Microsoft's Visual Studio Code is now our default development platform**](https://www.zdnet.com/article/facebook-microsofts-visual-studio-code-is-now-our-default-development-platform/)
			  * **[Microsoft: We want you to learn Python programming language for free](https://www.zdnet.com/article/microsoft-we-want-you-to-learn-python-programming-language-for-free/)**
			  * **[JPMorgan's Athena has 35 million lines of Python code, and won't be updated to Python 3 in time](https://www.techrepublic.com/article/jpmorgans-athena-has-35-million-lines-of-python-code-and-wont-be-updated-to-python-3-in-time/?ftag=CMG-01-10aaa1b) TechRepublic**
			  * **[Mozilla's radical open-source move helped rewrite rules of tech](https://www.cnet.com/news/mozilla-open-source-firefox-move-helped-rewrite-tech-rules-anniversary/?ftag=CMG-01-10aaa1b) CNET**
			  
			  [Editorial standards](https://www.zdnet.com/editorial-guidelines/)
	- Build a website with Python tools | Opensource.com](https://omnivore.app/me/build-a-website-with-python-tools-opensource-com-18bc48d159a)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/20/4/website-sphinx-tox)
	  author:: Moshe Zadka
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 21st, 2020]]
		- ### Content
			- ![Digital creative of a browser on the internet](https://proxy-prod.omnivore-image-cache.app/1040x584,sLRpnVRnu0lPbGNx3ONfi_TOKFoOzJOO3gbYL0oUx3vw/https://opensource.com/sites/default/files/lead-images/browser_web_internet_website.png "Digital creative of a browser on the internet") 
			  
			  Not every website is a blog, where the main feature is a list of posts, each with a specific timestamp that indicates how "fresh" it is. Sometimes, you just want a _website_.
			  
			  Maybe you are an amateur (or professional!) cook and want to show off your recipes. Maybe you are a poet and want a site to publish all your poems. Maybe you have strong opinions and want a place to store your rants, each in its timeless perfection. Here's where [Sphinx](https://www.sphinx-doc.org/en/master/) comes in.
			  
			  \#\# Build out a Sphinx website
			  
			  This how-to will use the example of a fictional startup that needs to build a website. In an alternate universe, multiplication and negation are computationally heavy. It is possible to do them on your local laptop, but it takes a long time. Enter the cloud and your fictional company, Calculate Solutions. It offers calculation as a service, in what is sure to disrupt the calculation industry.
			  
			  The engineers are hard at work building a minimum viable product, and marketing needs to make an attractive website showcasing the solution. With no available engineering resources, as all are devoted to cracking the multiplication and negation problems, and little budget, what are you to do?
			  
			  The story starts with a local Git repository in order to [version-control](https://opensource.com/resources/what-is-git) the website.
			  
			  Like all good Git repositories, this one begins with a **.gitignore**, which tells Git to ignore files or directories listed in it. This one is a simple, humble, **.gitignore**:
			  
			  ```jboss-cli
			  /build
			  ```
			  
			  I will explain why you want to add this directory to the **.gitignore** file below.
			  
			  Next, it's time to put the copywriters to work writing the marketing copy. In **doc/index.rst**, they write correctly formatted [reStructuredText](https://docutils.sourceforge.io/rst.html):
			  
			  ```asciidoc
			  Calculate Solutions
			  -------------------
			  
			  Learn how to add, subtract, multiply, and divide!
			  
			  .. toctree::
			  
			    multiplication
			    negation
			  ```
			  
			  The punctuation and use of **toctree** allow us to create a table of contents of the subpages we intend to build. Next, we can put more details about Calculate Solutions' specific offerings in **doc/multiplication.rst** and **doc/negation.rst**.
			  
			  Then add a short file, **doc/conf.py**, with important metadata:
			  
			  ```ini
			  master_doc = 'index'
			  project = "Calculate.Solutions"
			  copyright = '2020, Calculate Solutions, Inc.'
			  author = 'Calculate Solutions, Inc.'
			  version = release = ''
			  ```
			  
			  And set the version and release to the empty string. Sphinx's original purpose was to document software, which is a use case where the version and release tag are important, but they are not useful for Calculate Solutions' website.
			  
			  Finally, make sure you can build the documentation locally and see that it looks OK. To do so, use [tox](https://opensource.com/article/19/5/python-tox), a handy automation and testing library.
			  
			  Save the following tox configuration file to **tox.ini**:
			  
			  ```makefile
			  [tox]
			  envlist = website
			  toxworkdir = {toxinidir}/build/tox
			  skipsdist = True
			  
			  [testenv:website]
			  basepython = python3.7
			  deps =
			    sphinx
			  changedir = doc
			  commands =
			    sphinx-build -W -b html -d {envtmpdir}/doctrees . {envtmpdir}/html
			  ```
			  
			  The tox configuration is split into two parts: one is the general configuration, and one sets up the specific "test environment." The general configuration sets the list of test environments; here, you have only one, the one that builds your website locally. You must also configure the working directory. This is useful since, by default, tox will put its output into a **.tox** directory. "Hidden" directories like this are sometimes hard to open from GUI programs like web browsers. So instead, put it under **build**, the directory you were careful to ignore in **.gitignore**.
			  
			  By default, tox assumes you are testing a Python package. Since you are not, tell it to **skipsdist**.
			  
			  In the environment configuration, be explicit about which Python interpreter to use. Tox will sometimes try to infer this from the name of the environment, but in this case, detection does not work. Ensure the version you set here is available from your path.
			  
			  From there, tell tox to [install Sphinx](https://opensource.com/article/19/11/document-python-sphinx) in the virtual environment it creates for this run, then change into your **doc** directory, where you put your configuration and marketing copy, and run the **sphinx** command that builds the website.
			  
			  Now you can simply run:
			  
			  ```elixir
			  $ tox
			  ```
			  
			  After a successful run, open **build/docs/tmp/html/index.html** in a browser to see if it looks OK.
			  
			  \#\# Publish your site
			  
			  Now you need to get your site out of local development and into the world! You could make a [CI/CD pipeline](https://opensource.com/article/19/4/devops-pipeline) that would auto-refresh and push it out, set up a load balancer, and set up HTTPS certificates, but that seems like it would require a lot of work.
			  
			  Enter [Read The Docs](https://readthedocs.io/). Read the Docs is a free website that auto-publishes Sphinx-based documentation, and it is all built with open source software by [very thoughtful maintainers](https://docs.readthedocs.io/en/stable/open-source-philosophy.html).
			  
			  After creating or logging into your Read The Docs account, go to the Import screen.
			  
			  ![Read the Docs Import screen](https://proxy-prod.omnivore-image-cache.app/675x207,sWmslFZZWQfuZmiJsarTZft_iWezyzEkWvrWTEj9W9-M/https://opensource.com/sites/default/files/uploads/import.png "Read the Docs Import screen")
			  
			  You might have to refresh your account or project list, but eventually, you should see your project in the list of options.
			  
			  ![Project in Read the Docs](https://proxy-prod.omnivore-image-cache.app/675x85,sBBaokebD8haY7bgvwj4wcb95DV6aYoUnP6NQ-EBhi2A/https://opensource.com/sites/default/files/uploads/project.png "Project")
			  
			  You do not need any of the Advanced options, so accept the defaults and move on.
			  
			  ![Project details in Read the Docs](https://proxy-prod.omnivore-image-cache.app/675x427,sPwN4dCFX1P8eA_LeEKrQV4d4BuBD_YzFZRe6vxzR-Q8/https://opensource.com/sites/default/files/uploads/details.png "Project details in Read the Docs")
			  
			  Start a build by clicking the Build button, and your first documentation build should begin:
			  
			  ![Builds view in Read The Docs](https://proxy-prod.omnivore-image-cache.app/1628x700,sKrXvMwzZA0MBcDfZzy2ViuGU4uVGwxHkxoU8XM5qZEo/https://opensource.com/sites/default/files/images/build.png "Builds view in Read The Docs")
			  
			  The first build should add the webhook to connect your Git repository to publish to Read the Docs. If you run into a problem, there is [ample documentation](https://docs.readthedocs.io/en/stable/webhooks.html) on how to add it manually.
			  
			  When the build is done, your website is up!
			  
			  ![Sphinx website](https://proxy-prod.omnivore-image-cache.app/675x375,ss5MV13h_JQ0pl4Tijp5WrkgaqEz3SuigdU4vPeXk8TI/https://opensource.com/sites/default/files/uploads/website.png "Sphinx website")
			  
			  However, you want it to be on your _real domain_, **calculate.solutions**.
			  
			  To do so, use the Custom Domain feature on Read The Docs.
			  
			  ![Custom Domain on Read the Docs](https://proxy-prod.omnivore-image-cache.app/675x452,ssTgO3Cte6AEfCIUCbTVWSEEnqSygfqkTHTt5NUiaAk8/https://opensource.com/sites/default/files/uploads/domain.png "Custom Domain on Read the Docs")
			  
			  Finally, your website is ready for its public unveiling.
			  
			  ![Read the Docs final website](https://proxy-prod.omnivore-image-cache.app/675x329,sQg0h_AWDkhrw9sFT3Ngrbhsx3YsfLuUsStIpNBE7zG0/https://opensource.com/sites/default/files/uploads/final.png "Read the Docs final website")
			  
			  Although you are a disruptive startup, you are still diligent in supporting open source services, so upgrade your accounts to gold and apply the Ad-Free option to your website.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/647x396,sJNovdRxb-mWi0FXRgk2gd8C7lYpKlkIivPiJWCDoT7o/https://opensource.com/sites/default/files/images/read-the-docs-gold-opensourcedotcom.png)
			  
			  This also gives your site a more professional look!
			  
			  \#\# No-Ops website
			  
			  If you need a website for your recipes, poems, rants, or Calculation-as-a-Service startup, Sphinx and Read The Docs are an easy way to No-Ops your way into a website. Combining the site with a minimal tox and Sphinx configuration, you have a scalable infrastructure that you don't have to run yourself. Success!
			  
			  ![Moshe sitting down, head slightly to the side. His t-shirt has Guardians of the Galaxy silhoutes against a background of sound visualization bars.](https://proxy-prod.omnivore-image-cache.app/150x150,s9Qfp74cj8H4XChqFVMWHlU33PrKyfSoEaPCdCvl6-34/https://opensource.com/sites/default/files/styles/150x150/public/pictures/m48a0550-half.jpg?itok=jlWNwEOM) 
			  
			  Moshe has been involved in the Linux community since 1998, helping in Linux "installation parties". He has been programming Python since 1999, and has contributed to the core Python interpreter. Moshe has been a DevOps/SRE since before those terms existed, caring deeply about software reliability, build reproducibility and other such things.
			  
			  \#\# Related Content
			  
			  [ ![Creative Commons License](https://proxy-prod.omnivore-image-cache.app/0x0,s4aXUWHfwSQ2Woh5RX82tXHIVoMMMXKO7tdegsKArCy8/https://opensource.com/themes/osdc/assets/img/cc-by-sa-4.png "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.")](http://creativecommons.org/licenses/by-sa/4.0/)This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.
	- The 7 most popular ways to plot data in Python | Opensource.com](https://omnivore.app/me/the-7-most-popular-ways-to-plot-data-in-python-opensource-com-18bc48d1531)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/20/4/plot-data-python)
	  author:: Shaun Taylor-Morgan
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 2nd, 2020]]
		- ### Content
			- ![wavegraph](https://proxy-prod.omnivore-image-cache.app/520x292,sZXjzjDOqBWfkpGjRoghsnOFM7l5GYq1Bix9-9zPz_wE/https://opensource.com/sites/default/files/lead-images/LIFE_wavegraph.png "The legacy of open source and the tide of progress") 
			  
			  "How do I make plots in Python?" used to have a simple answer: Matplotlib was the only way. Nowadays, Python is the language of [data science](https://opensource.com/resources/data-science), and there's a lot more choice. What should you use?
			  
			  This guide will help you decide. It will show you how to use each of the four most popular Python plotting libraries—**Matplotlib**, **Seaborn**, **Plotly**, and **Bokeh**—plus a couple of great up-and-comers to consider: **Altair**, with its expressive API, and **Pygal**, with its beautiful SVG output. I'll also look at the very convenient plotting API provided by **pandas**.
			  
			  For each library, I've included source code snippets, as well as a full web-based example using [Anvil](https://anvil.works/), our platform for building web apps with nothing but Python. Let's take a look.
			  
			  \#\# An example plot
			  
			  Each library takes a slightly different approach to plotting data. To compare them, I'll make the same plot with each library and show you the source code. For my example data, I chose this grouped bar chart of British election results since 1966:
			  
			  ![Bar chart of British election data](https://proxy-prod.omnivore-image-cache.app/675x323,svoOn7d5pSwuJons5-chPRQYkMW_yp-eiBbBCYmiv2J8/https://opensource.com/sites/default/files/uploads/british-election-data-chart.png "Bar chart of British election data")
			  
			  I compiled the [dataset of British election history](https://en.wikipedia.org/wiki/United%5FKingdom%5Fgeneral%5Felections%5Foverview) from Wikipedia: the number of seats in the UK parliament won by the Conservative, Labour, and Liberal parties (broadly defined) in each election from 1966 to 2019, plus the number of seats won by "others." You can [download it as a CSV file](https://anvil.works/blog/img/plotting-in-python/uk-election-results.csv).
			  
			  \#\# Matplotlib
			  
			  [Matplotlib](https://matplotlib.org/) is the oldest Python plotting library, and it's still the most popular. It was created in 2003 as part of the [SciPy Stack](https://www.scipy.org/about.html), an open source scientific computing library similar to [Matlab](https://www.mathworks.com/products/matlab.html).
			  
			  Matplotlib gives you precise control over your plots—for example, you can define the individual x-position of each bar in your barplot. Here is the code to graph this (which you can run [here](https://anvil.works/blog/plotting-in-matplotlib)):
			  
			  ```routeros
			    import matplotlib.pyplot as plt
			    import numpy as np
			    from votes import wide as df
			  
			    \# Initialise a figure. subplots() with no args gives one plot.
			    fig, ax = plt.subplots()
			  
			    \# A little data preparation
			    years = df['year']
			    x = np.arange(len(years))
			  
			    \# Plot each bar plot. Note: manually calculating the 'dodges' of the bars
			    ax.bar(x - 3*width/2, df['conservative'], width, label='Conservative', color='\#0343df')
			    ax.bar(x - width/2, df['labour'], width, label='Labour', color='\#e50000')
			    ax.bar(x + width/2, df['liberal'], width, label='Liberal', color='\#ffff14')
			    ax.bar(x + 3*width/2, df['others'], width, label='Others', color='\#929591')
			  
			    \# Customise some display properties
			    ax.set_ylabel('Seats')
			    ax.set_title('UK election results')
			    ax.set_xticks(x)    \# This ensures we have one tick per year, otherwise we get fewer
			    ax.set_xticklabels(years.astype(str).values, rotation='vertical')
			    ax.legend()
			  
			    \# Ask Matplotlib to show the plot
			    plt.show()
			  ```
			  
			  And here are the election results plotted in Matplotlib:
			  
			  ![Matplotlib plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x520,suLvnQZk8aOXunLHuvBSLWpg9WVb6RRIhtUDsTSskoOM/https://opensource.com/sites/default/files/uploads/matplotlib.png "Matplotlib plot of British election data")
			  
			  \#\# Seaborn
			  
			  [Seaborn](https://seaborn.pydata.org/) is an abstraction layer on top of Matplotlib; it gives you a really neat interface to make a wide range of useful plot types very easily.
			  
			  It doesn't compromise on power, though! Seaborn gives [escape hatches](https://anvil.works/blog/escape-hatches-and-ejector-seats) to access the underlying Matplotlib objects, so you still have complete control.
			  
			  Seaborn's code is simpler than the raw Matplotlib (runnable [here](https://anvil.works/blog/plotting-in-seaborn)):
			  
			  ```routeros
			    import seaborn as sns
			    from votes import long as df
			  
			    \# Some boilerplate to initialise things
			    sns.set()
			    plt.figure()
			  
			    \# This is where the actual plot gets made
			    ax = sns.barplot(data=df, x="year", y="seats", hue="party", palette=['blue', 'red', 'yellow', 'grey'], saturation=0.6)
			  
			    \# Customise some display properties
			    ax.set_title('UK election results')
			    ax.grid(color='\#cccccc')
			    ax.set_ylabel('Seats')
			    ax.set_xlabel(None)
			    ax.set_xticklabels(df["year"].unique().astype(str), rotation='vertical')
			  
			    \# Ask Matplotlib to show it
			    plt.show()
			  ```
			  
			  And produces this chart:
			  
			  ![Seaborn plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x519,sa6ft1PYtkHxj9TXaF7lJH6vE4lqS97fHbt2rw1zQ_NU/https://opensource.com/sites/default/files/uploads/seaborn.png "Seaborn plot of British election data")
			  
			  \#\# Plotly
			  
			  [Plotly](https://plot.ly/) is a plotting ecosystem that includes a Python plotting library. It has three different interfaces:
			  
			  * An object-oriented interface
			  * An imperative interface that allows you to specify your plot using JSON-like data structures
			  * A high-level interface similar to Seaborn called Plotly Express
			  
			  Plotly plots are designed to be embedded in web apps. At its core, Plotly is actually a JavaScript library! It uses [D3](https://d3js.org/) and [stack.gl](http://stack.gl/) to draw the plots.
			  
			  You can build Plotly libraries in other languages by passing JSON to the JavaScript library. The official Python and R libraries do just that. At Anvil, we ported the Python Plotly API to [run in the web browser](https://anvil.works/docs/client/components/plots).
			  
			  Here's the source code in Plotly (which you can run [here](https://anvil.works/blog/plotting-in-plotly)):
			  
			  ```routeros
			    import plotly.graph_objects as go
			    from votes import wide as df
			  
			    \#  Get a convenient list of x-values
			    years = df['year']
			    x = list(range(len(years)))
			  
			    \# Specify the plots
			    bar_plots = [
			        go.Bar(x=x, y=df['conservative'], name='Conservative', marker=go.bar.Marker(color='\#0343df')),
			        go.Bar(x=x, y=df['labour'], name='Labour', marker=go.bar.Marker(color='\#e50000')),
			        go.Bar(x=x, y=df['liberal'], name='Liberal', marker=go.bar.Marker(color='\#ffff14')),
			        go.Bar(x=x, y=df['others'], name='Others', marker=go.bar.Marker(color='\#929591')),
			    ]
			  
			    \# Customise some display properties
			    layout = go.Layout(
			        title=go.layout.Title(text="Election results", x=0.5),
			        yaxis_title="Seats",
			        xaxis_tickmode="array",
			        xaxis_tickvals=list(range(27)),
			        xaxis_ticktext=tuple(df['year'].values),
			    )
			  
			    \# Make the multi-bar plot
			    fig = go.Figure(data=bar_plots, layout=layout)
			  
			    \# Tell Plotly to render it
			    fig.show()
			  ```
			  
			  And the election results plot:
			  
			  ![Plotly plot of British election data](https://proxy-prod.omnivore-image-cache.app/992x474,sSGhnZ64mBcX-WglHFYuA7Y17C8pEJNVEXlIUHL9YF0I/https://opensource.com/sites/default/files/uploads/plotly.gif "Plotly plot of British election data")
			  
			  \#\# Bokeh
			  
			  [Bokeh](https://docs.bokeh.org/en/latest/index.html) (pronounced "BOE-kay") specializes in building interactive plots, so this standard example doesn't show it off to its best. Like Plotly, Bokeh's plots are designed to be embedded in web apps; it outputs its plots as HTML files.
			  
			  Here is the code in Bokeh (which you can run [here](https://anvil.works/blog/plotting-in-bokeh))
			  
			  ```routeros
			    from bokeh.io import show, output_file
			    from bokeh.models import ColumnDataSource, FactorRange, HoverTool
			    from bokeh.plotting import figure
			    from bokeh.transform import factor_cmap
			    from votes import long as df
			  
			    \# Specify a file to write the plot to
			    output_file("elections.html")
			  
			    \# Tuples of groups (year, party)
			    x = [(str(r[1]['year']), r[1]['party']) for r in df.iterrows()]
			    y = df['seats']
			  
			    \# Bokeh wraps your data in its own objects to support interactivity
			    source = ColumnDataSource(data=dict(x=x, y=y))
			  
			    \# Create a colourmap
			    cmap = {
			        'Conservative': '\#0343df',
			        'Labour': '\#e50000',
			        'Liberal': '\#ffff14',
			        'Others': '\#929591',
			    }
			    fill_color = factor_cmap('x', palette=list(cmap.values()), factors=list(cmap.keys()), start=1, end=2)
			  
			    \# Make the plot
			    p = figure(x_range=FactorRange(*x), width=1200, title="Election results")
			    p.vbar(x='x', top='y', width=0.9, source=source, fill_color=fill_color, line_color=fill_color)
			  
			    \# Customise some display properties
			    p.y_range.start = 0
			    p.x_range.range_padding = 0.1
			    p.yaxis.axis_label = 'Seats'
			    p.xaxis.major_label_orientation = 1
			    p.xgrid.grid_line_color = None
			  ```
			  
			  And the plot:
			  
			  ![Bokeh plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x333,s67fwX4VBV-LUUekkC7YMAMN4LXSrZkLlAH0tcge6Jlo/https://opensource.com/sites/default/files/uploads/bokeh.png "Bokeh plot of British election data")
			  
			  \#\# Altair
			  
			  [Altair](https://altair-viz.github.io/) is based on a declarative plotting language (or "visualization grammar") called [Vega](https://vega.github.io/vega/). This means it's a well-thought-through API that scales well for complex plots, saving you from getting lost in nested-for-loop hell.
			  
			  As with Bokeh, Altair outputs its plots as HTML files. Here's the code (which you can run [here](https://anvil.works/blog/plotting-in-altair)):
			  
			  ```pgsql
			    import altair as alt
			    from votes import long as df
			  
			    \# Set up the colourmap
			    cmap = {
			        'Conservative': '\#0343df',
			        'Labour': '\#e50000',
			        'Liberal': '\#ffff14',
			        'Others': '\#929591',
			    }
			  
			    \# Cast years to strings
			    df['year'] = df['year'].astype(str)
			  
			    \# Here's where we make the plot
			    chart = alt.Chart(df).mark_bar().encode(
			        x=alt.X('party', title=None),
			        y='seats',
			        column=alt.Column('year', sort=list(df['year']), title=None),
			        color=alt.Color('party', scale=alt.Scale(domain=list(cmap.keys()), range=list(cmap.values())))
			    )
			  
			    \# Save it as an HTML file.
			    chart.save('altair-elections.html')
			  ```
			  
			  And the resulting chart:
			  
			  ![Altair plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x175,s0vczNB4PcZGgLKRxBIpyD29bQHYBIBkocCPHQNc4F5E/https://opensource.com/sites/default/files/uploads/altair.png "Altair plot of British election data")
			  
			  \#\# Pygal
			  
			  [Pygal](http://www.pygal.org/en/stable/) focuses on visual appearance. It produces SVG plots by default, so you can zoom them forever or print them out without them getting pixellated. Pygal plots also come with some good interactivity features built-in, making Pygal another underrated candidate if you're looking to embed plots in a web app.
			  
			  The source code looks like this (and you can run it [here](https://anvil.works/blog/plotting-in-pygal)):
			  
			  ```routeros
			    import pygal
			    from pygal.style import Style
			    from votes import wide as df
			  
			    \# Define the style
			    custom_style = Style(
			        colors=('\#0343df', '\#e50000', '\#ffff14', '\#929591')
			        font_family='Roboto,Helvetica,Arial,sans-serif',
			        background='transparent',
			        label_font_size=14,
			    )
			  
			    \# Set up the bar plot, ready for data
			    c = pygal.Bar(
			        title="UK Election Results",
			        style=custom_style,
			        y_title='Seats',
			        width=1200,
			        x_label_rotation=270,
			    )
			  
			    \# Add four data sets to the bar plot
			    c.add('Conservative', df['conservative'])
			    c.add('Labour', df['labour'])
			    c.add('Liberal', df['liberal'])
			    c.add('Others', df['others'])
			  
			    \# Define the X-labels
			    c.x_labels = df['year']
			  
			    \# Write this to an SVG file
			    c.render_to_file('pygal.svg')
			  ```
			  
			  And the chart:
			  
			  ![Pygal plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x351,syPx8GeAtx_6E-hWhA-rQk_3S2lbjAPm8-J9omB0287k/https://opensource.com/sites/default/files/uploads/pygal.png "Pygal plot of British election data")
			  
			  \#\# Pandas
			  
			  [Pandas](https://pandas.pydata.org/) is an extremely popular data science library for Python. It allows you to do all sorts of data manipulation scalably, but it also has a convenient plotting API. Because it operates directly on data frames, the pandas example is the most concise code snippet in this article—even shorter than the Seaborn code!
			  
			  The pandas API is a wrapper around Matplotlib, so you can also use the underlying Matplotlib API to get fine-grained control of your plots.
			  
			  Here's the election results plot in pandas. The code is beautifully concise!
			  
			  ```pgsql
			    from matplotlib.colors import ListedColormap
			    from votes import wide as df
			  
			    cmap = ListedColormap(['\#0343df', '\#e50000', '\#ffff14', '\#929591'])
			  
			    ax = df.plot.bar(x='year', colormap=cmap)
			  
			    ax.set_xlabel(None)
			    ax.set_ylabel('Seats')
			    ax.set_title('UK election results')
			  
			    plt.show()
			  ```
			  
			  And the resulting chart:
			  
			  ![Pandas plot of British election data](https://proxy-prod.omnivore-image-cache.app/675x522,s-kzmxRiv3Aonnbn7OPS7Fy3nTDJvqohc2ckHf5mJ4R0/https://opensource.com/sites/default/files/uploads/pandas.png "Pandas plot of British election data")
			  
			  To run this example, check out [here](https://anvil.works/blog/plotting-in-pandas).
			  
			  \#\# Plot your way
			  
			  Python offers many ways to plot the same data without much code. While you can get started quickly creating charts with any of these methods, they do take some local configuration. [Anvil](https://anvil.works/) offers a beautiful web-based experience for Python development if you're in need. Happy plotting!
			  
			  ---
			  
			  _This article is based on [Plotting in Python: comparing the options](https://anvil.works/blog/plotting-in-python) on Anvil's blog and is reused with permission._
			  
			  ![User profile image.](https://proxy-prod.omnivore-image-cache.app/150x150,s_RZBN1Xd38sT-VzejmnSidN4-cpORQ8tDWy5uXeFpfQ/https://opensource.com/sites/default/files/styles/150x150/public/pictures/shaun1.png?itok=SgXDtEUj) 
			  
			  Shaun started programming in earnest by simulating burning fusion plasmas in the world's biggest laser system. He fell in love with Python as a data analysis tool, and has never looked back. Now he wants to turn everything into Python.
			  
			  \#\# Related Content
			  
			  [ ![Creative Commons License](https://proxy-prod.omnivore-image-cache.app/0x0,s4aXUWHfwSQ2Woh5RX82tXHIVoMMMXKO7tdegsKArCy8/https://opensource.com/themes/osdc/assets/img/cc-by-sa-4.png "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.")](http://creativecommons.org/licenses/by-sa/4.0/)This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.
	- Time series / date functionality — pandas 2.1.2 documentation](https://omnivore.app/me/time-series-date-functionality-pandas-2-1-2-documentation-18bc48d123e)
	  collapsed:: true
	  site:: [pandas.pydata.org](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)
	  labels:: [[eda]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Dec 31st, 2017]]
		- ### Content
			- pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy `datetime64` and `timedelta64` dtypes, pandas has consolidated a large number of features from other Python libraries like `scikits.timeseries` as well as created a tremendous amount of new functionality for manipulating time series data.
			  
			  For example, pandas supports:
			  
			  Parsing time series information from various sources and formats
			  
			  In [1]: import datetime
			  
			  In [2]: dti = pd.to_datetime(
			   ...:     ["1/1/2018", np.datetime64(), datetime.datetime(2018, 1, 1)]
			   ...: )
			   ...: 
			  
			  In [3]: dti
			  Out[3]: DatetimeIndex(['2018-01-01', '2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None)
			  
			  Generate sequences of fixed-frequency dates and time spans
			  
			  In [4]: dti = pd.date_range(, periods=3, freq="H")
			  
			  In [5]: dti
			  Out[5]: 
			  DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',
			  
			              dtype='datetime64[ns]', freq='H')
			  
			  Manipulating and converting date times with timezone information
			  
			  In [6]: dti = dti.tz_localize("UTC")
			  
			  In [7]: dti
			  Out[7]: 
			  DatetimeIndex(['2018-01-01 00:00:00+00:00', '2018-01-01 01:00:00+00:00',
			  
			              dtype='datetime64[ns, UTC]', freq='H')
			  
			  In [8]: dti.tz_convert("US/Pacific")
			  Out[8]: 
			  DatetimeIndex(['2017-12-31 16:00:00-08:00', '2017-12-31 17:00:00-08:00',
			  
			              dtype='datetime64[ns, US/Pacific]', freq='H')
			  
			  Resampling or converting a time series to a particular frequency
			  
			  In [9]: idx = pd.date_range(, periods=5, freq="H")
			  
			  In [10]: ts = pd.Series(range(len(idx)), index=idx)
			  
			  In [11]: ts
			  Out[11]: 
			  
			  
			  Freq: H, dtype: int64
			  
			  In [12]: ts.resample("2H").mean()
			  Out[12]: 
			  
			  
			  Freq: 2H, dtype: float64
			  
			  Performing date and time arithmetic with absolute or relative time increments
			  
			  In [13]: friday = pd.Timestamp()
			  
			  In [14]: friday.day_name()
			  Out[14]: 'Friday'
			  
			  \# Add 1 day
			  In [15]: saturday = friday + pd.Timedelta("1 day")
			  
			  In [16]: saturday.day_name()
			  Out[16]: 'Saturday'
			  
			  \# Add 1 business day (Friday --> Monday)
			  In [17]: monday = friday + pd.offsets.BDay()
			  
			  In [18]: monday.day_name()
			  Out[18]: 'Monday'
			  
			  pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more.
			  
			  \#\# Overview[\#](\#overview "Permalink to this heading")
			  
			  pandas captures 4 general time related concepts:
			  
			  1. Date times: A specific date and time with timezone support. Similar to `datetime.datetime` from the standard library.
			  2. Time deltas: An absolute time duration. Similar to `datetime.timedelta` from the standard library.
			  3. Time spans: A span of time defined by a point in time and its associated frequency.
			  4. Date offsets: A relative time duration that respects calendar arithmetic. Similar to `dateutil.relativedelta.relativedelta` from the `dateutil` package.
			  
			  | Concept      | Scalar Class | Array Class    | pandas Data Type                         | Primary Creation Method           |
			  | ------------ | ------------ | -------------- | ---------------------------------------- | --------------------------------- |
			  | Date times   | Timestamp    | DatetimeIndex  | datetime64\[ns\] or datetime64\[ns, tz\] | to\_datetime or date\_range       |
			  | Time deltas  | Timedelta    | TimedeltaIndex | timedelta64\[ns\]                        | to\_timedelta or timedelta\_range |
			  | Time spans   | Period       | PeriodIndex    | period\[freq\]                           | Period or period\_range           |
			  | Date offsets | DateOffset   | None           | None                                     | DateOffset                        |
			  
			  For time series data, it’s conventional to represent the time component in the index of a [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\#pandas.Series "pandas.Series") or [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\#pandas.DataFrame "pandas.DataFrame")so manipulations can be performed with respect to the time element.
			  
			  In [19]: pd.Series(range(3), index=pd.date_range("2000", freq="D", periods=3))
			  Out[19]: 
			  
			  
			  Freq: D, dtype: int64
			  
			  However, [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\#pandas.Series "pandas.Series") and [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\#pandas.DataFrame "pandas.DataFrame") can directly also support the time component as data itself.
			  
			  In [20]: pd.Series(pd.date_range("2000", freq="D", periods=3))
			  Out[20]: 
			  
			  
			  dtype: datetime64[ns]
			  
			  [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\#pandas.Series "pandas.Series") and [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\#pandas.DataFrame "pandas.DataFrame") have extended data type support and functionality for `datetime`, `timedelta`and `Period` data when passed into those constructors. `DateOffset`data however will be stored as `object` data.
			  
			  In [21]: pd.Series(pd.period_range("1/1/2011", freq="M", periods=3))
			  Out[21]: 
			  0    2011-01
			  1    2011-02
			  2    2011-03
			  dtype: period[M]
			  
			  In [22]: pd.Series([pd.DateOffset(1), pd.DateOffset(2)])
			  Out[22]: 
			  0         <DateOffset>
			  1    <2 * DateOffsets>
			  dtype: object
			  
			  In [23]: pd.Series(pd.date_range("1/1/2011", freq="M", periods=3))
			  Out[23]: 
			  
			  
			  dtype: datetime64[ns]
			  
			  Lastly, pandas represents null date times, time deltas, and time spans as `NaT` which is useful for representing missing or null date like values and behaves similar as `np.nan` does for float data.
			  
			  In [24]: pd.Timestamp(pd.NaT)
			  Out[24]: NaT
			  
			  In [25]: pd.Timedelta(pd.NaT)
			  Out[25]: NaT
			  
			  In [26]: pd.Period(pd.NaT)
			  Out[26]: NaT
			  
			  \# Equality acts as np.nan would
			  In [27]: pd.NaT == pd.NaT
			  Out[27]: False
			  
			  \#\# Timestamps vs. time spans[\#](\#timestamps-vs-time-spans "Permalink to this heading")
			  
			  Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time.
			  
			  In [28]: import datetime
			  
			  In [29]: pd.Timestamp(datetime.datetime(2012, 5, 1))
			  Out[29]: 
			  
			  In [30]: pd.Timestamp()
			  Out[30]: 
			  
			  In [31]: pd.Timestamp(2012, 5, 1)
			  Out[31]: 
			  
			  However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by `Period` can be specified explicitly, or inferred from datetime string format.
			  
			  For example:
			  
			  In [32]: pd.Period("2011-01")
			  Out[32]: Period('2011-01', 'M')
			  
			  In [33]: pd.Period("2012-05", freq="D")
			  Out[33]: 
			  
			  [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html\#pandas.Timestamp "pandas.Timestamp") and [Period](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Period.html\#pandas.Period "pandas.Period") can serve as an index. Lists of`Timestamp` and `Period` are automatically coerced to [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html\#pandas.DatetimeIndex "pandas.DatetimeIndex")and [PeriodIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.PeriodIndex.html\#pandas.PeriodIndex "pandas.PeriodIndex") respectively.
			  
			  In [34]: dates = [
			   ....:     pd.Timestamp(),
			   ....:     pd.Timestamp(),
			   ....:     pd.Timestamp(),
			   ....: ]
			   ....: 
			  
			  In [35]: ts = pd.Series(np.random.randn(3), dates)
			  
			  In [36]: type(ts.index)
			  Out[36]: pandas.core.indexes.datetimes.DatetimeIndex
			  
			  In [37]: ts.index
			  Out[37]: DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None)
			  
			  In [38]: ts
			  Out[38]: 
			  
			  
			  dtype: float64
			  
			  In [39]: periods = [pd.Period("2012-01"), pd.Period("2012-02"), pd.Period("2012-03")]
			  
			  In [40]: ts = pd.Series(np.random.randn(3), periods)
			  
			  In [41]: type(ts.index)
			  Out[41]: pandas.core.indexes.period.PeriodIndex
			  
			  In [42]: ts.index
			  Out[42]: PeriodIndex(['2012-01', '2012-02', '2012-03'], dtype='period[M]')
			  
			  In [43]: ts
			  Out[43]: 
			  2012-01   -1.135632
			  2012-02    1.212112
			  2012-03   -0.173215
			  Freq: M, dtype: float64
			  
			  pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of `Timestamp` and sequences of timestamps using instances of`DatetimeIndex`. For regular time spans, pandas uses `Period` objects for scalar values and `PeriodIndex` for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases.
			  
			  \#\# Converting to timestamps[\#](\#converting-to-timestamps "Permalink to this heading")
			  
			  To convert a [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\#pandas.Series "pandas.Series") or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the `to_datetime` function. When passed a `Series`, this returns a `Series` (with the same index), while a list-like is converted to a `DatetimeIndex`:
			  
			  In [44]: pd.to_datetime(pd.Series(["Jul 31, 2009", "Jan 10, 2010", None]))
			  Out[44]: 
			  
			  
			  2          NaT
			  dtype: datetime64[ns]
			  
			  In [45]: pd.to_datetime([, ])
			  Out[45]: DatetimeIndex(['2005-11-23', '2010-12-31'], dtype='datetime64[ns]', freq=None)
			  
			  If you use dates which start with the day first (i.e. European style), you can pass the `dayfirst` flag:
			  
			  In [46]: pd.to_datetime([], dayfirst=True)
			  Out[46]: DatetimeIndex(['2012-01-04 10:00:00'], dtype='datetime64[ns]', freq=None)
			  
			  In [47]: pd.to_datetime([], dayfirst=True)
			  Out[47]: DatetimeIndex(['2012-04-14 10:00:00'], dtype='datetime64[ns]', freq=None)
			  
			  Warning
			  
			  You see in the above example that `dayfirst` isn’t strict. If a date can’t be parsed with the day being first it will be parsed as if`dayfirst` were `False` and a warning will also be raised.
			  
			  If you pass a single string to `to_datetime`, it returns a single `Timestamp`.`Timestamp` can also accept string input, but it doesn’t accept string parsing options like `dayfirst` or `format`, so use `to_datetime` if these are required.
			  
			  In [48]: pd.to_datetime()
			  Out[48]: 
			  
			  In [49]: pd.Timestamp()
			  Out[49]: 
			  
			  You can also use the `DatetimeIndex` constructor directly:
			  
			  In [50]: pd.DatetimeIndex([, , ])
			  Out[50]: DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq=None)
			  
			  The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation:
			  
			  In [51]: pd.DatetimeIndex([, , ], freq="infer")
			  Out[51]: DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq='2D')
			  
			  \#\#\# Providing a format argument[\#](\#providing-a-format-argument "Permalink to this heading")
			  
			  In addition to the required datetime string, a `format` argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably.
			  
			  In [52]: pd.to_datetime(, format="%Y/%m/%d")
			  Out[52]: 
			  
			  In [53]: pd.to_datetime(, format="%d-%m-%Y %H:%M")
			  Out[53]: 
			  
			  For more information on the choices available when specifying the `format`option, see the Python [datetime documentation](https://docs.python.org/3/library/datetime.html\#strftime-and-strptime-behavior).
			  
			  \#\#\# Assembling datetime from multiple DataFrame columns[\#](\#assembling-datetime-from-multiple-dataframe-columns "Permalink to this heading")
			  
			  You can also pass a `DataFrame` of integer or string columns to assemble into a `Series` of `Timestamps`.
			  
			  In [54]: df = pd.DataFrame(
			   ....:     {"year": [2015, 2016], "month": [2, 3], "day": [4, 5], "hour": [2, 3]}
			   ....: )
			   ....: 
			  
			  In [55]: pd.to_datetime(df)
			  Out[55]: 
			  
			  
			  dtype: datetime64[ns]
			  
			  You can pass only the columns that you need to assemble.
			  
			  In [56]: pd.to_datetime(df[["year", "month", "day"]])
			  Out[56]: 
			  
			  
			  dtype: datetime64[ns]
			  
			  `pd.to_datetime` looks for standard designations of the datetime component in the column names, including:
			  
			  * required: `year`, `month`, `day`
			  * optional: `hour`, `minute`, `second`, `millisecond`, `microsecond`, `nanosecond`
			  
			  \#\#\# Invalid data[\#](\#invalid-data "Permalink to this heading")
			  
			  The default behavior, `errors='raise'`, is to raise when unparsable:
			  
			  In [57]: pd.to_datetime([, 'asd'], errors='raise')
			  ---------------------------------------------------------------------------
			  ValueError                                Traceback (most recent call last)
			  Cell In[57], line 1
			  ----> 1 pd.to_datetime([, 'asd'], errors='raise')
			  
			  File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:1144, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
			   1142         result = _convert_and_box_cache(argc, cache_array)
			   1143     else:
			  -> 1144         result = convert_listlike(argc, format)
			   1145 else:
			   1146     result = convert_listlike(np.array([arg]), format)[0]
			  
			  File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:488, in _convert_listlike_datetimes(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)
			    486 \# `format` could be inferred, or user didn't ask for mixed-format parsing.
			    487 if format is not None and format != "mixed":
			  --> 488     return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)
			    490 result, tz_parsed = objects_to_datetime64ns(
			    491     arg,
			    492     dayfirst=dayfirst,
			   (...)
			    496     allow_object=True,
			    497 )
			    499 if tz_parsed is not None:
			    500     \# We can take a shortcut since the datetime64 numpy array
			    501     \# is in UTC
			  
			  File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:519, in _array_strptime_with_fallback(arg, name, utc, fmt, exact, errors)
			    508 def _array_strptime_with_fallback(
			    509     arg,
			    510     name,
			   (...)
			    514     errors: str,
			    515 ) -> Index:
			    516     """
			    517     Call array_strptime, with fallback behavior depending on 'errors'.
			    518     """
			  --> 519     result, timezones = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)
			    520     if any(tz is not None for tz in timezones):
			    521         return _return_parsed_timezone_results(result, timezones, utc, name)
			  
			  File strptime.pyx:534, in pandas._libs.tslibs.strptime.array_strptime()
			  
			  File strptime.pyx:355, in pandas._libs.tslibs.strptime.array_strptime()
			  
			  ValueError: time data "asd" doesn't match format "%Y/%m/%d", at position 1. You might want to try:
	- Hypothesis testing in Machine learning using Python | by Yogesh Agrawal | Towards Data Science](https://omnivore.app/me/hypothesis-testing-in-machine-learning-using-python-by-yogesh-ag-18bc48d100c)
	  collapsed:: true
	  site:: [Towards Data Science](https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce)
	  author:: Yogesh Agrawal
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Jan 21st, 2019]]
		- ### Content
			- Well probably all who are beginner in machine learning or in intermediate level or statistic student heard about this buzz word [hypothesis testing.](https://en.wikipedia.org/wiki/Statistical%5Fhypothesis%5Ftesting)
			  
			  Today i will give a brief introduction over this topic which created headache for me when i was learning this. I put all those concept together and examples using python.
			  
			  some question in mind before i will go for broader things -
			  
			  **What is hypothesis testing ? why do we use it ? what are basic of hypothesis ? which are important parameter of hypothesis testing ?**
			  
			  Let’s start one by one :
			  
			  **1\. What is hypothesis testing ?**
			  
			  Hypothesis testing is a statistical method that is used in making statistical decisions using experimental data. Hypothesis Testing is basically an assumption that we make about the population parameter.
			  
			  Ex : you say avg student in class is 40 or a boy is taller than girls.
			  
			  all those example we assume need some statistic way to prove those. we need some mathematical conclusion what ever we are assuming is true.
			  
			  **2**. **why do we use it ?**
			  
			  **Hypothesis testing** is an essential procedure in statistics. A **hypothesis test** evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data. When **we** say that a finding is statistically significant, it’s thanks to a **hypothesis test**.
			  
			  **3**. **what are basic of hypothesis ?**
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/350x224,sR-l75q84S-2hY3YgS1sUfjFOKsZWhERbUKMHmoD2Eyw/https://miro.medium.com/v2/resize:fit:350/1*U-cR-vP8pYUmLUDwCPv23A.png)
			  
			  Normal Curve images with different mean and variance
			  
			  The basic of hypothesis is [normalisation](https://en.wikipedia.org/wiki/Normalization%5F%28statistics%29) and [standard normalisation](https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization). all our hypothesis is revolve around basic of these 2 terms. let’s see these.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/350x239,s5lDxODxJ0h7tOksaZUBOaTL5OJrCnY7yoiXPocl7IYc/https://miro.medium.com/v2/resize:fit:350/1*2vTwIrqdELKJY-tpheO7GA.jpeg)
			  
			  Standardised Normal curve image and separation on data in percentage in each section.
			  
			  You must be wondering what’s difference between these two image, one might say i don’t find, while other will see some flatter graph compare to steep. well buddy this is not what i want to represent , in 1st first you can see there are different normal curve all those normal curve can have different mean’s and variances where as in 2nd image if you notice the graph is properly distributed and **mean =0 and variance =1 always**. concept of z-score comes in picture when we use **standardised normal data.**
			  
			  **Normal Distribution -**
			  
			  A variable is said to be normally distributed or have a **normal distribution** if **its distribution** has the shape of a **normal curve** — a special bell-shaped **curve**. … The graph of a **normal distribution** is called the **normal curve**, which has all of the following **properties**: 1\. The mean, median, and mode are equal.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/455x168,s6kyZRlEKYW6Z_4PCUV7Ak-sGnOGposq6aIv3Lqvn8PQ/https://miro.medium.com/v2/resize:fit:455/1*gBnxoTRwo9sDovvegHfm6g.png)
			  
			  Normal distribution formula
			  
			  **Standardised Normal Distribution —**
			  
			  A standard normal distribution is a normal distribution with mean 0 and standard deviation 1
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/200x87,sBUKtDasSkaPE8rML_HrUonuZyOB2TLF7G150xF2oVfc/https://miro.medium.com/v2/resize:fit:200/1*UY43iz5Uesa1m9ItIrxYCg.png)
			  
			  Standard Normal Distribution
			  
			  **Which are important parameter of hypothesis testing ?**
			  
			  **Null hypothesis :-** In inferential statistics, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups
			  
			  In other words it is a basic assumption or made based on domain or problem knowledge.
			  
			  Example : a company production is = 50 unit/per day etc.
			  
			  \#\# Alternative hypothesis :-
			  
			  The alternativehypothesis is the hypothesis used in **hypothesis** testing that is contrary to the null hypothesis. It is usually taken to be that the observations are the result of a real effect (with some amount of chance variation superposed)
			  
			  Example : a company production is !=50 unit/per day etc.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/700x467,sLb1fifvGinehQWstW2UpCQh_7B4-BeHCajdxM1hx6lE/https://miro.medium.com/v2/resize:fit:700/1*fEPOHXPQO_ZNJC4UQDXmqw.png)
			  
			  Null and Alternate hypothesis.
			  
			  **Level of significance:** Refers to the degree of significance in which we accept or reject the null-hypothesis. 100% accuracy is not possible for accepting or rejecting a hypothesis, so we therefore select a level of significance that is usually 5%.
			  
			  This is normally denoted with alpha(maths symbol ) and generally it is 0.05 or 5% , which means your output should be 95% confident to give similar kind of result in each sample.
			  
			  **Type I error:** When we reject the null hypothesis, although that hypothesis was true. Type I error is denoted by alpha. In hypothesis testing, the normal curve that shows the critical region is called the alpha region
			  
			  **Type II errors:** When we accept the null hypothesis but it is false. Type II errors are denoted by beta. In Hypothesis testing, the normal curve that shows the acceptance region is called the beta region.
			  
			  **One tailed test :-** A test of a statistical hypothesis , where the region of rejection is on only **one** side of the sampling distribution , is called a **one**\-**tailed test**.
			  
			  Example :- a college has ≥ 4000 student or data science ≤ 80% org adopted.
			  
			  **Two-tailed test :-** A **two**\-**tailed test** is a statistical **test** in which the critical area of a distribution is **two**\-**sided** and testswhether a sample is greater than or less than a certain range of values. If the sample being testedfalls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis.
			  
			  Example : a college != 4000 student or data science != 80% org adopted
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/700x486,slKfe6ZPcUnA5Q1E4xJV3s7qeI2dYpYMZ1ymPdKtFYsk/https://miro.medium.com/v2/resize:fit:700/1*Fwmazvo993cH6q79bpfeIw.jpeg)
			  
			  one and two-tailed images
			  
			  **P-value :-** The **P value**, or calculated probability, is the probability of finding the observed, or more extreme, results when the null hypothesis (H 0) of a study question is true — the **definition** of ‘extreme’ depends on how the hypothesis is being tested.
			  
			  If your P value is less than the chosen significance level then you reject the null hypothesis i.e. accept that your sample gives reasonable evidence to support the alternative hypothesis. It does NOT imply a “meaningful” or “important” difference; that is for you to decide when considering the real-world relevance of your result.
			  
			  Example : you have a coin and you don’t know whether that is fair or tricky so let’s decide **null** and **alternate hypothesis**
			  
			  **H0** : a coin is a fair coin.
			  
			  **H1** : a coin is a tricky coin. and **alpha** \= **5% or 0.05**
			  
			  Now let’s toss the coin and calculate **p- value** ( probability value).
			  
			  Toss a coin 1st time and result is **tail**\- P-value = 50% (as head and tail have equal probability)
			  
			  Toss a coin 2nd time and result is **tail, now p-value =** 50/2 **\= 25%**
			  
			  and similarly we Toss 6 consecutive time and got result as P-value = **1.5%** but we set our significance level as 95% means 5% error rate we allow and here we see we are beyond that level i.e. our null- hypothesis does not hold good so we need to reject and propose that this coin is a tricky coin which is actually.
			  
			  [**Degree of freedom**](http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-degrees-of-freedom-in-statistics) :- Now imagine you’re not into hats. You’re into data analysis.You have a data set with 10 values. If you’re not estimating anything, each value can take on any number, right? Each value is completely free to vary.But suppose you want to test the population mean with a sample of 10 values, using a 1-sample t test. You now have a constraint — the estimation of the mean. What is that constraint, exactly? By definition of the mean, the following relationship must hold: The sum of all values in the data must equal _n_ x mean, where _n_ is the number of values in the data set.
			  
			  So if a data set has 10 values, the sum of the 10 values _must_ equal the mean x 10\. If the mean of the 10 values is 3.5 (you could pick any number), this constraint requires that the sum of the 10 values must equal 10 x 3.5 = 35.
			  
			  With that constraint, the first value in the data set is free to vary. Whatever value it is, it’s still possible for the sum of all 10 numbers to have a value of 35\. The second value is also free to vary, because whatever value you choose, it still allows for the possibility that the sum of all the values is 35.
			  
			  Now Let’s see some of widely used hypothesis testing type :-
			  
			  1. T Test ( Student T test)
			  2. Z Test
			  3. ANOVA Test
			  4. Chi-Square Test
			  
			  **T- Test :-** A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which may be related in certain features. It is mostly used when the data sets, like the set of data recorded as outcome from flipping a coin a 100 times, would follow a normal distribution and may have unknown [variances](https://www.investopedia.com/terms/v/variance.asp). T test is used as a [hypothesis testing](https://www.investopedia.com/terms/h/hypothesistesting.asp) tool, which allows testing of an assumption applicable to a population.
			  
			  T-test has 2 types : 1\. one sampled t-test 2\. two-sampled t-test.
			  
			  **One sample t-test** : The One Sample _t_ Test determines whether the sample mean is statistically different from a known or hypothesised population mean. The One Sample _t_ Test is a parametric test.
			  
			  Example :- you have 10 ages and you are checking whether avg age is 30 or not. (check code below for that using python)
			  
			  from scipy.stats import ttest_1samp  
			  import numpy as npages = np.genfromtxt(“ages.csv”)print(ages)ages_mean = np.mean(ages)  
			  print(ages_mean)  
			  tset, pval = ttest_1samp(ages, 30)print(“p-values”,pval)if pval < 0.05:    \# alpha value is 0.05 or 5%  
			   print(" we are rejecting null hypothesis")  
			  else:  
			  print("we are accepting null hypothesis")
			  
			  Output for above code is :
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/700x258,sLWoDQqqJkuSwthvvCDpNKSHNE-29Or2js9uk8Evhov4/https://miro.medium.com/v2/resize:fit:700/1*oErU154LwSGLrPA5TewC3Q.png)
			  
			  one-sample t-test output
			  
			  **Two sampled T-test :-**The Independent **Samples t Test** or 2-sample t-test compares the means of two independent groups in order to determine whether there is statistical evidence that the associated population means are significantly different. The Independent **Samples t Test** is a parametric **test**. This **test** is also known as: Independent **t Test**.
			  
			  Example : is there any association between week1 and week2 ( code is given below in python)
			  
			  from scipy.stats import ttest_ind  
			  import numpy as npweek1 = np.genfromtxt("week1.csv",  delimiter=",")  
			  week2 = np.genfromtxt("week2.csv",  delimiter=",")print(week1)  
			  print("week2 data :-\n")  
			  print(week2)  
			  week1_mean = np.mean(week1)  
			  week2_mean = np.mean(week2)print("week1 mean value:",week1_mean)  
			  print("week2 mean value:",week2_mean)week1_std = np.std(week1)  
			  week2_std = np.std(week2)print("week1 std value:",week1_std)  
			  print("week2 std value:",week2_std)ttest,pval = ttest_ind(week1,week2)  
			  print("p-value",pval)if pval <0.05:  
			  print("we reject null hypothesis")  
			  else:  
			  print("we accept null hypothesis")
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/700x428,s8vD4GCVxvutQwvYtJDk8qGDr0rezQNn5OCYo6FJEGro/https://miro.medium.com/v2/resize:fit:700/1*knyWSvP5efefqkg0IgtfmQ.png)
			  
			  2-sampled t-test output
			  
			  **Paired sampled t-test :-** The paired sample t-test is also called dependent sample t-test. It’s an uni variate test that tests for a significant difference between 2 related variables. An example of this is if you where to collect the blood pressure for an individual before and after some treatment, condition, or time point.
			  
			  **H0 :- means difference between two sample is 0**
			  
			  **H1:- mean difference between two sample is not 0**
			  
			  check the code below for same
			  
			  import pandas as pd  
			  from scipy import stats  
			  df = pd.read_csv("blood_pressure.csv")df[['bp_before','bp_after']].describe()ttest,pval = stats.ttest_rel(df['bp_before'], df['bp_after'])  
			  print(pval)if pval<0.05:  
			    print("reject null hypothesis")  
			  else:  
			    print("accept null hypothesis")
			  
			  \#\# When you can run a Z Test.
			  
			  Several different types of tests are used in statistics (i.e. [f test](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/f-test/), [chi square test](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/chi-square/), [t test](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/t-test/)). You would use a Z test if:
			  
			  * Your [sample size](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/find-sample-size/) is greater than 30\. Otherwise, use a [t test](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/t-test/).
			  * Data points should be [independent ](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/dependent-events-independent/\#or)from each other. In other words, one data point isn’t related or doesn’t affect another data point.
			  * Your data should be normally distributed. However, for large sample sizes (over 30) this doesn’t always matter.
			  * Your data should be randomly selected from a population, where each item has an equal chance of being selected.
			  * [Sample sizes](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/find-sample-size/) should be equal if at all possible.
			  
			  Example again we are using z-test for blood pressure with some mean like 156 (python code is below for same) **one-sample Z test.**
			  
			  import pandas as pd  
			  from scipy import stats  
			  from statsmodels.stats import weightstats as stestsztest ,pval = stests.ztest(df['bp_before'], x2=None, value=156)  
			  print(float(pval))if pval<0.05:  
			    print("reject null hypothesis")  
			  else:  
			    print("accept null hypothesis")
			  
			  **Two-sample Z test-** In two sample z-test , similar to t-test here we are checking two independent data groups and deciding whether sample mean of two group is equal or not.
			  
			  **H0 : mean of two group is 0**
			  
			  **H1 : mean of two group is not 0**
			  
			  Example : we are checking in blood data after blood and before blood data.(code in python below)
			  
			  ztest ,pval1 = stests.ztest(df['bp_before'], x2=df['bp_after'], value=0,alternative='two-sided')  
			  print(float(pval1))if pval<0.05:  
			    print("reject null hypothesis")  
			  else:  
			    print("accept null hypothesis")
			  
			  **ANOVA (F-TEST) :-** The t-test works well when dealing with two groups, but sometimes we want to compare more than two groups at the same time. For example, if we wanted to test whether voter age differs based on some categorical variable like race, we have to compare the means of each level or group the variable. We could carry out a separate t-test for each pair of groups, but when you conduct many tests you increase the chances of false positives. The [analysis of variance](https://en.wikipedia.org/wiki/Analysis%5Fof%5Fvariance) or ANOVA is a statistical inference test that lets you compare multiple groups at the same time.
			  
			  **F = Between group variability / Within group variability**
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/640x500,scwStCF4X2IOTNgt3Kvaj7dmvlmQ3B00lj01iJfFL3Gc/https://miro.medium.com/v2/resize:fit:640/1*SV4xlXEFCKzgT4PbB6nlgA.png)
			  
			  F-Test or Anova concept image
			  
			  Unlike the z and t-distributions, the F-distribution does not have any negative values because between and within-group variability are always positive due to squaring each deviation.
			  
			  **One Way F-test(Anova) :-** It tell whether two or more groups are similar or not based on their mean similarity and f-score.
			  
			  Example : there are 3 different category of plant and their weight and need to check whether all 3 group are similar or not (code in python below)
			  
			  df_anova = pd.read_csv('PlantGrowth.csv')  
			  df_anova = df_anova[['weight','group']]grps = pd.unique(df_anova.group.values)  
			  d_data = {grp:df_anova['weight'][df_anova.group == grp] for grp in grps}
			  
			  F, p = stats.f_oneway(d_data['ctrl'], d_data['trt1'], d_data['trt2'])
			  
			  print("p-value for significance is: ", p)if p<0.05:  
			    print("reject null hypothesis")  
			  else:  
			    print("accept null hypothesis")
			  
			  **Two Way F-test :-** Two way F-test is extension of 1-way f-test, it is used when we have 2 independent variable and 2+ groups. 2-way F-test does not tell which variable is dominant. if we need to check individual significance then **Post-hoc** testing need to be performed.
			  
			  Now let’s take a look at the Grand mean crop yield (the mean crop yield not by any sub-group), as well the mean crop yield by each factor, as well as by the factors grouped together
			  
			  import statsmodels.api as sm  
			  from statsmodels.formula.api import olsdf_anova2 = pd.read_csv("<https://raw.githubusercontent.com/Opensourcefordatascience/Data-sets/master/crop%5Fyield.csv>")model = ols('Yield ~ C(Fert)*C(Water)', df_anova2).fit()  
			  print(f"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}")res = sm.stats.anova_lm(model, typ= 2)  
			  res
			  
			  **Chi-Square Test-** The test is applied when you have two [categorical variables](https://stattrek.com/Help/Glossary.aspx?Target=Categorical+variable) from a single population. It is used to determine whether there is a significant association between the two variables.
			  
			  For example, in an election survey, voters might be classified by gender (male or female) and voting preference (Democrat, Republican, or Independent). We could use a chi-square test for independence to determine whether gender is related to voting preference
			  
			  check example in python below
			  
			  df_chi = pd.read_csv('chi-test.csv')  
			  contingency_table=pd.crosstab(df_chi["Gender"],df_chi["Shopping?"])  
			  print('contingency_table :-\n',contingency_table)\#Observed Values  
			  Observed_Values = contingency_table.values   
			  print("Observed Values :-\n",Observed_Values)b=stats.chi2_contingency(contingency_table)  
			  Expected_Values = b[3]  
			  print("Expected Values :-\n",Expected_Values)no_of_rows=len(contingency_table.iloc[0:2,0])  
			  no_of_columns=len(contingency_table.iloc[0,0:2])  
			  ddof=(no_of_rows-1)*(no_of_columns-1)  
			  print("Degree of Freedom:-",ddof)  
			  alpha = 0.05from scipy.stats import chi2  
			  chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])  
			  chi_square_statistic=chi_square[0]+chi_square[1]  
			  print("chi-square statistic:-",chi_square_statistic)critical_value=chi2.ppf(q=1-alpha,df=ddof)  
			  print('critical_value:',critical_value)\#p-value  
			  p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)  
			  print('p-value:',p_value)print('Significance level: ',alpha)  
			  print('Degree of Freedom: ',ddof)  
			  print('chi-square statistic:',chi_square_statistic)  
			  print('critical_value:',critical_value)  
			  print('p-value:',p_value)if chi_square_statistic>=critical_value:  
			    print("Reject H0,There is a relationship between 2 categorical variables")  
			  else:  
			    print("Retain H0,There is no relationship between 2 categorical variables")
			  
			    if p_value<=alpha:  
			    print("Reject H0,There is a relationship between 2 categorical variables")  
			  else:  
			    print("Retain H0,There is no relationship between 2 categorical variables")
			  
			  You can get all code in my [git](https://github.com/yug95/MachineLearning) repository.
			  
			  ah, finally we came to end of this article. I hope this article would have helped. any feedback is always appreciated.
			  
			  For more update check my [git](https://github.com/yug95/MachineLearning) and follow we on medium.
	- Understanding the Transform Function in Pandas - Practical Business Python](https://omnivore.app/me/understanding-the-transform-function-in-pandas-practical-busines-18bc48d0887)
	  collapsed:: true
	  site:: [pbpython.com](https://pbpython.com/pandas_transform.html)
	  author:: Chris Moffitt
	  labels:: [[pandas]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 3rd, 2017]]
		- ### Content
			- ![article header image](https://proxy-prod.omnivore-image-cache.app/0x0,sXuQ6Gv26LmS_iTOSWy96XCBa44L7e1QqBHHHR8s45nQ/https://pbpython.com/images/transform-example.png)
			  
			  \#\# Introduction
			  
			  One of the compelling features of [pandas](http://pandas.pydata.org/) is that it has a rich library of methods for manipulating data. However, there are times when it is not clear what the various functions do and how to use them. If you are approaching a problem from an Excel mindset, it can be difficult to translate the planned solution into the unfamiliar pandas command. One of those “unknown” functions is the `transform` method. Even after using pandas for a while, I have never had the chance to use this function so I recently took some time to figure out what it is and how it could be helpful for real world analysis. This article will walk through an example where` transform` can be used to efficiently summarize data.
			  
			  \#\# What is transform?
			  
			  I have found the best coverage of this topic in Jake VanderPlas’ excellent[Python Data Science Handbook](http://amzn.to/2oy9jbR). I plan to write a review on this book in the future but the short and sweet is that it is a great resource that I highly recommend.
			  
			  As described in the book, `transform` is an operation used in conjunction with `groupby` (which is one of the most useful operations in pandas). I suspect most pandas users likely have used `aggregate`, `filter` or` apply` with `groupby` to summarize data. However, `transform`is a little more difficult to understand - especially coming from an Excel world. Since Jake made all of his book available via jupyter [notebooks](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks) it is a good place to start to understand how [transform](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb) is unique:
			  
			  > While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean.
			  
			  With that basic definition, I will go through another example that can explain how this is useful in other instances outside of centering data.
			  
			  \#\# Problem Set
			  
			  For this example, we will analyze some fictitious [sales data](https://github.com/chris1610/pbpython/blob/master/data/sales%5Ftransactions.xlsx?raw=true). In order to keep the dataset small, here is a sample of 12 sales transactions for our company:
			  
			  | account | name   | order         | sku   | quantity | unit price | ext price |         |
			  | ------- | ------ | ------------- | ----- | -------- | ---------- | --------- | ------- |
			  | 0       | 383080 | Will LLC      | 10001 | B1-20000 | 7          | 33.69     | 235.83  |
			  | 1       | 383080 | Will LLC      | 10001 | S1-27722 | 11         | 21.12     | 232.32  |
			  | 2       | 383080 | Will LLC      | 10001 | B1-86481 | 3          | 35.99     | 107.97  |
			  | 3       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 48         | 55.82     | 2679.36 |
			  | 4       | 412290 | Jerde-Hilpert | 10005 | S1-82801 | 21         | 13.62     | 286.02  |
			  | 5       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 9          | 92.55     | 832.95  |
			  | 6       | 412290 | Jerde-Hilpert | 10005 | S1-47412 | 44         | 78.91     | 3472.04 |
			  | 7       | 412290 | Jerde-Hilpert | 10005 | S1-27722 | 36         | 25.42     | 915.12  |
			  | 8       | 218895 | Kulas Inc     | 10006 | S1-27722 | 32         | 95.66     | 3061.12 |
			  | 9       | 218895 | Kulas Inc     | 10006 | B1-33087 | 23         | 22.55     | 518.65  |
			  | 10      | 218895 | Kulas Inc     | 10006 | B1-33364 | 3          | 72.30     | 216.90  |
			  | 11      | 218895 | Kulas Inc     | 10006 | B1-20000 | \-1        | 72.18     | \-72.18 |
			  
			  You can see in the data that the file contains 3 different orders (10001, 10005 and 10006) and that each order consists has multiple products (aka skus).
			  
			  The question we would like to answer is: “What percentage of the order total does each sku represent?”
			  
			  For example, if we look at order 10001 with a total of $576.12, the break down would be:
			  
			  * B1-20000 = $235.83 or 40.9%
			  * S1-27722 = $232.32 or 40.3%
			  * B1-86481 = $107.97 or 18.7%
			  
			  The tricky part in this calculation is that we need to get a total for each order and combine it back with the transaction level detail in order to get the percentages. In Excel, you could try to use some version of a subtotal to try to calculate the values.
			  
			  \#\# First Approach - Merging
			  
			  If you are familiar with pandas, your first inclination is going to be trying to group the data into a new dataframe and combine it in a multi-step process. Here’s what that approach would look like.
			  
			  Import all the modules we need and read in our data:
			  
			  import pandas as pd
			  
			  df = pd.read_excel("sales_transactions.xlsx")
			  
			  Now that the data is in a dataframe, determining the total by order is simple with the help of the standard `groupby` aggregation.
			  
			  df.groupby('order')["ext price"].sum()
			  
			  order
			  10001     576.12
			  10005    8185.49
			  10006    3724.49
			  Name: ext price, dtype: float64
			  
			  Here is a simple image showing what is happening with the standard `groupby` 
			  
			  ![Groupby Example](https://proxy-prod.omnivore-image-cache.app/0x0,sQkUMrHf05Bh1iZJeQzPTxnaeimjWofu5jNwgPtqjQHw/https://pbpython.com/images/groupby-example.png) 
			  
			  The tricky part is figuring out how to combine this data back with the original dataframe. The first instinct is to create a new dataframe with the totals by order and merge it back with the original. We could do something like this:
			  
			  order_total = df.groupby('order')["ext price"].sum().rename("Order_Total").reset_index()
			  df_1 = df.merge(order_total)
			  df_1["Percent_of_Order"] = df_1["ext price"] / df_1["Order_Total"]
			  
			  | account | name   | order         | sku   | quantity | unit price | ext price | order total | Order\_Total | Percent\_of\_Order |            |
			  | ------- | ------ | ------------- | ----- | -------- | ---------- | --------- | ----------- | ------------ | ------------------ | ---------- |
			  | 0       | 383080 | Will LLC      | 10001 | B1-20000 | 7          | 33.69     | 235.83      | 576.12       | 576.12             | 0.409342   |
			  | 1       | 383080 | Will LLC      | 10001 | S1-27722 | 11         | 21.12     | 232.32      | 576.12       | 576.12             | 0.403249   |
			  | 2       | 383080 | Will LLC      | 10001 | B1-86481 | 3          | 35.99     | 107.97      | 576.12       | 576.12             | 0.187409   |
			  | 3       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 48         | 55.82     | 2679.36     | 8185.49      | 8185.49            | 0.327330   |
			  | 4       | 412290 | Jerde-Hilpert | 10005 | S1-82801 | 21         | 13.62     | 286.02      | 8185.49      | 8185.49            | 0.034942   |
			  | 5       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 9          | 92.55     | 832.95      | 8185.49      | 8185.49            | 0.101759   |
			  | 6       | 412290 | Jerde-Hilpert | 10005 | S1-47412 | 44         | 78.91     | 3472.04     | 8185.49      | 8185.49            | 0.424170   |
			  | 7       | 412290 | Jerde-Hilpert | 10005 | S1-27722 | 36         | 25.42     | 915.12      | 8185.49      | 8185.49            | 0.111798   |
			  | 8       | 218895 | Kulas Inc     | 10006 | S1-27722 | 32         | 95.66     | 3061.12     | 3724.49      | 3724.49            | 0.821890   |
			  | 9       | 218895 | Kulas Inc     | 10006 | B1-33087 | 23         | 22.55     | 518.65      | 3724.49      | 3724.49            | 0.139254   |
			  | 10      | 218895 | Kulas Inc     | 10006 | B1-33364 | 3          | 72.30     | 216.90      | 3724.49      | 3724.49            | 0.058236   |
			  | 11      | 218895 | Kulas Inc     | 10006 | B1-20000 | \-1        | 72.18     | \-72.18     | 3724.49      | 3724.49            | \-0.019380 |
			  
			  This certainly works but there are several steps needed to get the data combined in the manner we need.
			  
			  \#\# Second Approach - Using Transform
			  
			  Using the original data, let’s try using `transform` and `groupby` and see what we get:
			  
			  df.groupby('order')["ext price"].transform('sum')
			  
			  0      576.12
			  1      576.12
			  2      576.12
			  3     8185.49
			  4     8185.49
			  5     8185.49
			  6     8185.49
			  7     8185.49
			  8     3724.49
			  9     3724.49
			  10    3724.49
			  11    3724.49
			  dtype: float64
			  
			  You will notice how this returns a different size data set from our normal `groupby`functions. Instead of only showing the totals for 3 orders, we retain the same number of items as the original data set. That is the unique feature of using `transform`.
			  
			  The final step is pretty simple:
			  
			  df["Order_Total"] = df.groupby('order')["ext price"].transform('sum')
			  df["Percent_of_Order"] = df["ext price"] / df["Order_Total"]
			  
			  | account | name   | order         | sku   | quantity | unit price | ext price | order total | Order\_Total | Percent\_of\_Order |            |
			  | ------- | ------ | ------------- | ----- | -------- | ---------- | --------- | ----------- | ------------ | ------------------ | ---------- |
			  | 0       | 383080 | Will LLC      | 10001 | B1-20000 | 7          | 33.69     | 235.83      | 576.12       | 576.12             | 0.409342   |
			  | 1       | 383080 | Will LLC      | 10001 | S1-27722 | 11         | 21.12     | 232.32      | 576.12       | 576.12             | 0.403249   |
			  | 2       | 383080 | Will LLC      | 10001 | B1-86481 | 3          | 35.99     | 107.97      | 576.12       | 576.12             | 0.187409   |
			  | 3       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 48         | 55.82     | 2679.36     | 8185.49      | 8185.49            | 0.327330   |
			  | 4       | 412290 | Jerde-Hilpert | 10005 | S1-82801 | 21         | 13.62     | 286.02      | 8185.49      | 8185.49            | 0.034942   |
			  | 5       | 412290 | Jerde-Hilpert | 10005 | S1-06532 | 9          | 92.55     | 832.95      | 8185.49      | 8185.49            | 0.101759   |
			  | 6       | 412290 | Jerde-Hilpert | 10005 | S1-47412 | 44         | 78.91     | 3472.04     | 8185.49      | 8185.49            | 0.424170   |
			  | 7       | 412290 | Jerde-Hilpert | 10005 | S1-27722 | 36         | 25.42     | 915.12      | 8185.49      | 8185.49            | 0.111798   |
			  | 8       | 218895 | Kulas Inc     | 10006 | S1-27722 | 32         | 95.66     | 3061.12     | 3724.49      | 3724.49            | 0.821890   |
			  | 9       | 218895 | Kulas Inc     | 10006 | B1-33087 | 23         | 22.55     | 518.65      | 3724.49      | 3724.49            | 0.139254   |
			  | 10      | 218895 | Kulas Inc     | 10006 | B1-33364 | 3          | 72.30     | 216.90      | 3724.49      | 3724.49            | 0.058236   |
			  | 11      | 218895 | Kulas Inc     | 10006 | B1-20000 | \-1        | 72.18     | \-72.18     | 3724.49      | 3724.49            | \-0.019380 |
			  
			  As an added bonus, you could combine into one statement if you did not want to show the individual order totals:
			  
			  df["Percent_of_Order"] = df["ext price"] / df.groupby('order')["ext price"].transform('sum')
			  
			  Here is a diagram to show what is happening:
			  
			  ![Groupby Example](https://proxy-prod.omnivore-image-cache.app/0x0,sXuQ6Gv26LmS_iTOSWy96XCBa44L7e1QqBHHHR8s45nQ/https://pbpython.com/images/transform-example.png) 
			  
			  After taking the time to understand `transform`, I think you will agree that this tool can be very powerful - even if it is a unique approach as compared to the standard Excel mindset.
			  
			  \#\# Conclusion
			  
			  I am continually amazed at the power of pandas to make complex numerical manipulations very efficient. Despite working with pandas for a while, I never took the time to figure out how to use `transform.` Now that I understand how it works, I am sure I will be able to use it in future analysis and hope that you will find this useful as well.
	- A glute imbalance can lead to injuries—here's how to fix it | Well+Good](https://omnivore.app/me/a-glute-imbalance-can-lead-to-injuries-here-s-how-to-fix-it-well-18bc48d053a)
	  collapsed:: true
	  site:: [Well+Good](https://www-wellandgood-com.cdn.ampproject.org/v/s/www.wellandgood.com/good-sweat/glute-imbalance/amp/?_gsa=1&amp_js_v=a2&usqp=mq331AQCKAE%3D)
	  author:: Rachel Lapidos
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 27th, 2019]]
		- ### Content
			- ![glute imbalance](https://proxy-prod.omnivore-image-cache.app/0x0,s0K0MF8KNa4FgARLrOdKawOC_XXWngSX4Q68ZbkjsvGU/https://www-wellandgood-com.cdn.ampproject.org/i/s/www.wellandgood.com/wp-content/uploads/2019/11/GettyImages-Khosrork_falsexfalse_true_70.webp) 
			  
			  Photo: Getty Images/Khosrork 
			  
			  A fun fitness fact that you’ve likely realized during your workouts: One side of your body tends to be a [little stronger than the other](https://www.wellandgood.com/muscle-imbalance/). In most cases, it’s totally normal and fine, and just means that lifting a 20-pound weight on your right might be a little easier than doing it on your left. But muscle imbalances in your [glutes,](https://www.wellandgood.com/best-exercises-for-glutes/) which are _very_ common, can lead to pain and even injury.
			  
			  In fact, the majority of people are dealing with some sort of imbalance in their behind, and it’s got nothing to do with their workout routines. “If you have a sedentary occupation—which 80 percent of people do—you can guarantee that your glutes are under active,” says Steve Stonehouse, NASM, director of education for [Stride](https://www.runwithstride.com/). “Along with being under active, there’s a high likelihood you have some glute imbalance as well.”
			  
			  This, pros caution, can lead to a slew of issues throughout your body. “You could deal with a host of injuries tied to these type of imbalances,” says Stonehouse. “The kinetic chain concept basically states that nothing works in isolation. A nagging knee injury could be more of a hip/glute issue than an actual knee issue. The same applies with the ankles or even shoulder issues.” Glute imbalances can even be causing you [lower back pain, ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4713798/)which no one wants to deal with.
			  
			  To find out for sure if this applies to you, Jackie Vick, NCFS, [Gold’s Gym](https://www.goldsgym.com/) personal trainer, suggests testing yourself with unilateral and bilateral movements. “For a unilateral, try a single-leg deadlift. Did one hip rotate? For a bilateral, try a squat and see if there’s a shift in the hips. Is one lower?” she says. If you answered “yes” to all of the above questions, it’s a good idea to work on balancing your butt muscles to avoid problems down the line.
			  
			  \#\#\#\# Related Stories
			  
			  [ ![](https://proxy-prod.omnivore-image-cache.app/0x0,sZgmZIYycnHshyfaRCVV19Uf-LuZWbQuuNmOJeI8uMwo/https://www-wellandgood-com.cdn.ampproject.org/i/s/www.wellandgood.com/wp-content/uploads/2020/06/Stocksy-Ivan-Gener-knee-pain-from-lunges-425x285.jpg) ](https://www.wellandgood.com/knee-pain-from-lunges/?itm%5Fsource=parsely-api)
			  
			  First, Stonehouse suggests figuring out what’s causing the imbalance. “Ninety percent of the time, an imbalance is caused by the opposing muscle being overactive,” he says. In the case of your glutes, that would be your hip flexors “That causes what’s called an altered reciprocal inhibition, where one muscle is preventing another muscle from doing its job properly.” In this sitch, you can address the problem with a [kneeling hip-flexor stretch](https://www.youtube.com/watch?v=NBjhd3RftOY).
			  
			  To get to the glutes themselves, Vick recommends isometric work. “The best exercises for balance are all about loading the weaker glute,” she says. “Focus on isometric contractions that get the weaker side firing correctly,” adds Stonehouse. Keep scrolling for the glute-balancing exercises to try to—ahem—round out the muscles in question.
			  
			  [![](https://proxy-prod.omnivore-image-cache.app/0x0,s9H0T-p4ubloOiShjBYZfllEH3XAo8gr_FbG6ucqVr2I/https://i-ytimg-com.cdn.ampproject.org/i/s/i.ytimg.com/vi/irLFxL49AvU/hqdefault.jpg)](https://www.youtube.com/watch?v=irLFxL49AvU)
			  
			  **1\. Single leg raises:** Vick recommends doing single-leg raises. Hold three seconds before releasing to the next step.
			  
			  [![](https://proxy-prod.omnivore-image-cache.app/0x0,sfX-ucrBDGzYmdRipdLKCiz-Km5CBGl2dGmmzq_t5sI0/https://i-ytimg-com.cdn.ampproject.org/i/s/i.ytimg.com/vi/ZVfcRHhcBgg/hqdefault.jpg)](https://www.youtube.com/watch?v=ZVfcRHhcBgg)
			  
			  **2\. Fire hydrants:** Raise one leg out to the side from a quadruped position. Hold at the top for three seconds.
			  
			  [![](https://proxy-prod.omnivore-image-cache.app/0x0,sZVxPdiQ6aL9kHIhonRCJl3W1C-TMtF_0s97pH_J2xKc/https://i-ytimg-com.cdn.ampproject.org/i/s/i.ytimg.com/vi/EG5_gXcfozw/hqdefault.jpg)](https://www.youtube.com/watch?v=EG5%5FgXcfozw)
			  
			  **3\. Clam shells:** Lie on one side, with the weaker glute on top. Raise the top leg to lift into a clam shell. Hold three seconds before releasing.
			  
			  [![](https://proxy-prod.omnivore-image-cache.app/0x0,sk7VDz7CrBCf8vO3qrT-NFRFEetxlf79s_BYKGPUYBm4/https://i-ytimg-com.cdn.ampproject.org/i/s/i.ytimg.com/vi/nnu1egtB0eg/hqdefault.jpg)](https://www.youtube.com/watch?v=nnu1egtB0eg)
			  
			  **4\. Banded side steps:** “Stay low in a quarter of a squat and step 10 to 20 steps, then rest,” says Vick. Repeat two to four times.  
			  
			  [![](https://proxy-prod.omnivore-image-cache.app/0x0,sz2cwkC-syzLY1AwUMczoBJaH1bD8fakU-mczYVySUxU/https://i-ytimg-com.cdn.ampproject.org/i/s/i.ytimg.com/vi/wPM8icPu6H8/hqdefault.jpg)](https://www.youtube.com/watch?v=wPM8icPu6H8)
			  
			  **5\. Glute bridge:** Stonehouse recommends doing a set of bridges before a leg workout to get your glute muscles properly firing. Do 10 reps of 10 second holds while squeezing your glutes as tightly as possible.
			  
			  **Oh, and here’s what to know about [yoga butt](https://www.wellandgood.com/what-is-yoga-butt-hamstring-injury/), another glutes-related muscle imbalance. Also useful: This [dynamic hamstring stretch](https://www.wellandgood.com/dynamic-hamstring-stretch/) called the “waterfall” to open up your legs after all that glute work.** 
			  
			  \#\#\#\# Continue Reading
			  
			  \#\#\#\# Continue Reading
	- Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://omnivore.app/me/get-started-with-py-spark-and-jupyter-notebook-in-3-minutes-18bc48d00fb)
	  collapsed:: true
	  site:: [sicara.ai](https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes)
	  author:: Rédigé par Charles Bochet
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[May 1st, 2017]]
		- ### Content
			- 1. [Why use PySpark in a Jupyter Notebook?](\#8dqfk)
			  2. [Install pySpark](\#brm4m)
			  3. [Install Jupyter Notebook](\#5at5f)
			  4. [Your first Python program on Spark](\#bd5vd)
			  5. [PySpark in Jupyter](\#f8tkn)
			  
			  **Spark is a fast and powerful framework.**
			  
			  **[Apache Spark](http://spark.apache.org/) is a must for [Big data](https://www.sicara.ai/fr/parlons-data/big-data-histoire-application)’s lovers**. In a few words, Spark is a fast and powerful framework that provides an API to perform massive distributed processing over resilient sets of data.
			  
			  [Jupyter Notebook](http://jupyter.org/) is a popular application that enables you to edit, run and share Python code into a web view. It allows you to modify and re-execute parts of your code in a very flexible way. **That’s why Jupyter is a great tool to test and prototype programs.**
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,staoJCAK_ZyK7VkOOa2kyx__I-w5ER4d6leQIsi7uATw/https://images.storychief.com/account_16771/2_cfc23b707223dc97309270fa46a82df3_800.png) 
			  
			  Jupyter Notebook running Python code
			  
			  I wrote this article for Linux users but I am sure Mac OS users can benefit from it too.
			  
			  \#\# Why use PySpark in a Jupyter Notebook?
			  
			  While using Spark, most [data engineers](https://www.sicara.ai/fr/parlons-data/devenir-data-engineer) recommends to develop either in Scala (which is the “native” Spark language) or in Python through complete [PySpark API](http://spark.apache.org/docs/latest/api/python/index.html).
			  
			  Python for Spark is obviously slower than Scala. However like many developers, I love Python because it’s flexible, robust, easy to learn, and benefits from all my favorites libraries. In my opinion, **Python is the perfect language for prototyping in Big Data/Machine Learning fields.**
			  
			  If you prefer to develop in Scala, you will find many alternatives on the following github repository: [alexarchambault/jupyter-scala](https://github.com/alexarchambault/jupyter-scala\#comparison-to-alternatives)
			  
			  To learn more about Python vs. Scala pro and cons for Spark context, please refer to this interesting article: [Scala vs. Python for Apache Spark](https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213).
			  
			  Now, let’s get started.
			  
			  ---
			  
			  \#\# Install pySpark
			  
			  Before installing pySpark, you must have Python and Spark installed. I am using Python 3 in the following examples but you can easily adapt them to Python 2\. Go to the [Python official website](https://www.python.org/) to install it. I also encourage you to set up a [virtualenv](https://virtualenvwrapper.readthedocs.io/en/latest/)
			  
			  To install Spark, make sure you have [Java 8 or higher installed on your computer](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install%5Foverview.html). Then, visit the [Spark downloads page](http://spark.apache.org/downloads.html). Select the latest Spark release, a prebuilt package for Hadoop, and download it directly.
			  
			  Unzip it and move it to your /opt folder:
			  
			  ```angelscript
			  $ tar -xzf spark-1.2.0-bin-hadoop2.4.tgz$ mv spark-1.2.0-bin-hadoop2.4 /opt/spark-1.2.0
			  ```
			  
			  Create a symbolic link:
			  
			  ```vim
			  $ ln -s /opt/spark-1.2.0 /opt/spark̀
			  ```
			  
			  This way, you will be able to download and use multiple Spark versions.
			  
			  Finally, tell your bash (or zsh, etc.) where to find Spark. To do so, configure your $PATH variables by adding the following lines in your `~/.bashrc` (or `~/.zshrc`) file:
			  
			  ```routeros
			  export SPARK_HOME=/opt/sparkexport PATH=$SPARK_HOME/bin:$PATH
			  ```
			  
			  \#\# Install Jupyter Notebook
			  
			  Install Jupyter notebook:
			  
			  ```cmake
			  $ pip install jupyter
			  ```
			  
			  You can run a regular jupyter notebook by typing:
			  
			  ```elixir
			  $ jupyter notebook
			  ```
			  
			  \#\# Your first Python program on Spark
			  
			  Let’s check if PySpark is properly installed without using Jupyter Notebook first.
			  
			  You may need to restart your terminal to be able to run PySpark. Run:
			  
			  ```maxima
			  $ pysparkWelcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  '_/   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0      /_/Using Python version 3.5.2 (default, Jul  2 2016 17:53:06)SparkSession available as 'spark'.>>>
			  ```
			  
			  It seems to be a good start! Run the following program: _(I bet you understand what it does!)_
			  
			  ```gml
			  import randomnum_samples = 100000000def inside(p):       x, y = random.random(), random.random()  return x*x + y*y < 1count = sc.parallelize(range(0, num_samples)).filter(inside).count()pi = 4 * count / num_samplesprint(pi)sc.stop()
			  ```
			  
			  The output will probably be around **`3.14`.**
			  
			  \#\# PySpark in Jupyter
			  
			  There are two ways to get PySpark available in a Jupyter Notebook:
			  
			  * Configure PySpark driver to use Jupyter Notebook: running `pyspark `will automatically open a Jupyter Notebook
			  * Load a regular Jupyter Notebook and load PySpark using findSpark package
			  
			  First option is quicker but specific to Jupyter Notebook, second option is a broader approach to get PySpark available in your favorite IDE.
			  
			  \#\#\# Method 1 — Configure PySpark driver
			  
			  Update PySpark driver environment variables: add these lines to your `~/.bashrc` (or `~/.zshrc`) file.
			  
			  ```routeros
			  export PYSPARK_DRIVER_PYTHON=jupyterexport PYSPARK_DRIVER_PYTHON_OPTS='notebook'
			  ```
			  
			  Restart your terminal and launch PySpark again:
			  
			  ```elixir
			  $ pyspark
			  ```
			  
			  Now, this command should start a Jupyter Notebook in your web browser. Create a new notebook by clicking on ‘New’ > ‘Notebooks Python \[default\]’.
			  
			  Copy and paste our Pi calculation script and run it by pressing Shift + Enter.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sYd_XsiAHaHUfqpUWP8dtVHGpi3fHoTEmeOUQwNaLAP0/https://images.storychief.com/account_16771/3_6da5a61d2805fa9d930c3e176a8db9cc_800.png) 
			  
			  Jupyter Notebook: Pi Calculation script
			  
			  \#\#\#\# Done! 
			  
			  You are now able to run PySpark in a Jupyter Notebook :)
			  
			  \#\#\# Method 2 — FindSpark package
			  
			  There is another and more generalized way to use PySpark in a Jupyter Notebook: use [findSpark](https://github.com/minrk/findspark) package to make a Spark Context available in your code.
			  
			  findSpark package is not specific to Jupyter Notebook, you can use this trick in your favorite IDE too.
			  
			  To install findspark:
			  
			  ```cmake
			  $ pip install findspark
			  ```
			  
			  Launch a regular Jupyter Notebook:
			  
			  ```elixir
			  $ jupyter notebook
			  ```
			  
			  Create a new Python \[default\] notebook and write the following script:
			  
			  ```gml
			  import findsparkfindspark.init()import pysparkimport randomsc = pyspark.SparkContext(appName="Pi")num_samples = 100000000def inside(p):       x, y = random.random(), random.random()  return x*x + y*y < 1count = sc.parallelize(range(0, num_samples)).filter(inside).count()pi = 4 * count / num_samplesprint(pi)sc.stop()
			  ```
			  
			  The output should be:
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s3rJVYEaUoir2OpveEwj4Or3Cbedxb2hoWD2srd60b4c/https://images.storychief.com/account_16771/4_b9a2a72401c52b8d1804b577c31b3166_800.png) 
			  
			  Jupyter Notebook: Pi calculation
			  
			  ---
			  
			  _I hope this 3-minutes guide will help you easily getting started with Python and Spark. Here are a few resources if you want to go the extra mile:_
			  
			  * <https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213>
			  * <http://queirozf.com/entries/comparing-interactive-solutions-for-running-apache-spark-zeppelin-spark-notebook-and-jupyter-scala>
			  * <http://spark.apache.org/docs/latest/api/python/index.html>
			  * <https://github.com/jadianes/spark-py-notebooks>I
			  
			  And if you want to tackle some bigger challenges, don't miss out the more evolved JupyterLab environnement or the PyCharm integration of jupyter notebooks.
			  
			  If you are looking for Data engineering expert's, don't hesitate to [contact us](https://www.sicara.ai/en/contact) ! 
			  
			  ---
			  
			  **Thanks to Pierre-Henri Cumenge, Antoine Toubhans, Adil Baaj, Vincent Quagliaro, and Adrien Lina.**
			  
			  \#\#  Cet article a été écrit par
			  
			  ![Charles Bochet](https://proxy-prod.omnivore-image-cache.app/0x0,sE29PRPRKrWXfwCU3rxDaZnqziaM_mG2P5jG0MCFq2tY/https://d37oebn0w9ir6a.cloudfront.net/user_40909/Sicara-charlesb_aa3f5b4922feb150dd7cb11132ac0a60.jpg) 
			  
			  Charles Bochet
	- 20 Linux commands every sysadmin should know | Opensource.com](https://omnivore.app/me/20-linux-commands-every-sysadmin-should-know-opensource-com-18bc48cfacc)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/17/7/20-sysadmin-commands)
	  author:: Rosemary Wang
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Oct 13th, 2020]]
		- ### Content
			- ![Avoiding data disasters with Sanoid](https://proxy-prod.omnivore-image-cache.app/520x292,s41Ow8PxZkpWB5_NDIg3F_4H7W95M7uMaTHcR-kxzy_A/https://opensource.com/sites/default/files/lead-images/rh_003499_01_linux11x_cc.png "Avoiding data disasters with Sanoid") 
			  
			  In a world bursting with new tools and diverse development environments, it's practically a necessity for any developer or engineer to learn some basic sysadmin commands. Specific commands and packages can help developers organize, troubleshoot, and optimize their applications and—when things go wrong—provide valuable triage information to operators and sysadmins.
			  
			  Whether you are a new developer or want to manage your own application, the following 20 basic sysadmin commands can help you better understand your applications. They can also help you describe problems to sysadmins troubleshooting why an application might work locally but not on a remote host. These commands apply to Linux development environments, containers, virtual machines (VMs), and bare metal.
			  
			  \#\# 1\. curl
			  
			  **curl** transfers a URL. [Use this command](https://opensource.com/article/20/5/curl-cheat-sheet) to test an application's endpoint or connectivity to an upstream service endpoint. **c** **url** can be useful for determining if your application can reach another service, such as a database, or checking if your service is healthy.
			  
			  As an example, imagine your application throws an HTTP 500 error indicating it can't reach a MongoDB database:
			  
			  `$ curl -I -s myapplication:5000
			  HTTP/1.0 500 INTERNAL SERVER ERROR`
			  
			  The **\-I** option shows the header information and the **\-s** option silences the response body. Checking the endpoint of your database from your local desktop:
			  
			  `$ curl -I -s database:27017
			  HTTP/1.0 200 OK`
			  
			  So what could be the problem? Check if your application can get to other places besides the database from the application host:
			  
			  `$ curl -I -s https://opensource.com
			  HTTP/1.1 200 OK`
			  
			  That seems to be okay. Now try to reach the database from the application host. Your application is using the database's hostname, so try that first:
			  
			  `$ curl database:27017
			  curl: (6) Couldn't resolve host 'database'`
			  
			  This indicates that your application cannot resolve the database because the URL of the database is unavailable or the host (container or VM) does not have a nameserver it can use to resolve the hostname.
			  
			  \#\# 2\. python -m json.tool / jq
			  
			  After you issue **curl**, the output of the API call may be difficult to read. Sometimes, you want to [pretty-print](https://en.wikipedia.org/wiki/Prettyprint) the JSON output to find a specific entry. Python has a built-in JSON library that can help with this. You use **python -m json.tool** to indent and organize the JSON. To use Python's JSON module, pipe the output of a JSON file into the **python -m json.tool** command.
			  
			  `$ cat test.json
			  {"title":"Person","type":"object","properties":{"firstName":{"type":"string"},"lastName":{"type":"string"},"age":{"description":"Age in years","type":"integer","minimum":0}},"required":["firstName","lastName"]}`
			  
			  To use the Python library, pipe the output to Python with the **\-m** (module) option.
			  
			  `$ cat test.json | python -m json.tool
			  {
			    "properties": {
			        "age": {
			            "description": "Age in years",
			            "minimum": 0,
			            "type": "integer"
			        },
			        "firstName": {
			            "type": "string"
			        },
			        "lastName": {
			            "type": "string"
			        }
			    },
			    "required": [
			        "firstName",
			        "lastName"
			    ],
			    "title": "Person",
			    "type": "object"
			  }`
			  
			  For more advanced JSON parsing, you can [install **jq**](https://stedolan.github.io/jq/download/). **j** **q** provides some [options](https://stedolan.github.io/jq/manual/v1.5/) that extract specific values from the JSON input. To pretty-print like the Python module above, simply apply **jq** to the output.
			  
			  `$ cat test.json | jq
			  {
			  "title": "Person",
			  "type": "object",
			  "properties": {
			    "firstName": {
			      "type": "string"
			    },
			    "lastName": {
			      "type": "string"
			    },
			    "age": {
			      "description": "Age in years",
			      "type": "integer",
			      "minimum": 0
			    }
			  },
			  "required": [
			    "firstName",
			    "lastName"
			  ]
			  }`
			  
			  \#\# 3\. ls
			  
			  **ls** lists files in a directory. Sysadmins and developers issue [this command](https://opensource.com/article/19/7/master-ls-command) quite often. In the container space, this command can help determine your container image's directory and files. Besides looking up your files, **ls** can help you examine your permissions. In the example below, you can't run myapp because of a permissions issue. When you check the permissions using **ls -l**, you realize that the permissions do not have an "x" in **\-rw-r--r--**, which are read and write only.
			  
			  `$ ./myapp
			  bash: ./myapp: Permission denied
			  $ ls -l myapp
			  -rw-r--r--. 1 root root 33 Jul 21 18:36 myapp`
			  
			  \#\# 4\. tail
			  
			  **tail** displays the last part of a file. You usually don't need every log line to troubleshoot. Instead, you want to check what your logs say about the most recent request to your application. For example, you can use **tail** to check what happens in the logs when you make a request to your [Apache HTTP server](https://httpd.apache.org/).
			  
			  ![Use tail -f to follow Apache HTTP server logs and see the requests as they happen.](https://proxy-prod.omnivore-image-cache.app/800x321,sVq0_Rk0Y3YjWg_N9M8iKlas89-_xLz1v-vHGhUwPWs8/https://opensource.com/sites/default/files/u128651/example_tail.png)
			  
			  The **\-f** option indicates the "follow" option, which outputs the log lines as they are written to the file. The example has a background script that accesses the endpoint every few seconds and the log records the request. Instead of following the log in real time, you can also use **tail** to see the last 100 lines of the file with the **\-n** option.
			  
			  `$ tail -n 100 /var/log/httpd/access_log`
			  
			  \#\# 5\. cat
			  
			  **cat** concatenates and prints files. [You might issue **cat**](https://opensource.com/article/19/2/getting-started-cat-command) to check the contents of your dependencies file or to confirm the version of the application that you have already built locally.
			  
			  `$ cat requirements.txt
			  flask
			  flask_pymongo`
			  
			  The example above checks whether your Python Flask application has Flask listed as a dependency.
			  
			  \#\# 6\. grep
			  
			  **grep** searches file patterns. If you are looking for a specific pattern in the output of another command, **grep** highlights the relevant lines. Use this command for searching log files, specific processes, and more. If you want to see if Apache Tomcat starts up, you might become overwhelmed by the number of lines. By piping that output to the **grep** command, you isolate the lines that indicate server startup.
			  
			  `$ cat tomcat.log | grep org.apache.catalina.startup.Catalina.start
			  01-Jul-2017 18:03:47.542 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 681 ms`
			  
			  \#\# 7\. ps
			  
			  The **ps** command, part of the procps-ng package which provides useful commands for investigating process IDs, shows the status of a running process. Use this command to determine a running application or confirm an expected process. For example, if you want to check for a running Tomcat web server, you use **ps** with its options to obtain the process ID of Tomcat.
			  
			  `$ ps -ef
			  UID        PID  PPID  C STIME TTY          TIME CMD
			  root         1     0  2 18:55 ?        00:00:02 /docker-java-home/jre/bi
			  root        59     0  0 18:55 pts/0    00:00:00 /bin/sh
			  root        75    59  0 18:57 pts/0    00:00:00 ps -ef`
			  
			  For even more legibility, use **ps** and pipe it to **grep**.
			  
			  `$ ps -ef | grep tomcat
			  root         1     0  1 18:55 ?        00:00:02 /docker-java-home/jre/bi`
			  
			  \#\# 8\. env
			  
			  **env** allows you to set or print the environment variables. During troubleshooting, you may find it useful for checking if the wrong environment variable prevents your application from starting. In the example below, this command is used to check the environment variables set on your application's host.
			  
			  `$ env
			  PYTHON_PIP_VERSION=9.0.1
			  HOME=/root
			  DB_NAME=test
			  PATH=/usr/local/bin:/usr/local/sbin
			  LANG=C.UTF-8
			  PYTHON_VERSION=3.4.6
			  PWD=/
			  DB_URI=mongodb://database:27017/test`
			  
			  Notice that the application is using Python and has environment variables to connect to a MongoDB database.
			  
			  \#\# 9\. top
			  
			  **top** displays and updates sorted process information. Use this [monitoring tool](https://opensource.com/life/16/2/open-source-tools-system-monitoring) to determine which processes are running and how much memory and CPU they consume. A common case occurs when you run an application and it dies a minute later. First, you check the application's return error, which is a memory error.
			  
			  `$ tail myapp.log
			  Traceback (most recent call last):
			  MemoryError`
			  
			  Is your application _really_ out of memory? To confirm, use **top** to determine how much CPU and memory your application consumes. When issuing **top**, you notice a Python application using most of the CPU, with its memory usage climbing, and suspect it is your application. While it runs, you hit the "C" key to see the full command and reverse-engineer if the process is your application. It turns out to be your memory-intensive application (**memeater.py**). When your application has run out of memory, the system kills it with an out-of-memory (OOM) error.
			  
			  ![Issuing top against an application that consumes all of its memory.](https://proxy-prod.omnivore-image-cache.app/800x332,szvmwXWDLudJ4TPnmscj8VxhCdmuOfso3l6GWaVY1P2w/https://opensource.com/sites/default/files/u128651/example_top.png)
			  
			  ![Pressing C while running top shows the full command](https://proxy-prod.omnivore-image-cache.app/800x290,sygCUuZoOK4csxg-J7_CMij2g-YsrNxwGKh9d-UE8QEU/https://opensource.com/sites/default/files/u128651/example_topwithc.png)
			  
			  In addition to checking your own application, you can use **top** to debug other processes that utilize CPU or memory.
			  
			  \#\# 10\. netstat
			  
			  **netstat** shows the network status. This command shows network ports in use and their incoming connections. However, **netstat** does not come out-of-the-box on Linux. If you need to install it, you can find it in the **[net-tools](https://wiki.linuxfoundation.org/networking/net-tools)** package. As a developer who experiments locally or pushes an application to a host, you may receive an error that a port is already allocated or an address is already in use. Using **netstat** with protocol, process and port options demonstrates that Apache HTTP server already uses port 80 on the below host.
			  
			  ![netstat verifies that Apache is running on port 80](https://proxy-prod.omnivore-image-cache.app/800x284,s0hlYFl3m9ZMKIvzfHtB4_KPOq-xeMfxoLyogfOzLRxw/https://opensource.com/sites/default/files/u128651/example_netstat.png)
			  
			  \#\# 11\. ip
			  
			  If **ip** address does not work on your host, it must be installed with the [**iproute2**](https://wiki.linuxfoundation.org/networking/iproute2) package. The subcommand **address** (or just **ip a** for short) shows the interfaces and IP addresses of your application's host. You use **ip address** to verify your container or host's IP address. For example, when your container is attached to two networks, **ip address** can show which interface connects to which network. For a simple check, you can always use the **ip address** command to get the IP address of the host. The example below shows that the web tier container has an IP address of 172.17.0.2 on interface eth0.
			  
			  ```routeros
			  $ ip address show eth0
			  3: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
			    link/ether d4:3b:04:9e:b2:c2 brd ff:ff:ff:ff:ff:ff
			    inet 10.1.1.3/27 brd 10.1.1.31 scope global dynamic noprefixroute eth0
			       valid_lft 52072sec preferred_lft 52072sec
			  
			  ```
			  
			  \#\# 12\. lsof
			  
			  **lsof** lists the open files associated with your application. On some Linux machine images, you need to install **lsof** with the [lsof](http://www.linuxfromscratch.org/blfs/view/svn/general/lsof.html) package. In Linux, almost any interaction with the system is treated like a file. As a result, if your application writes to a file or opens a network connection, **lsof** will reflect that interaction as a file. Similar to **netstat**, you can use **lsof** to check for listening ports. For example, if you want to check if port 80 is in use, you use **lsof** to check which process is using it. Below, you can see that httpd (Apache) listens on port 80\. You can also use **lsof** to check the process ID of httpd, examining where the web server's binary resides (**/usr/sbin/httpd**).
			  
			  ![lsof reveals the origin of process information](https://proxy-prod.omnivore-image-cache.app/800x252,syCmAuURyi78lQRB7VQcaTwwNKXPuLFoM3Fqrcvxcj1k/https://opensource.com/sites/default/files/u128651/example_lsof.png)
			  
			  The name of the open file in the list of open files helps pinpoint the origin of the process, specifically Apache.
			  
			  \#\# 13\. df
			  
			  You can use **df** (display free disk space) to troubleshoot disk space issues. When you run your application on a container orchestrator, you might receive an error message signaling a lack of free space on the container host. While disk space should be managed and optimized by a sysadmin, you can use **df** to figure out the existing space in a directory and confirm if you are indeed out of space.
			  
			  ```angelscript
			  $ df -h
			  Filesystem            Size  Used Avail Use% Mounted on
			  devtmpfs              7.7G     0  7.7G   0% /dev
			  /dev/mapper/RHEL-Root  50G   16G   35G  31% /
			  /dev/nvme0n1p2        3.0G  246M  2.8G   9% /boot
			  /dev/mapper/RHEL-Home 100G   88G   13G  88% /home
			  /dev/nvme0n1p1        200M  9.4M  191M   5% /boot/efi
			  /dev/sdb1             114G   55G   54G  51% /run/media/tux/red
			  
			  ```
			  
			  The **\-h** option prints out the information in human-readable format. By default, as in the example, **df** provides results for everything under the root directory, but you can also limit results by providing a directory as part of your command (such as **df -h /home**).
			  
			  \#\# 14\. du
			  
			  To retrieve more detailed information about which files use the disk space in a directory, you can use the **du** command. If you wanted to find out which log takes up the most space in the **/var/log** directory, for example, you can use **du** with the **\-h** (human-readable) option and the **\-s** option for the total size.
			  
			  `$ du -sh /var/log/*
			  1.8M  /var/log/anaconda
			  384K  /var/log/audit
			  4.0K  /var/log/boot.log
			  0 /var/log/chrony
			  4.0K  /var/log/cron
			  4.0K  /var/log/maillog
			  64K /var/log/messages`
			  
			  The example above reveals the largest directory under **/var/log** to be **/var/log/audit**. You can use **du** in conjunction with **df** to determine what utilizes the disk space on your application's host.
			  
			  \#\# 15\. id
			  
			  To check the user running the application, use the **id** command to return the user identity. The example below uses [Vagrant](https://www.vagrantup.com/) to test the application and isolate its development environment. After you log into the Vagrant box, if you try to install Apache HTTP Server (a dependency) the system states that you cannot perform the command as root. To check your user and group, issue the **id** command and notice that you are running as the "vagrant" user in the "vagrant" group.
			  
			  `$ dnf -y install httpd
			  Loaded plugins: fastestmirror
			  You need to be root to perform this command.
			  $ id
			  uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023`
			  
			  To correct this, you must run the command as a superuser, which provides elevated privileges.
			  
			  \#\# 16\. chmod
			  
			  When you run your application binary for the first time on your host, you may receive the error message "permission denied." As seen in the example for **ls**, you can check the permissions of your application binary.
			  
			  `$ ls -l
			  total 4
			  -rw-rw-r--. 1 vagrant vagrant 34 Jul 11 02:17 test.sh`
			  
			  This shows that you don't have execution rights (no "x") to run the binary. **c** **hmod** can correct the permissions to enable your user to run the binary.
			  
			  `$ chmod +x test.sh
			  [vagrant@localhost ~]$ ls -l
			  total 4
			  -rwxrwxr-x. 1 vagrant vagrant 34 Jul 11 02:17 test.sh`
			  
			  As demonstrated in the example, this updates the permissions with execution rights. Now when you try to execute your binary, the application doesn't throw a permission-denied error. **Chmod** may be useful when you load a binary into a container as well. It ensures that your container has the correct permissions to execute your binary.
			  
			  \#\# 17\. dig / nslookup
			  
			  A domain name server (DNS) helps resolve a URL to a set of application servers. However, you may find that a URL does not resolve, which causes a connectivity issue for your application. For example, say you attempt to access your database at the **mydatabase** URL from your application's host. Instead, you receive a "cannot resolve" error. To troubleshoot, you try using **dig** (DNS lookup utility) or **nslookup** (query Internet name servers) to figure out why the application can't seem to resolve the database.
			  
			  `$ nslookup mydatabase
			  Server:   10.0.2.3
			  Address:  10.0.2.3\#53
			  
			  ** server can't find mydatabase: NXDOMAIN`
			  
			  Using **nslookup** shows that **mydatabase** can't be resolved. Trying to resolve with **dig** yields the same result.
			  
			  `$ dig mydatabase
			  
			  ; <<>> DiG 9.9.4-RedHat-9.9.4-50.el7_3.1 <<>> mydatabase
			  ;; global options: +cmd
			  ;; connection timed out; no servers could be reached`
			  
			  These errors could be caused by many different issues. If you can't debug the root cause, reach out to your sysadmin for more investigation. For local testing, this issue may indicate that your host's nameservers aren't configured appropriately. To use these commands, you will need to install the [**BIND Utilities**](http://www.linuxfromscratch.org/blfs/view/svn/basicnet/bind-utils.html) package.
			  
			  \#\# 18\. firewall-cmd
			  
			  Traditionally, firewalls were configured on Linux with the **iptables** command, and while it retains its ubiquity it has actually been largely [replaced by **nftables**](https://developers.redhat.com/blog/2016/10/28/what-comes-after-iptables-its-successor-of-course-nftables/). A friendly front-end for [nftables](https://access.redhat.com/documentation/en-us/red%5Fhat%5Fenterprise%5Flinux/8/html/configuring%5Fand%5Fmanaging%5Fnetworking/getting-started-with-nftables%5Fconfiguring-and-managing-networking), and the one that ships with many distributions by default, is **firewall-cmd**. This command helps you set up rules governing what network traffic, both outgoing and incoming, your computer allows. These rules can be grouped into _zones_, so you can quickly and easily move from one set of rules to another, depending on your requirements.
			  
			  The command syntax is straightforward. You use the command and some number of options, all of which are named in ways that help you almost construct a human-readable sentence. For instance, to see what zone you're currently in:
			  
			  ```vim
			  $ sudo firewall-cmd --get-active-zones``
			  corp
			  interfaces: ens0
			  dmz
			  interfaces: ens1
			  ```
			  
			  In this example, your computer has two network devices, and one is assigned to the `corp` zone, while the other is assigned to the `dmz` zone.
			  
			  To see what each zone permits, you can use the `--list-all` option:
			  
			  ```routeros
			  $ sudo firewall-cmd --zone corp --list-all
			  corp
			  target: default
			  interfaces: ens0
			  services: cockpit dhcpv6-client ssh
			  ports: 
			  protocols: 
			  [...]
			  ```
			  
			  Adding services is just as easy:
			  
			  ```dockerfile
			  $ sudo firewall-cmd --add-service http --permanent
			  $ sudo firewall-cmd --reload
			  ```
			  
			  Interacting with **firewall-cmd** is designed to be intuitive, and it has an extensive collection of predifined services, plus the ability to write **nft** rules directly. Once you start using **firewall-cmd**, you can [download our **firewall-cmd** cheat sheet](https://opensource.com/downloads/firewall-cheat-sheet) to help you remember its most important options.
			  
			  \#\# 19\. sestatus
			  
			  You usually find [SELinux](https://opensource.com/article/18/7/sysadmin-guide-selinux) (a Linux security module) enforced on an application host managed by an enterprise. SELinux provides least-privilege access to processes running on the host, preventing potentially malicious processes from accessing important files on the system. In some situations, an application needs to access a specific file but may throw an error. To check if SELinux blocks the application, use **tail** and **grep** to look for a "denied" message in the **/var/log/audit** logging. Otherwise, you can check to see if the box has SELinux enabled by using **sestatus**.
			  
			  `$ sestatus
			  SELinux status:                 enabled
			  SELinuxfs mount:                /sys/fs/selinux
			  SELinux root directory:         /etc/selinux
			  Loaded policy name:             targeted
			  Current mode:                   enforcing
			  Mode from config file:          enforcing
			  Policy MLS status:              enabled
			  Policy deny_unknown status:     allowed
			  Max kernel policy version:      28`
			  
			  The output above indicates that the application's host has SELinux enabled. On your local development environment, you can update SELinux to be more permissive. If you need help with a remote host, your sysadmin can help you determine the best practice for allowing your application to access the file it needs. If you're interacting with SELinux frequently, [download our SELinux cheat sheet](https://opensource.com/downloads/cheat-sheet-selinux) for quick reference.
			  
			  \#\# 20\. history
			  
			  When you issue so many commands for testing and debugging, you may forget the useful ones! Every shell has a variant of the [**history** command](https://opensource.com/article/18/6/history-command). It shows the history of commands you have issued since the start of the session. You can use **history** to log which commands you used to troubleshoot your application. For example, when you issue **history** over the course of this article, it shows the various commands you experimented with and learned.
			  
			  `$ history
			    1  clear
			    2  df -h
			    3  du
			  `
			  
			  What if you want to execute a command in your previous history, but you don't want to retype it? Use **!** before the command number to re-execute.
			  
			  ![Re-execute a command in your history](https://proxy-prod.omnivore-image-cache.app/800x121,s9BbhCoznbNlfWf-6tcZSVu1vkxicOkhJRZ_fROPcUQA/https://opensource.com/sites/default/files/u128651/example_history.png)
			  
			  Basic commands can enhance your troubleshooting expertise when determining why your application works in one development environment but perhaps not in another. Many sysadmins leverage these commands to debug problems with systems. Understanding some of these useful troubleshooting commands can help you communicate with sysadmins and resolve issues with your application.
			  
			  ---
			  
			  _This article was originally published in July 2017 and has been updated by the editor._
			  
			  ![User profile image.](https://proxy-prod.omnivore-image-cache.app/150x150,s9JwuhGxnCTa68jzvgUVqbNvNyRfRpXYq6BOeOEeY5Ew/https://opensource.com/sites/default/files/styles/150x150/public/pictures/wang_rosemary.jpg?itok=fuXQo92R) 
			  
			  As a developer, engineer, and enthusiast of cloud computing and infrastructure automation, Rosemary Wang bridges the technical and cultural barrier between infrastructure engineers and application developers.
			  
			  \#\# Contributors
			  
			  ![Seth Kenlon](https://proxy-prod.omnivore-image-cache.app/150x150,s1jBgolUGi6F_3R_5y2_KYMzIAVUQT64zl-XsmMyTywA/https://opensource.com/sites/default/files/styles/150x150/public/pictures/seth_headshot-lawrence_0.jpg?itok=jZUHBHx4) 
			  
			  \#\# Related Content
			  
			  [ ![Creative Commons License](https://proxy-prod.omnivore-image-cache.app/0x0,s4aXUWHfwSQ2Woh5RX82tXHIVoMMMXKO7tdegsKArCy8/https://opensource.com/themes/osdc/assets/img/cc-by-sa-4.png "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.")](http://creativecommons.org/licenses/by-sa/4.0/)This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.
	- The 5 Classification Evaluation metrics every Data Scientist must know - MLWhiz](https://omnivore.app/me/the-5-classification-evaluation-metrics-every-data-scientist-mus-18bc48cef0b)
	  collapsed:: true
	  site:: [mlwhiz.com](https://mlwhiz.com/blog/2019/11/07/eval_metrics/)
	  author:: Rahul Agarwal
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 6th, 2019]]
		- ### Content
			- By Rahul Agarwal
			  
			  07 November 2019
			  
			  ![The 5 Classification Evaluation metrics every Data Scientist must know](https://proxy-prod.omnivore-image-cache.app/0x0,s4ILKT9TRzcWn2XckV39C60NEyprJw1nQzKZa3eAus6k/https://mlwhiz.com/images/eval/main.jpeg)
			  
			  _**What do we want to optimize for?**_ Most of the businesses fail to answer this simple question.
			  
			  _**Every business problem is a little different, and it should be optimized differently.**_
			  
			  We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. _**But do we really want accuracy as a metric of our model performance?**_
			  
			  _**What if we are predicting the number of asteroids that will hit the earth.**_
			  
			  Just say zero all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable. What should we do in such cases?
			  
			  > _Designing a Data Science project is much more important than the modeling itself._
			  
			  _**This post is about various evaluation metrics and how and when to use them.**_
			  
			  ---
			  
			  \#\# 1\. Accuracy, Precision, and Recall:
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,seZ1-91bG_nhTOscQo43LQqRWJMo2IaKdAXLiTbCi2Uo/https://mlwhiz.com/images/eval/7.png)
			  
			  \#\#\# A. Accuracy
			  
			  Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.
			  
			  Accuracy = (TP+TN)/(TP+FP+FN+TN)
			  
			  Accuracy is the proportion of true results among the total number of cases examined.
			  
			  \#\#\#\# _**When to use?**_
			  
			  Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.
			  
			  \#\#\#\# _**Caveats**_
			  
			  Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? _**What if we are predicting if an asteroid will hit the earth?**_ Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.
			  
			  \#\#\# B. Precision
			  
			  Let’s start with _precision_, which answers the following question: what proportion of **predicted Positives** is truly Positive?
			  
			  Precision = (TP)/(TP+FP)
			  
			  In the asteroid prediction problem, we never predicted a true positive.
			  
			  And thus precision=0
			  
			  \#\#\#\# _**When to use?**_
			  
			  Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.
			  
			  \#\#\#\# _**Caveats**_
			  
			  _Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money._
			  
			  \#\#\# C. Recall
			  
			  Another very useful measure is _recall_, which answers a different question: what proportion of **actual Positives** is correctly classified?
			  
			  Recall = (TP)/(TP+FN)
			  
			  In the asteroid prediction problem, we never predicted a true positive.
			  
			  And thus recall is also equal to 0.
			  
			  \#\#\#\# _**When to use?**_
			  
			  Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.
			  
			  \#\#\#\# _**Caveats**_
			  
			  _**Recall is 1 if we predict 1 for all examples.**_
			  
			  And thus comes the idea of utilizing tradeoff of precision vs. recall — _**F1 Score**_.
			  
			  ---
			  
			  \#\# 2\. F1 Score:
			  
			  This is my _**favorite evaluation metric**_ and I tend to use this a lot in my classification projects.
			  
			  The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,shDFGQF9XUA7J_-UbmGynAUC8GSiNuOtUezBNK-jB1ew/https://mlwhiz.com/images/eval/1.png)
			  
			  Let us start with a binary prediction problem. _**We are predicting if an asteroid will hit the earth or not.**_
			  
			  So if we say “No” for the whole training set. Our precision here is 0\. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.
			  
			  And hence the F1 score is also 0\. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.
			  
			  \#\#\# _**When to use?**_
			  
			  We want to have a model with both good precision and recall.
			  
			  ![Precision-Recall Tradeoff](https://proxy-prod.omnivore-image-cache.app/0x0,s9qGqvzDdNgMPsbZTeWReh7cROVeVcXdIgs8yMtGqGZo/https://mlwhiz.com/images/eval/2.png)
			  
			  Simply stated the _**F1 score sort of maintains a balance between the precision and recall for your classifier**_. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.
			  
			  > \#\# If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
			  
			  \#\#\# How to Use?
			  
			  You can calculate the F1 score for binary prediction problems using:
			  
			  ```angelscript
			  from sklearn.metrics import f1_score
			  y_true = [0, 1, 1, 0, 1, 1]
			  y_pred = [0, 0, 1, 0, 0, 1]
			  
			  f1_score(y_true, y_pred)
			  
			  ```
			  
			  This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.
			  
			  ```mipsasm
			  \# y_pred is an array of predictions
			  def bestThresshold(y_true,y_pred):
			    best_thresh = None
			    best_score = 0
			    for thresh in np.arange(0.1, 0.501, 0.01):
			        score = f1_score(y_true, np.array(y_pred)>thresh)
			        if score > best_score:
			            best_thresh = thresh
			            best_score = score
			    return best_score , best_thresh
			  
			  ```
			  
			  \#\#\#\# Caveats
			  
			  The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.
			  
			  To solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,sJLsqpRFgD5-g2if8R2Nl_hZ3ZkH0sY9qdW9aP_L7A_A/https://mlwhiz.com/images/eval/3.png)
			  
			  Here we give β times as much importance to recall as precision.
			  
			  ```angelscript
			  from sklearn.metrics import fbeta_score
			  
			  y_true = [0, 1, 1, 0, 1, 1]
			  y_pred = [0, 0, 1, 0, 0, 1]
			  
			  fbeta_score(y_true, y_pred,beta=0.5)
			  
			  ```
			  
			  F1 Score can also be used for Multiclass problems. See[this](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)awesome blog post by[Boaz Shmueli](https://mlwhiz.com/blog/2019/11/07/eval%5Fmetrics/undefined)for details.
			  
			  ---
			  
			  \#\# 3\. Log Loss/Binary Crossentropy
			  
			  Log loss is a pretty good evaluation metric for binary classifiers and it is sometimes the optimization objective as well in case of Logistic regression and Neural Networks.
			  
			  Binary Log loss for an example is given by the below formula where p is the probability of predicting 1.
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,shXVw7RcYtHlrPwlOaEMRu2oIy49MMRN9_P7ntRd5cic/https://mlwhiz.com/images/eval/4.png)
			  
			  ![As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1.](https://proxy-prod.omnivore-image-cache.app/0x0,sphhtQY1riuv1fsbBhMPzFW6vw4aIs59OpeCSQ7obCPA/https://mlwhiz.com/images/eval/5.png)
			  
			  _As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1._
			  
			  \#\#\#\# When to Use?
			  
			  _When the output of a classifier is prediction probabilities._ **Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label.** This gives us a more nuanced view of the performance of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.
			  
			  \#\#\#\# How to Use?
			  
			  ```routeros
			  from sklearn.metrics import log_loss  
			  
			  \# where y_pred are probabilities and y_true are binary class labels
			  log_loss(y_true, y_pred, eps=1e-15)
			  
			  ```
			  
			  \#\#\#\# Caveats
			  
			  It is susceptible in case of[imbalanced](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c)datasets. You might have to introduce class weights to penalize minority errors more or you may use this after balancing your dataset.
			  
			  ---
			  
			  \#\# 4\. Categorical Crossentropy
			  
			  The log loss also generalizes to the multiclass problem. The classifier in a multiclass setting must assign a probability to each class for all examples. If there are N samples belonging to M classes, then the _Categorical Crossentropy_ is the summation of -ylogp values:
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,sx8KobXtXWDPPZoWTs6NbFqQkePemTYHvLd3zh_XGdS8/https://mlwhiz.com/images/eval/6.png)
			  
			  $y\_{ij}$ is 1 if the sample i belongs to class j else 0
			  
			  $p\_{ij}$ is the probability our classifier predicts of sample i belonging to class j.
			  
			  \#\#\#\# When to Use?
			  
			  _When the output of a classifier is multiclass prediction probabilities. We generally use Categorical Crossentropy in case of Neural Nets._ In general, minimizing Categorical cross-entropy gives greater accuracy for the classifier.
			  
			  \#\#\#\# How to Use?
			  
			  ```pgsql
			  from sklearn.metrics import log_loss  
			  
			  \# Where y_pred is a matrix of probabilities with shape ***= (n_samples, n_classes)*** and y_true is an array of class labels
			  log_loss(y_true, y_pred, eps=1e-15)
			  
			  ```
			  
			  \#\#\#\# Caveats:
			  
			  It is susceptible in case of[imbalanced](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c)datasets.
			  
			  ---
			  
			  \#\# 5\. AUC
			  
			  AUC is the area under the ROC curve.
			  
			  _**AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes**_
			  
			  _**What is the ROC curve?**_
			  
			  ![MLWhiz: Data Science, Machine Learning, Artificial Intelligence](https://proxy-prod.omnivore-image-cache.app/0x0,seZ1-91bG_nhTOscQo43LQqRWJMo2IaKdAXLiTbCi2Uo/https://mlwhiz.com/images/eval/7.png)
			  
			  We have got the probabilities from our classifier. We can use various threshold values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we will have a ROC curve.
			  
			  Where True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.
			  
			  Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP+FP)
			  
			  and False positive rate or FPR is just the proportion of false we are capturing using our algorithm.
			  
			  1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)
			  
			  ![ROC Curve](https://proxy-prod.omnivore-image-cache.app/0x0,sAo7XWps5zvTlcnAS7wRxNL6TQM3IZ-5BH3YwvuMYKPg/https://mlwhiz.com/images/eval/8.png)
			  
			  Here we can use the ROC curves to decide on a Threshold value. The choice of threshold value will also depend on how the classifier is intended to be used.
			  
			  If it is a cancer classification application you don’t want your threshold to be as big as 0.5\. Even if a patient has a 0.3 probability of having cancer you would classify him to be 1.
			  
			  Otherwise, in an application for reducing the limits on the credit card, you don’t want your threshold to be as less as 0.5\. You are here a little worried about the negative effect of decreasing limits on customer satisfaction.
			  
			  \#\#\#\# When to Use?
			  
			  AUC is **scale-invariant**. It measures how well predictions are ranked, rather than their absolute values. So, for example, if you as a marketer want to find a list of users who will respond to a marketing campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which you will create a list of users to send the marketing campaign.
			  
			  Another benefit of using AUC is that it is **classification-threshold-invariant** like log loss. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen, unlike F1 score or accuracy which depend on the choice of threshold.
			  
			  \#\#\#\# How to Use?
			  
			  ```angelscript
			  import numpy as np
			  from sklearn.metrics import roc_auc_score
			  y_true = np.array([0, 0, 1, 1])
			  y_scores = np.array([0.1, 0.4, 0.35, 0.8])
			  
			  print(roc_auc_score(y_true, y_scores))
			  
			  ```
			  
			  \#\#\#\# Caveats
			  
			  Sometimes we will need well-calibrated probability outputs from our models and AUC doesn’t help with that.
			  
			  ---
			  
			  \#\# Conclusion
			  
			  An important step while creating our[machine learning pipeline](https://towardsdatascience.com/6-important-steps-to-build-a-machine-learning-system-d75e3b83686)is evaluating our different models against each other. A bad choice of an evaluation metric could wreak havoc to your whole system.
			  
			  _**So, always be watchful of what you are predicting and how the choice of evaluation metric might affect/alter your final predictions.**_
			  
			  Also, the choice of an evaluation metric should be well aligned with the business objective and hence it is a bit subjective. And you can come up with your own evaluation metric as well.
			  
			  \#\# Continue Learning
			  
			  If you want to[learn](https://towardsdatascience.com/how-did-i-start-with-data-science-3f4de6b501b0?source=---------8------------------)more about how to structure a Machine Learning project and the best practices, I would like to call out his awesome[third course](https://coursera.pxf.io/KeEOO7)named Structuring Machine learning projects in the Coursera[Deep Learning Specialization](https://coursera.pxf.io/7mKnnY). Do check it out. It talks about the pitfalls and a lot of basic ideas to improve your models.
			  
			  Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at[<strong>Medium</strong>](https://mlwhiz.medium.com/?source=post%5Fpage---------------------------)or Subscribe to my[<strong>blog</strong>](https://mlwhiz.ck.page/a9b8bda70c)
			  
			  Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
			  
			  [![Start your future with a Data Analysis Certificate.](https://proxy-prod.omnivore-image-cache.app/0x0,suAuv4xZG31S3AyNCO6qTRqaB4-YS2aYgERxQyX3BxXk/https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16)](https://coursera.pxf.io/coursera)
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,szKJt4DXNhsy1A3PeE0qwKTx6tovIKDoATe6QcLKtiSM/https://embed.filekitcdn.com/e/oZKFHLN3TQ2PBVSpk3FJXE/2hmfN9v6NnKho4c91rrwNG)
			  
			  Receive a FREE PDF on Advanced Python Tricks. Also, get my Hand Picked Tutorials and manually curated courses to help you master ML and DS.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,snevLSOUrugyh1By4_gCC3RaM7ADyyFpFedx9Odj4A2c/https://embed.filekitcdn.com/e/oZKFHLN3TQ2PBVSpk3FJXE/g6ehpZu2ZCDcg8TWZyG7UP)
			  
			  Lets make it a little easier. Subscribe here to get all the updates for my Upcoming Book
	- The Mind at Work: Guido van Rossum on how Python makes thinking in code easier | Dropbox Blog](https://omnivore.app/me/the-mind-at-work-guido-van-rossum-on-how-python-makes-thinking-i-18bc48ccfec)
	  collapsed:: true
	  site:: [blog.dropbox.com](https://blog.dropbox.com/topics/work-culture/-the-mind-at-work--guido-van-rossum-on-how-python-makes-thinking)
	  author:: Anthony Wing Kosner
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 24th, 2019]]
		- ### Content
			- A conversation with the creator of the world’s most popular programming language on removing brain friction for better work. 
			  
			  Nothing defines the 21st century more than the ubiquitous effects of computer programming. Almost everything we do, particularly at work, is mediated by screens displaying the results of the enormous amount of computation that we now take for granted. If you’re one of the 99.7% of the human race that are not programmers, how all of this happens is a bit of a mystery. As science fiction writer Arthur C. Clarke quotably wrote, “Any sufficiently advanced technology is indistinguishable from magic.”
			  
			  Of course, it isn’t magic. It is, however, both complicated and complex, with codebases of tech companies measured in millions of lines of code. When you’re reasoning about a real system you might want to build in code, you’re thinking about the complex relationships between different functions over time. Your code can be more or less complicated in how it is written and structured, but the problem you're trying to solve has an inherent complexity that can’t be reduced to something simpler. 
			  
			  Becoming a programmer is not just about ideas, and you won’t last long at it if you can’t deal with the laser-focused details of describing your ideas in code. “I'm a little skeptical of the claim that the systems thinking is primary there, because it's much easier to come up with an idea for a system than it is to take an idea and turn it into working code,” says Guido van Rossum, the creator and retired BDFL (Benevolent Dictator for Life) of the Python programming language. JavaScript still owns the web, and Java runs 2.5 billion Android phones, but for general purpose programming and education, Python has become the default standard.
			  
			  If anyone has made turning an idea into working code easier, for more people, it’s Van Rossum over his 30-year history with Python. And he’s done it with a self-effacing grace and an understated humor—the language is named after the surreal comedy of Monty Python, not the actual Burmese python. In its quiet way, the Python programming language has managed to make some of the complications of programming computers less difficult for our brains to manage.
			  
			  To understand how Van Rossum accomplished this amazing feat, we have to go back into the history of computing to the era of mainframes and machine language. “The mainframe is a machine that costs many millions of dollars, and the combined pay of all those programmers is peanuts compared to the cost of the mainframe,” he says, explaining that cost logically prioritized machine time over human time. “But as I experienced desktop workstations and PCs, I realized that a change of mindset about cost of the programmer's time versus cost of the computer's time was overdue.” Van Rossum doesn’t think he was the first person to observe this shift, but he really capitalized on it in the design of Python.
			  
			  This simple idea of giving humans priority over machines is at the core of the philosophy behind Python. Certainly the fact that it’s an interpreted language as opposed to a compiled language means that the programmer gets immediate feedback about the code they’re writing without needing to take the time to recompile it after making each change. This is very common now, but thirty years ago it was quite controversial because the conventional wisdom was that faster (for the computer) was better. Updating this belief has had a large positive impact on the productivity of programmers.
			  
			  > “In Python, every symbol you type is essential.”  
			  > —Guido van Rossum
			  
			  “There are a whole bunch of common programming tasks that are easy in Python,” says Van Rossum. “For someone who is not yet a programmer, who wants to become a programmer, for those people Python is particularly easy to get.” Indeed, many computer science schools are switching over from Java to Python, because it’s much easier to grasp for beginners. The reasons behind this are complex, with many factors each reducing little bits of friction. What’s simple is the philosophy behind all of the improvements: Everything should have a necessary purpose. The lack of extraneous code makes it easier to focus on what you need to pay attention to. “In Python, every symbol you type is essential,” Van Rossum says. 
			  
			  This concision makes it easy to accomplish something meaningful in Python, which is one of the reasons for its wide adoption. “The typical way that we introduce Python to beginning programmers is also important. We can show them very small snippets of code that require very little understanding of terminology and concepts from programming before they make sense,” Van Rossum explains, “whereas the smallest Java program, for example, contains a whole bunch of what are, to the uninitiated eye, noise characters.”
			  
			  This quietness and simplicity of design makes it easier to see what’s going on. “Python for me is incredibly visual,” says Van Rossum. “When I read Python, I definitely see it as a two-dimensional structure, rather than one-dimensional, like language. That is probably because Python uses indentation for grouping, but probably also because my mind just likes thinking visually.”
			  
			  He’s not the only one, of course, who thinks visually. We all do to some extent. But he’s particularly sensitive to the effects of the visual on cognition. “Some poorly formatted text can drive me crazy. They interrupt my visual parsing of the flow and the structure, and in that sense, I do think in Python,” Van Rossum admits. “I can grasp code much better when it's formatted properly.” It takes more information to resolve the uncertainty about what code indentations mean if they’re arbitrary than if those indentations have a clear purpose, as they do in Python. So if the experience is easier, it’s because fewer bits of information have to be processed for you to know what’s going on.
			  
			  Python’s readability is not just typographic, but conceptual. Van Rossum thinks Python may be closer to our visual understanding of the structures that we are representing in code than other languages because, “Python makes that structure mandatory.” 
			  
			  "While I was researching my book, [CODERS](https://www.penguinrandomhouse.com/books/539883/coders-by-clive-thompson/),” says author Clive Thompson, “I talked to a lot of developers who absolutely love Python. Nearly all said something like ‘Python is beautiful.’ They loved its readability—they found that it was far easier to glance at Python code and see its intent. Shorn of curly brackets, indented in elegant visual shelves, anything written in Python really looks like modern poetry." They also find that Python is fun to write, which is more important than it may seem. As Thompson writes, “When you meet a coder, you’re meeting someone whose core daily experience is of unending failure and grinding frustration.”
			  
			  > “You primarily write your code to communicate with other coders, and, to a lesser extent, to impose your will on the computer.”  
			  > —Guido van Rossum
			  
			  Building the priority of the programmer’s time into the language has had a curious effect on the community that’s grown around it. There’s a social philosophy that flows out of Python in terms of the programmer’s responsibility to write programs for other people. There’s an implicit suggestion, very much supported by Van Rossum in the ways he talks and writes about Python, to take a little more time in order to make your code more interpretable to someone else in the future. Expressing your respect for others and their time through the quality of your work is an ethos that Van Rossum has stealthily propagated in the world. “You primarily write your code to communicate with other coders, and, to a lesser extent, to impose your will on the computer,” he says. 
			  
			  The universality of the culture that’s spread around Python has achieved some measure of what Van Rossum intended two decades ago with a short-lived project called CP4E (Computer Programming for Everybody). “I'm usually not a very visionary thinker. People always ask me, what's next for Python, and I never know. But I put on my most visionary hat, and assumed that it would make sense for everyone to learn programming.” Personal computers had been around for 20 years, but mostly they were glorified typewriters and calculators. Van Rossum asked, “isn't it crazy that all those people have computers, and very few of them learn to program?”
			  
			  In the time since, he has focused on making programming easier to learn and easier to do through the advancements in Python, now at version 3.7\. He still thinks that programming teaches generally valuable skills, like problem solving, following directions carefully, and understanding what directions mean. But he’s also found that, “there are certain introductions to programming that are fun for kids to do, but they're not fun for all kids, and I don't think I would want to make it a mandatory part of the curriculum.”
			  
			  At the same time, the need for people to program their computers has also diminished because of the growth of software, particularly on the web, that allows you to intuitively do what used to require programing to accomplish. “I'm not so sure that it needs to happen anymore,” Van Rossum says of CP4E. “I think computers have made it to that point, where they're just a useful thing that not everybody needs to know what goes on inside.”
			  
			  > “Python is now also the language of amateurs, and I mean that in the best possible way.”  
			  > —Guido van Rossum
			  
			  But that are a growing number of people who are using Python in many fields. “The currently prevailing theory about Python's unexpected success,” says Van Rossum, “is that at some point, it established itself into data science and machine learning, and scientific data processing in general, and once you have critical mass, it's easier for everyone to use the same system as their colleagues and their competitors, than to try something different.” And even though it started as purely a tool for professional programmers, Van Rossum says, “Python is now also the language of amateurs, and I mean that in the best possible way.”
			  
			  A successful open source software project, like Python, has to be easy to learn for beginners, but also have practical application to real world problems that more advanced users will want to solve. Just as for beginners you want to keep things simple so all of their brain energy is spent on learning the complications of the programming environment, for advanced users you want to help them manage the complexity of these competing abstractions. Part of the motivation for making the implementation of Python as simple as possible is to be able to change your mind, to learn, to iterate. “If you write a prototype in Python and you get it to work, often, that’s not a very big effort,” says Van Rossum, “and then you can afford to throw away your prototype and write the same thing again based on what you've learned. You can still write it in Python, but the second version will be much better than the first.” 
			  
			  Part of the enduring appeal of Python is the optimism and humility of starting over. “If you've invested much more time into writing and debugging code, you're much less eager to throw it all away and start over.” Co-founder and CEO, Drew Houston wrote the first prototype of Dropbox in Python on a five-hour bus ride from Boston to New York. “The early prototypes of Dropbox were thrown away, largely, many times,” says Van Rossum.
			  
			  What can we learn from Python about how to design better tools for thinking? Tim Peters, one of Python’s major contributors, offers some clues in the aphoristic [The Zen of Python](https://www.python.org/dev/peps/pep-0020/\#id3), in which he channels Van Rossum’s guiding principles. Particularly relevant to our present discussion are this pair: “Simple is better than complex. Complex is better than complicated.” This could almost be a recipe for how the brain prioritizes its functions to use energy efficiently.
			  
			  Equally important for Van Rossum is the social aspect of thinking and building tools. What has he taken away from his thirty year journey with Python? “I have learned that you can't do it alone, which is not an easy lesson for me. I've learned that you don't always get the outcome that you went for, but maybe the outcome you get is just as good, or better.”
	- Flex Wheeler: The Comeback Diaries Archives - Generation Iron Fitness & Strength Sports Network](https://omnivore.app/me/flex-wheeler-the-comeback-diaries-archives-generation-iron-fitne-18bc48ccdeb)
	  collapsed:: true
	  site:: [Generation Iron Fitness & Strength Sports Network](https://generationiron.com/category/watch/flex-wheeler-the-comeback-diaries/)
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Sep 7th, 2017]]
		- ### Content
			- Password recovery
			  
			  Recover your password
			  
			  your email
			  
			  A password will be e-mailed to you.
	- Homemade Bacon Pizza | Homemade Pizza Dough Recipe and Toppings](https://omnivore.app/me/homemade-bacon-pizza-homemade-pizza-dough-recipe-and-toppings-18bc48ccdbd)
	  collapsed:: true
	  site:: [Bless this Mess](https://www.blessthismessplease.com/herbed-tomato-bacon-pizza/)
	  author:: Melissa
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Oct 15th, 2019]]
		- ### Content
			- **Herb and Tomato Bacon Pizza** is not only super tasty, but it is also a great way to bond with your family as you make and enjoy dinner together! I seriously, seriously love pizza, and this recipe's a winner because it's pretty straightforward, but DELISH! You'll love making it with your people.
			  
			  **Looking for more pizza recipes?** Try this [Homemade Pizza with Chicken and Bacon](https://www.blessthismessplease.com/chicken-mushroom-pizza-with-creamy/) or this [Homemade Supreme Pizza Recipe](https://www.blessthismessplease.com/homemade-supreme-pizza-recipe/)!
			  
			  ![Homemade pizza dough topped with bacon, cheese, tomatoes, and herbs is the perfect pizza to make tonight. Your whole family will love it! \#pizza \#homemadepizza \#tomatobaconpizza](https://proxy-prod.omnivore-image-cache.app/900x1350,s7h4DALC7yOjXzQxzvhYEClMTXyTCLoTuJnv9xIDJkWE/https://www.blessthismessplease.com/wp-content/uploads/2012/09/Herbed-Tomato-Bacon-Pizza-900-4.jpg)
			  
			  \#\#\# MY OTHER RECIPES
			  
			  Sorry, the video player failed to load.(Error Code: 104153)
			  
			  \#\# Healthy Homemade Herbed Pizza
			  
			  Homemade pizza... There's nothing better for a Friday night at home if you ask me! My mom's homemade pizza was famous when I was a kid. I remember once when I was in junior high school, I had a classmate over to work on a project (a solar system diorama I do believe). She happened to be around at dinner time and had Mom's pizza with us. She was so shocked and amazed that people made their own pizza, and then she was even more shocked when she tasted it.
			  
			  \#\# Fresh Tomato and Bacon Pizza Recipe
			  
			  There's just no match for homemade pizza! And this one is GOOD. I mean, you really can't go wrong with fresh tomatoes, an herb medley, and (the kicker) bacon, right? It's a totally simple combo that ends up tasting splendid on some pizza dough with melty cheese and garlicky tomato sauce. We love a good pizza around here because everyone can get involved in the making of it. The kids love adding their own ratios of toppings, and I love that they're helping in the kitchen!
			  
			  ![Homemade pizza dough topped with bacon, cheese, tomatoes, and herbs is the perfect pizza to make tonight. Your whole family will love it! \#pizza \#homemadepizza \#tomatobaconpizza](https://proxy-prod.omnivore-image-cache.app/1350x900,suJefphvG__d-qiqxM3b0OdowE_wAa2u0A-7ifl030XI/https://www.blessthismessplease.com/wp-content/uploads/2012/09/Herbed-Tomato-Bacon-Pizza-900-11.jpg)
			  
			  \#\# How Long Do I Cook Homemade Pizza?
			  
			  Bake this recipe for 12 to 15 minutes on a preheated pizza stone (or over-turned baking sheet) at 500 degrees F., until the top is bubbly and the edges are starting to brown. Sometimes I don't want my oven that high though so you can also cook it at 425 for closer to 20 minutes.
			  
			  \#\# How Do You Serve Homemade Pizza?
			  
			  I serve it right off of the parchment paper (which gets all dark and cooked looking), or I’ll tear off a new piece and serve it on a crisp white sheet of parchment. Very pretty. And VERY simple! (For cleanup and beyond!)
			  
			  \#\# How Long Do You Wait To Cut Pizza?
			  
			  I have the hardest time waiting, but if you allow the pizza to cool for at least 3 to 4 minutes, the cheese will have time to set and won't slide off! Worth the wait, absolutely.
			  
			  ![Homemade pizza dough topped with bacon, cheese, tomatoes, and herbs is the perfect pizza to make tonight. Your whole family will love it! \#pizza \#homemadepizza \#tomatobaconpizza](https://proxy-prod.omnivore-image-cache.app/900x1350,sZEKMn8HeWYXH5RNNk0vFhff-gU-sVjqXaX2NP-oFb0Y/https://www.blessthismessplease.com/wp-content/uploads/2012/09/Herbed-Tomato-Bacon-Pizza-900-13.jpg)
			  
			  [Print](https://www.blessthismessplease.com/herbed-tomato-bacon-pizza/print/22646/)
			  
			  \#\#\# Description
			  
			  **Herb and Tomato Bacon Pizza** is not only super tasty, but it is also a great way to bond with your family as you make and enjoy dinner together! I seriously, seriously love pizza, and this recipe's a winner because it's pretty straightforward, but DELISH! You'll love making it with your people.
			  
			  ---
			  
			  \#\#\#\# For the dough:
			  
			  * 2 ½ cups warm water
			  * 1 tablespoon yeast
			  * 1 tablespoon sugar or honey
			  * 1 teaspoon salt
			  * ¼ cup olive oil
			  * 4 to 6 cups flour (whole-wheat or unbleached all-purpose)
			  
			  \#\#\#\# For the pizza:
			  
			  * 2 cups pizza sauce (my easy recipe is in the notes)
			  * 3 cups shredded mozzarella
			  * 6 to 8 slices bacon, cooked crisp and crumbled
			  * 1 pint cherry tomatoes, halved
			  * ¼ cup chopped fresh herbs (a mix of basil, parsley, and oregano is delicious)
			  
			  Cook Mode Prevent your screen from going dark 
			  
			  ---
			  
			  1. In a large mixing bowl, add the water, yeast, sugar, salt, and olive oil. Allow mixture to stand for 5 minutes. Add one cup of flour, and mix to combine. Add another cup of flour and mix through. Mix by hand for 1 or 2 minutes. The dough should be the consistency of cake batter. Add another cup of flour and mix through. Add flour until you can’t mix it by hand very well. Sprinkle some flour on your table and turn the dough onto the table. Knead the dough by hand until it is smooth and elastic, adding flour as needed. This should take 6 to 10 minutes of kneading. Place the dough back in the bowl, cover it with a towel, and let it rise in a warm place for 15 to 60 minutes (this time varies a lot for me -- I just let it rise according to how long it is until dinner time).
			  2. While the dough is rising, preheat your oven to 500 degrees F. If you have a baking stone, put it in the oven to heat up, too. I would recommend using a stone for baking this pizza. If you don’t have a stone, you can turn your biggest cookie sheet upside down (so that the flat side is up), or use a rimless sheet. If you are using a stone let the stone heat for 30 minutes. If using a baking sheet, let it heat for 15 minutes.
			  3. When the oven is hot, cut your dough in half. Place half of the dough on a piece of parchment paper that is 18 inches long. Using a rolling pin, roll the dough (on top of the parchment paper) into a large circle. The dough should be between ¼- and ½-inch thick (I like mine thin, Husband likes his thicker... you'll have to find what you like). Top with 1 cup of pizza sauce, 1 ½ cups of shredded mozzarella, and half of the crumbled bacon, tomatoes, and herbs.
			  4. When the pizza is all ready, slide the pizza, which is on the parchment paper, onto a rimless cookie sheet or one that is upside down. This will help you get the pizza to the oven.
			  5. Open the oven and slide the pizza, which is still on the parchment paper, from the cookie sheet onto the hot baking stone or hot cookie sheet. I just tug on the edge of the parchment paper with my hands and slide it on to the stone. BE CAREFUL! Your oven is set to 500 degrees F. It’s super hot, obviously! Make sure the babies are out of the way, and be careful when sliding the pizza into the oven.
			  6. Bake for 12 to 15 minutes, until the top is bubbly and the edges are starting to brown. When the pizza is done, tug the edge of the parchment paper, and slide the pizza back onto the cookie sheet you used to transfer it to the oven. Again, be careful!
			  7. Let your stone reheat for 10 minutes. While the stone is heating, repeat the whole thing with the second half of the dough and bake the same way.
			  8. Repeat with the second half of the dough and the rest of the toppings.
			  9. Enjoy, enjoy, enjoy!
			  10. I serve it right off of the parchment paper (which gets all dark and cooked looking) or I’ll tear off a new piece and serve it on a crisp white sheet of parchment. Very pretty!
			  
			  \#\#\# Notes
			  
			  For the pizza sauce: 2 tablespoons olive oil, 2 cloves garlic, 1 (28-ounce) can crushed tomatoes (extra points if you home can them, which I do every summer!). In a small saucepan, cook the oil and garlic together over medium heat until the garlic starts to smell good. Add the tomatoes and simmer until it thickens a bit, about 15 minutes. Season with salt and pepper to taste. DONE! Really. Three ingredients and it tastes amazing!
			  
			  * Prep Time: 90 minutes
			  * Cook Time: 40 minutes
			  * Category: Dinner
			  * Method: Oven
			  * Cuisine: American
			  
			  ![Homemade pizza dough topped with bacon, cheese, tomatoes, and herbs is the perfect pizza to make tonight. Your whole family will love it! \#pizza \#homemadepizza \#tomatobaconpizza](https://proxy-prod.omnivore-image-cache.app/900x1350,s6O1RfSj5cySIFhaDZtDXW-MdXbsAxHhpaTQAZrsb19s/https://www.blessthismessplease.com/wp-content/uploads/2012/09/Herbed-Tomato-Bacon-Pizza-900-7.jpg)
			  
			  ![Homemade pizza dough topped with bacon, cheese, tomatoes, and herbs is the perfect pizza to make tonight. Your whole family will love it! \#pizza \#homemadepizza \#tomatobaconpizza](https://proxy-prod.omnivore-image-cache.app/900x1600,sUSNQd0qMfS6_IV5tMrvmnEF9qpuoy9OIKZwE6JFnF28/https://www.blessthismessplease.com/wp-content/uploads/2019/09/HerbedTomatoBaconPizza_900x1600.jpg) ![Your new go-to homemade pizza recipe. ](https://proxy-prod.omnivore-image-cache.app/600x900,s9m8JBAQhijDPe16fnzOg_mQ0Y54blAa-oZi6_o5QjTI/https://www.blessthismessplease.com/wp-content/uploads/2019/09/HerbedTomatoBaconPizza_600x900.jpg)
			  
			  \#\#\# More Pizza Recipes You'll Love:
			  
			  * [Easy Pepperoni Pizza Muffins](https://www.blessthismessplease.com/easy-pepperoni-pizza-muffins/)
			  * [Homemade Pizza with Chicken and Bacon](https://www.blessthismessplease.com/chicken-mushroom-pizza-with-creamy/)
			  * [Perfect Pepperoni Pizza Sticks](https://www.blessthismessplease.com/pepperoni-pizza-sticks/)
			  * [Amazing No-Knead Refrigerator Pizza Dough](https://www.blessthismessplease.com/amazing-no-knead-refrigerator-pizza-dough/)
			  * [Supreme Pizza Stuffed Zucchini](https://www.blessthismessplease.com/supreme-pizza-stuffed-zucchini/)
			  * [30 Minute Meaty Margherita Pizza](https://www.blessthismessplease.com/30-minute-meaty-margherita-pizza/)
			  * [20 Minute Easy Cheesy Pizza Sticks](https://www.blessthismessplease.com/easy-cheesy-pizza-sticks/)
			  * [Quick and Easy Flatbread Pizzas](https://www.blessthismessplease.com/flatbread-pizzas/)
			  
			  _This post was originally published in 2014, and has been updated and rephotographed in October 2019._
			  
			  I know you and your loved ones will totally love this **herbed tomato bacon pizza recipe**.It's tasty and a perfect way to empty your garden.
			  
			  Explore More
	- Inside Siegfried & Roy's Devastating Tiger Attack 16 Years Later: How The Vegas Icons Have Coped | Access](https://omnivore.app/me/inside-siegfried-roy-s-devastating-tiger-attack-16-years-later-h-18bc48cca07)
	  collapsed:: true
	  site:: [Access](https://www.accessonline.com/videos/inside-siegfried-roys-devastating-tiger-attack-16-years-later-how-the-vegas-icons-have-coped)
	  author:: Access
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Sep 26th, 2019]]
		- ### Content
			- \#\# Your Privacy Choices: Opt-out of sale of personal information and Opt-out of sharing or processing personal information for targeted ads 
			  
			  To provide you with a more relevant online experience, certain online ad partners may combine personal information that we make available with data across different businesses and otherwise assist us with related advertising activities, as described in our [Privacy Policy](https://www.nbcuniversal.com/privacy). This may be considered "selling" or "sharing/processing” for targeted online advertising under applicable law.
			  
			  If you are a resident of California, Connecticut, Colorado, Utah or Virginia, to opt out of us selling or sharing/processing your personal information:
			  
			  * Such as cookies and devices identifiers for the targeted ads and related purposes for this site/app on this browser/device: switch the “Allow Sale of My Personal Info or Sharing/Processing for Targeted Ads” toggle under Manage Preferences to OFF (grey color) by moving it LEFT and clicking “Confirm My Choice”.
			  * Such as your name, email address and other associated personal information for targeted advertising activities as described above, please submit the form below.
			  
			  **Please note that choices related to cookies and device identifiers are specific to the brand’s website or app on the browser or device where you are making the election.** 
			  
			  \#\#\# Manage Preferences
			  
			  \#\#\#\# Allow Sale of My Personal Info and Sharing/Processing for Targeted Ads
			  
			  Allow Sale of My Personal Info and Sharing/Processing for Targeted Ads 
			  
			  **California, Connecticut, Colorado, Utah & Virginia Residents Only:** To opt out of selling or sharing/processing for targeted advertising of information such as cookies and device identifiers processed for targeted ads (as defined by law) and related purposes for this site/app on this browser/device, switch this toggle to off (grey color) by moving it left and clicking “Confirm My Choice” below. (This will close this dialogue box, so please open the email Opt-Out Form 1st). 
			  
			  **ALL OTHER LOCATIONS:** If we do not detect that you are in **California, Connecticut, Colorado, Utah or Virginia,** this choice will not apply even if you toggle this button off.
			  
			  If you turn this off, you will still see ads, but they may be less relevant or based only on our first-party information about you.
			  
			  Please note, you must make the Manage Preference choices on each site/app on each browser/device you use to access the services. You must also renew this choice if you clear your cookies. You can change your precise geolocation permissions for our mobile apps in your mobile device settings. 
			  
			  \#\#\#\# Opt-Out Form
			  
			  Always Active
			  
			  To opt out of the use of your email and other personal information related to that email such as your name for targeted advertising activities please complete this [Opt-Out Form](https://privacyportal.onetrust.com/webform/17e5cb00-ad90-47f5-a58d-77597d9d2c16/cda09e33-f935-4960-87de-afa722cef3c2)
			  
			  \#\#\#\# Other Categories of Data Collection
			  
			  Always Active
			  
			  Please see our Cookie Notice for more details which can be found by navigating to the Privacy Policy in the menu settings page.
	- 10 Great Sci-fi Horror Films You’ve Probably Never Seen – Taste of Cinema – Movie Reviews and Classic Movie Lists](https://omnivore.app/me/10-great-sci-fi-horror-films-you-ve-probably-never-seen-taste-of-18bc48c8d0d)
	  collapsed:: true
	  site:: [Taste of Cinema - Movie Reviews and Classic Movie Lists](http://www.tasteofcinema.com/2019/10-great-sci-fi-horror-films-youve-probably-never-seen/)
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- ![](https://proxy-prod.omnivore-image-cache.app/560x318,sEJk4N5goP3sRqs9n88yzDuvXwHl7gbJGAS3hVBvvbvY/http://www.tasteofcinema.com/wp-content/uploads/2019/10/carriers-2009.jpg)
			  
			  Sci-fi and horror share some of the same genre roots and there have been several excellent mixes of those two genres in the past. Obviously, David Cronenberg is a particular expert here; from “The Fly” to “eXistenZ,” he kept on giving thought-provoking and at the same time, scary films to cinema. 
			  
			  Then there were, of course, Scott’s “Alien,” Carpenter’s “The Thing” and many more classics. While the genre doesn’t always deliver good stuff and we often get cheap rip-offs of better and more popular films, some really entertaining movies end up being overlooked. Here are 10 of them that you may like if somehow you missed them.
			  
			  10\. Spring (2014)
			  
			  ![spring movie](https://proxy-prod.omnivore-image-cache.app/560x323,sLWDiE71YnoD0MJKDv-_i0Cf0ZJQ2--6S4Y84dhppJz4/http://www.tasteofcinema.com/wp-content/uploads/2015/09/spring-movie.jpg)
			  
			  A romantic body horror film, how does it sound? Sounds good in “Spring.” After his mother dies, he loses his job, and is wanted by the police for a bar fight, Evan decides to travel to Italy to get his mind clear. There he meets a cosmopolitan, charming student named Louise, with whom he soon falls in love. It basically starts similarly to “Before Sunrise.” Then Evan decides to stay in Italy for the time being, and accepts a job on Angelo’s farm for accommodation. It quickly becomes clear that the highly intelligent Italian may have a dark secret.
			  
			  The movie takes a very Lovecraftian (and is very good at it) turn from here. Some may find it surprising and would maybe prefer the film to stay as a romance drama but that’s the strength of the film. Few filmmakers nowadays spend so much time introducing a character. And this preliminary work does not negate twists; on the contrary, even though one of the lovers is a monster, the tragic-beautiful romance itself makes this shocking revelation an even more human and honest portrayal than the vast majority of pure romance dramas. That’s how the film works on every level – it’s effective as a romance drama and is a very creatively made body horror.
			  
			  9\. The Cell (2000)
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/560x236,suveLKXuWHVkjTgTcHHWrNrL7NquzyKJS9Ikzbva_-sU/http://www.tasteofcinema.com/wp-content/uploads/2018/01/The-Cell-585.jpg)
			  
			  Exploring a person’s mind by literally wandering around in it. This is the new method experimented by therapist Catherine Dean who has to go into the serial killer’s mind to find the hiding place of the girl he kidnapped last. 
			  
			  Tarsem Singh is certainly a director with a strong visual sense, which gets heavily featured in “The Cell.” Jennifer Lopez may have an inconsistent filmography, but she has some interesting features in her resume. Movies like “Maid in Manhattan” and “The Wedding Planner” can make her filmography easy to dismiss, but then again, she was also in “Out of Sight” and more bold stuff like “U Turn.” 
			  
			  As for “The Cell,” it was actually a box office success, but it rarely gets talked about and even the critical reception was rather focused on its aesthetics. But the film was more than that. It had interesting characters that made us care about them, and while most of the cast was there more for their presence rather than acting talent, one member of the cast – Vincent D’Onofrio – is a great standout with a delicious performance. The film has everything – it sometimes plays like a children’s fairy tale, sometimes a brutal thriller like “Se7en,” and is often a surreal, visually satisfying, overall interesting cinematic experience.
			  
			  8\. Dreamscape (1984)
			  
			  ![Dreamscape (1984)](https://proxy-prod.omnivore-image-cache.app/560x336,sRmVDnaeomfBhhi-q2z3N50ATT8Fm8fj1KN-ufDL-elg/http://www.tasteofcinema.com/wp-content/uploads/2015/10/Dreamscape-1984.jpg)
			  
			  Before there was “Vanilla Sky,” “Inception” and all the rest, there was “Dreamscape.” An impressive and very cool mix of several genres like horror, science fiction, fantasy, adventure, action and more – “Dreamscape” is not without its flaws, but then again, it’s also wildly entertaining.
			  
			  The film is about an attempt to help people who experience recurring nightmares; a research program is using psychics to enter patients’ dreams. Everything works fine until the president becomes one of the patients, and an assassin attempts to kill him in his sleep. The cast of Dennis Quaid (such a compelling leading man performance here), Christopher Plummer and Max von Sydow is more than you could wish for, and another reason why the movie works so well. 
			  
			  Another plus side is that it never loses its sense of humour, which is very important when you make a film like this. It also may look like a B-movie thanks to its budget and the crazy directions the story takes, but the movie actually has more depth than you expect it to have, and some critics even argued that it has something to say about the political climate of its time.
			  
			  7\. Screamers (1995)
			  
			  ![Screamers (1995)](https://proxy-prod.omnivore-image-cache.app/560x316,sT3VOivm2yGDCm4GqN8KbG8rcs-Ippr31RJk_yfpF47Y/http://www.tasteofcinema.com/wp-content/uploads/2015/06/Screamers-1995.jpg)
			  
			  Screams may not be the next “The Thing” to give you that strong sense of paranoia, but still it’s a very entertaining picture that critics were way too harsh on. At least Roger Ebert, though mostly disappointed with the film, admitted that the movie was “made with a certain imagination and intelligence.” Despite being made with a low budget and the fact that it was a complete box office failure, it has a grim premise that works, even if it is not too scary. 
			  
			  It adapted ”Second Variety” by Philip K. Dick pretty faithfully and the atmosphere was cool as well. Considering its budget at the time, all of these things comes off as even more impressive. The ending feels slightly underwhelming, but before that, the seasoned genre specialist Duguay largely relies on creating suspense with the paranoia of his protagonists and he largely succeeds at that. 
			  
			  The theme is what happens if humanity’s technological development goes too far. The whole thing is actually similar to the Terminator series, even. You’ll likely complain about one or two things here, but still “Screamers” is a film that should be seen by everyone who loves themselves some ‘90s sci-fi horror.
			  
			  6\. Carriers (2009)
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/560x330,swFHPFFwahcI9DEZ5PA5Qrzzc88qIrsG6nsaGmh_0Ixw/http://www.tasteofcinema.com/wp-content/uploads/2019/10/carriers.jpg)
			  
			  A virus has mutated and invaded humans and there is no cure in sight. The epidemic has wiped out almost all humanity in our film. And four young friends make their way to a secluded beach to await the end of the epidemic. Then they encounter a man named Frank and his infected daughter, whose vehicle has run out of fuel. They escape from him when he attacks them, but their car breaks down.
			  
			  Put into extremely limited release by Paramount in 2009 after spending years in studio lockdown, “Carriers” unfortunately didn’t get enough attention when it was initially released and years after Chris Pine got famous, the movie still didn’t get any renewed attention. The film is admirable for trying to take a realistic approach with the ultimate fear of disease/death and how humans react in such a situation. 
			  
			  Not only is it an effective horror/science fiction, but it also has a surprisingly good dramatic depth behind its story and it also has a very fine cast to pull everything off, especially Pine. Unlike some other films on the list like “The Hidden,” this film has no “fun” side to it. It’s a very bleak film and the pacing can turn off some people. But “Carriers” is worth watching if you like post-apocalyptic and viral outbreak kinds of movies.
			  
			  Pages: 1 [2](http://www.tasteofcinema.com/2019/10-great-sci-fi-horror-films-youve-probably-never-seen/2/)
	- 5 Miraculous Tools for Devs Who Want to Test - DEV Community](https://omnivore.app/me/5-miraculous-tools-for-devs-who-want-to-test-dev-community-18bc48c7f44)
	  collapsed:: true
	  site:: [DEV Community](https://dev.to/razgandeanu/5-miraculous-tools-for-devs-who-want-to-test-2688)
	  author:: Klaus
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Sep 25th, 2019]]
		- ### Content
			- [ ![Cover image for 5 Miraculous Tools for Devs Who Want to Test](https://proxy-prod.omnivore-image-cache.app/1000x420,s377hrMMBRRdpJfAoVt0kO-c4q_lvMet0aK3CKFp2Pms/https://res.cloudinary.com/practicaldev/image/fetch/s--4YpRnGlM--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://thepracticaldev.s3.amazonaws.com/i/11dlg4mym1mrgozqyrj6.png) ](https://res.cloudinary.com/practicaldev/image/fetch/s--4YpRnGlM--/c%5Fimagga%5Fscale,f%5Fauto,fl%5Fprogressive,h%5F420,q%5Fauto,w%5F1000/https://thepracticaldev.s3.amazonaws.com/i/11dlg4mym1mrgozqyrj6.png) 
			  
			  [Klaus](https://dev.to/razgandeanu)
			  
			  Posted on • Updated on 
			  
			  Testing, as we all know, is what saves us a lot of trouble.  
			  But not everyone is using the same tools or know all of the great ones out there that help make your testing more successful.
			  
			  Don't fall into the stereotype of the stubborn dev who relies entirely on unit tests. You might regret it, sooner than you think.
			  
			  \#\# [ ](\#1-endtest) **1\. [Endtest](https://endtest.io/)** 
			  
			  A platform that allows you to create, manage and run Automated Tests for Web Applications and Native Mobile Apps (Android & iOS), without having to write any code.  
			  
			  Some of the features:  
			  • Cross-browser grid, running on Windows and macOS machines  
			  • Codeless Editor for Automated Tests  
			  • Support for Web Applications  
			  • Support for both native and hybrid Android and iOS apps  
			  • Video recordings for test executions  
			  • Detailed logs  
			  • Chrome extension to record web tests  
			  • Element Inspector for mobile apps  
			  • Screenshot comparison  
			  • Data-driven testing with CSV files  
			  • Geolocation  
			  • Email, Slack and Webhook notifications  
			  • If Statements and Loops  
			  • Variables and re-usable components  
			  • Support for file uploads in the tests  
			  • An Endtest API, for integration with CI/CD systems  
			  • Advanced Assertions  
			  • Endtest Mailbox, for testing emails  
			  • Self-healing tests
			  
			  This is the Codeless Test Editor:  
			  [![Endtest Codeless Editor](https://proxy-prod.omnivore-image-cache.app/0x0,s6PuFGrumjQ2cvFOFGQ3nSFYUPbshxFAo7h-xZ1G3MRA/https://res.cloudinary.com/practicaldev/image/fetch/s--2ulEIBbu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/knvgfp5s11npi74qx3ir.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--2ulEIBbu--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/knvgfp5s11npi74qx3ir.png)
			  
			  This is what it looks like when you want to execute a test:  
			  [![Endtest UI](https://proxy-prod.omnivore-image-cache.app/0x0,su-HBNlao5yTncR4PYya3-KdERHeudh-wODyOpwFFTO8/https://res.cloudinary.com/practicaldev/image/fetch/s--Gvxt2hag--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/ku9gwet2genzvkp10r5n.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--Gvxt2hag--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/ku9gwet2genzvkp10r5n.png)
			  
			  And these are the detailed logs from a test execution:  
			  [![Endtest Results](https://proxy-prod.omnivore-image-cache.app/0x0,sj_nWsKUXYniCEhJ0UZmNmH554D_LoC3d-JBtA4OdAfk/https://res.cloudinary.com/practicaldev/image/fetch/s--UYx4is-8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/5717ka5bwovmlb8xduzt.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--UYx4is-8--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/5717ka5bwovmlb8xduzt.png)
			  
			  Their [Documentation](https://endtest.io/guides/docs/how-to-create-web-tests/) is fantastic. Lots of examples and videos.
			  
			  You can even run Automated Tests on Real Mobile Devices:
			  
			  [![endtest mobile](https://proxy-prod.omnivore-image-cache.app/0x0,s5JbAoZrpqBMAx32DAS08DkhZ16oLY4-7ilBjWrmY7D4/https://res.cloudinary.com/practicaldev/image/fetch/s--qLDt6iL9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.imgur.com/hxu10zo.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--qLDt6iL9--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://i.imgur.com/hxu10zo.png)
			  
			  According to their Twitter account, they seem to be adding new features every week:
			  
			  [![endtest twitter](https://proxy-prod.omnivore-image-cache.app/0x0,scIwj6dHaSHftJjwsLPKO2CZiL1QUr-LryxUE_3aAVXo/https://res.cloudinary.com/practicaldev/image/fetch/s--TkG6nRqe--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/anwh5kg0qpnpmc29jshf.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--TkG6nRqe--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/anwh5kg0qpnpmc29jshf.png)
			  
			  \#\# [ ](\#2-postman) **2\. [Postman](https://www.getpostman.com/.io)** 
			  
			  The easiest way to test any API.  
			  
			  [Postman](https://www.getpostman.com/.io) has evolved in the last few years and is now a complete solution for monitoring the health of your APIs.
			  
			  Some of the features:  
			  • API Client  
			  • Request Chaining  
			  • Data Security  
			  • Traffic Control  
			  • Orchestration  
			  • Logs/Documentation  
			  • API Monitoring  
			  • Monetization
			  
			  [![Postman](https://proxy-prod.omnivore-image-cache.app/0x0,sI7pHT1rwd62IOfryV_sbVcutI978jQ8Qu3PPHmqyFQA/https://res.cloudinary.com/practicaldev/image/fetch/s--ZvbZMNLP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/azzzrbaaf4j1zaq1zpjn.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--ZvbZMNLP--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/azzzrbaaf4j1zaq1zpjn.png)
			  
			  \#\# [ ](\#3-litmus) **3\. [Litmus](https://litmus.com/)** 
			  
			  You can use Litmus to build, test, and monitor emails.   
			  
			  What's interesting for us is the ability to open an email in the multitude of email clients on their side. 
			  
			  Want to know how your email looks in Outlook 2013 or in Apple Mail?
			  
			  All you have to do is fetch a temporary email address from [Litmus](https://litmus.com/) and send your email there. 
			  
			  You'll be able to view that in the email clients directly on their site.
			  
			  Test your emails and you might avoid embarrassing situations like this:
			  
			  [![Email testing](https://proxy-prod.omnivore-image-cache.app/0x0,sRTC6J7ZicoC4whcoloci7pEw8rzYDfwErTa8kZC_eEM/https://res.cloudinary.com/practicaldev/image/fetch/s--yVH2Uci4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/787p9rscf7xz2h8ghvrj.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--yVH2Uci4--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/787p9rscf7xz2h8ghvrj.png)
			  
			  [![Email testing](https://proxy-prod.omnivore-image-cache.app/0x0,scx_EORw8MqSeHI_C9FhqAyercoPc9wxIAdM8GgNQH64/https://res.cloudinary.com/practicaldev/image/fetch/s--wSIfqo0o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/cs786ev6r5yg5228rtv8.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--wSIfqo0o--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/cs786ev6r5yg5228rtv8.png)
			  
			  \#\# [ ](\#4-apache-jmeter) **4\. [Apache JMeter](https://jmeter.apache.org/)** 
			  
			  The best tool to perform load testing and to measure performance.  
			  
			  It was originally designed for testing Web Applications but has since expanded to other test functions.
			  
			  You can use it to test SOAP, REST, FTP, Database, LDAP, TCP, SMPT, etc.
			  
			  Other features include:  
			  • Full featured Test IDE  
			  • Ability to load test many different applications/server/protocol types   
			  • A complete and ready to present dynamic HTML report  
			  • Extract data from popular response formats (HTML, JSON , XML, etc) 
			  
			  [![Apache JMeter example](https://proxy-prod.omnivore-image-cache.app/0x0,sFPH7QApzyc3ZMZfcbdiETdRUu1KrX1ezPX7t5qXaVb4/https://res.cloudinary.com/practicaldev/image/fetch/s--utthSrlC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/8v2yw2h743hkb58cofqg.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--utthSrlC--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/8v2yw2h743hkb58cofqg.png)
			  
			  \#\# [ ](\#5-grabber) **5\. [Grabber](https://tools.kali.org/web-applications/grabber)** 
			  
			  A useful web application scanner.   
			  Now you can add Security Testing to your checklist.  
			  
			  Features include:  
			  • Cross-Site Scripting  
			  • SQL Injection   
			  • File Inclusion  
			  • Backup files check  
			  • Simple AJAX check   
			  • Hybrid analysis/Crystal ball testing for PHP application using PHP-SAT  
			  • JavaScript source code analyzer  
			  • JavaScript with JavaScript Lint
			  
			  [![Grabber Security](https://proxy-prod.omnivore-image-cache.app/0x0,sheZ6WCukzSE3Ctc0E2ZeperCnNWKkSlcAUDvkur9XH8/https://res.cloudinary.com/practicaldev/image/fetch/s--L_Mw2zQf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/j5dgpgoo77q5exz3x82o.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--L%5FMw2zQf--/c%5Flimit%2Cf%5Fauto%2Cfl%5Fprogressive%2Cq%5Fauto%2Cw%5F880/https://thepracticaldev.s3.amazonaws.com/i/j5dgpgoo77q5exz3x82o.png)
			  
			  \#\# Read next
			  
			  [ ![trinly01 profile image](https://proxy-prod.omnivore-image-cache.app/100x100,sA9cke7rL_VpAtkxA2YiC5B5mpFFNLwvI0An-x1kTT9c/https://res.cloudinary.com/practicaldev/image/fetch/s--crvp42wP--/c_imagga_scale,f_auto,fl_progressive,h_100,q_auto,w_100/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/257934/6725e89e-de78-4900-b708-e4bbb093e5c9.jpg) Data Transformation Magic: Simplify Complex Data with Remap.JS!  Trinmar Boado - ](https://dev.to/trinly01/data-transformation-magic-simplify-complex-data-with-remapjs-o1a) [ ![lixeletto profile image](https://proxy-prod.omnivore-image-cache.app/100x100,sXxc592Bi7Pdp1fPXBE8VInnhJwy46qmDmB9pukSMjcU/https://res.cloudinary.com/practicaldev/image/fetch/s--hOgIr7Rc--/c_imagga_scale,f_auto,fl_progressive,h_100,q_auto,w_100/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/393949/bdd2b0c8-ca93-4c85-9f92-4f62fe8fcb92.jpg) Construí o layout do site do Space Jam de 1996 com CSS moderno 🤯  Camilo Micheletto - ](https://dev.to/lixeletto/construi-o-layout-do-site-do-space-jam-de-1996-com-css-moderno-mhl) [ ![pradumnasaraf profile image](https://proxy-prod.omnivore-image-cache.app/100x100,sQvqkGwtwC2TiqqTOwP_HRXzWzB7DzCI5dZzpsmjsxu4/https://res.cloudinary.com/practicaldev/image/fetch/s--mRwueeaB--/c_imagga_scale,f_auto,fl_progressive,h_100,q_auto,w_100/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/682769/87fdd296-6176-41b1-86a4-740375c3e6d2.jpg) Open Source SPL Makes Microservices More “Micro”  Pradumna Saraf - ](https://dev.to/pradumnasaraf/open-source-spl-makes-microservices-more-micro-5epi) [ ![ben profile image](https://proxy-prod.omnivore-image-cache.app/100x100,skhWti2rv4mLyzTHFnEunsjNZuZYA6cY3a1o2ru59md0/https://res.cloudinary.com/practicaldev/image/fetch/s--ezPgfAlx--/c_imagga_scale,f_auto,fl_progressive,h_100,q_auto,w_100/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/1/f451a206-11c8-4e3d-8936-143d0a7e65bb.png) Does an OpenAi outage have an impact on your workflow?  Ben Halpern - ](https://dev.to/ben/does-an-openai-outage-have-an-impact-on-your-workflow-3gph)
	- 20 Motivational Quotes For The Gym — Plus, 3 Fitness Influencers To Follow To Keep You Inspired & On Track | YourTango](https://omnivore.app/me/20-motivational-quotes-for-the-gym-plus-3-fitness-influencers-to-18bc48c679c)
	  collapsed:: true
	  site:: [YourTango](https://www.yourtango.com/2019327218/20-motivational-quotes-gym-plus-3-fitness-influencers-follow-keep-you-inspired-track)
	  author:: Carlie Fox
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Aug 17th, 2019]]
		- ### Content
			- There is nothing harder than finding the motivation to go to the gym — or choosing to eat that green salad over that greasy slice of pizza heaven.
			  
			  Especially during the summer months, the constant feeling to look a certain way and actively follow a healthy lifestyle is time consuming. We take longer over-thinking how we look and the food we eat instead of just living in the moment and enjoying every minute of our precious time.
			  
			  There are some incredible and strong women out there who put their fitness journey on the Internet to help inspire others.
			  
			  \#\# Living a healthy lifestyle is more than just eating your veggies and working out regularly — it is practicing self-love and giving yourself the pamper time you deserve.
			  
			  **RELATED:** [**50 Best Motivational Quotes To Use For Your Workout Selfie Instagram Caption**](https://www.yourtango.com/2018310393/motivational-quotes-instagram-caption-workout-selfie-gym-inspiration)
			  
			  There are many fitness bloggers I look up to for inspiration and motivation.
			  
			  **These women aren't perfect and don't want you to think their body is the ultimate goal — they want to create an environment where every woman is confident in her skin and lives the healthiest lifestyle that she can.**
			  
			  Here is a quick rundown of my top three favorite fitness bloggers and why they are truly so inspiring:
			  
			  \#\# 1\. Maryana Dvorska
			  
			  A fresh face to the fitness vlogging world, Maryana Dvorska has powered through countless weight loss slumps until she finally found her groove. The 23-year-old fitness guru has transformed both her physical and mental health over the course of three years. She created both a YouTube channel and an Instagram that revolve around her favorite workouts, quick and healthy meals, and clothing favorites.
			  
			  She is not shy when talking about topics that may be a little uncomfortable – not being able to digest dairy and having serious bloating problems being a few. She constantly shares recipes for those who don't want to break the bank buying healthier food alternatives and strives to make her followers feel they can relate to her and her weight loss story.
			  
			  She partners with Gym Shark and Women's Best (an active wear online clothing store and a health food line) to give her followers discounts on her fitness favorites and must-haves.
			  
			  Her videos are inspiring and uplifting, as well as super informative. Whether you're just starting your fitness journey or are a gym fiend, Dvorska is your go-to-gal for all things fitness goals.
			  
			  She also has the cutest style.
			  
			  **RELATED:** [**25 Relatable Quotes That Perfectly Describe The Struggle Of Trying To Lose Weight**](https://www.yourtango.com/2019325849/25-funny-weight-loss-quotes-perfectly-describe-struggle-trying-lose-weight)
			  
			  \#\# 2\. Whitney Simmons
			  
			  An actual ray of sunshine, Whitney Simmons is one of the most upbeat fitness vloggers out there. Her Instagram workout videos are short, sweet and will make you really feel the burn. She is constantly uploading "What I Eat in A Day" videos that teach women eating more isn't always a bad thing. Her meal prep videos are bomb and always leave my mouth watering.
			  
			  She is also a Gym Shark partner and has promotions for her fans.
			  
			  Simmons also struggles with skin issues and psoriasis, but she never fears talking in detail about it with her followers. She preaches body positivity and loving the skin you're in. Her catch phrase, "It's a beautiful day to be alive," really shines through her videos and their overall message.
			  
			  She also always does clothing hauls that feature styles for women with curves. She is strong and is always posting videos encouraging her followers to take a few extra minutes out of their day to raise their heart rate and get active.
			  
			  **RELATED:** [**10 Smart Ways To Feel Confident In The Gym (When You Have No Idea What You're Doing)**](https://www.yourtango.com/2019325308/10-ways-feel-confident-gym-when-you-have-no-ides-what-youre-doing)
			  
			  Not always a fitness vlogger, Remi started out making YouTube videos revolved around dance, makeup, fashion, and her typical day-to-day life. It was about two years ago that she wanted to turn her unhealthy habits around and start taking care of her body.
			  
			  While she doesn't classify her videos as just fitness content, she does make videos about her current diet and workout routines. She has been very open with her followers, especially on her second channel, about her struggles with weight loss and body confidence.
			  
			  Cruz relates to so many women out there who have fallen off the wagon while trying to lose weight. She finally found her favorite meals and exercises that she shares freely with her audience. Soul Cycle is her go-to workout, but she also shows her gym routine and favorite meal prep ideas.
			  
			  She doesn't preach about being a certain size, nor does she want to be. She radiates confidence and happiness to her fans and her vlogs will make you want to get up at 6 a.m. to hit the gym.
			  
			  She is the definition of a person living their best life.
			  
			  **RELATED: [5 Lower-Calorie Summer Cocktail Drink Recipes That Won't Mess Up Your Diet](https://www.yourtango.com/2019325077/best-low-calorie-summer-cocktail-recipes-alcoholic-mixed-drinks-weight-loss-goals)**
			  
			  **If these women aren't enough to give you the push you need to get your butt to the gym, here are some fitness quotes that might become your new healthy lifestyle mantra. Take a look at these motivational quotes to keep you inspired on your journey to health and wellness:**
			  
			  \#\# 1\. Time to get those gains, baby.
			  
			  _"Wake up beauty, it's time to beast" –_ **Unknown**
			  
			  \#\# **2\. You get what you wish for, right?**
			  
			  _"Don't wish for a good body, work for it" –_ **Unknown** 
			  
			  \#\# **3\. Living a healthier life and gaining confidence takes power mentally and physically.**
			  
			  _"Master your mindset and you'll master your body" –_ **Unknown**
			  
			  \#\# **4\. There isn't just one stop you need to get to, it is a constant journey.**
			  
			  _"Fit is not a destination. It is a way of life" –_ **Unknown**
			  
			  \#\# 5\. Do you want that Kim K booty?
			  
			  _"You don't get the butt you want by sitting on it" –_ **Unknown**
			  
			  \#\# 6\. Every achieved goal matters, no matter how small.
			  
			  _"Slow progress is better than no progress. Stay positive and don't give up" –_ **Unknown**
			  
			  \#\# **7\. It's time to get ripped.**
			  
			  _"Hustle for that muscle"_ – **Unknown** 
			  
			  \#\# **8\. Your body is much stronger than you think.**
			  
			  _"One of the greatest pleasures in life is realizing that two weeks ago your body couldn't do what it just did"_ – **Unknown**
			  
			  \#\# **9\. Gain the confidence you need for you and nobody else.**
			  
			  _"I'm working on myself for myself by myself" –_ **Unknown**
			  
			  \#\# 10\. Get your hair up and your confidence out.
			  
			  _"If at first you don't succeed, fix your ponytail and try again" –_ **Unknown**
			  
			  **RELATED: [10 Mindfulness Exercises For Losing That Emotional Weight In Your Heart](https://www.yourtango.com/experts/susie-barolo/mindfulness-exercises-emotional-wellness-mental-health)**
			  
			  \#\# **11\. It will be the greatest love you ever feel.**
			  
			  "Fall in love with taking care of your body" – **Unknown**
			  
			  \#\# **12\. Because... why not?**
			  
			  _"Do it for the after selfie" –_ **Unknown**
			  
			  \#\# 13\. Your body is your forever home.
			  
			  Related Stories From YourTango:
			  
			  _"Take care of your body. It's the only place you have to live" –_ **Jim Rohn**
			  
			  \#\# **14\. Feel lucky to be here, doing what you do.**
			  
			  _"It's a beautiful day to be alive" –_ **Whitney Simmons**
			  
			  \#\# 15\. Three months or three days from now, you'll be happy you started.
			  
			  _"Three months from now, you will thank yourself"_ **– Unknown** 
			  
			  \#\# 16\. You don't need a magic mirror to motivate you.
			  
			  _"Mirror mirror on the wall, I'll always get up after I fall. And whether I run, walk, or have to crawl, I'll set my goals and achieve them all" –_ **Unknown**
			  
			  \#\# 17\. You want a strong and faithful relationship with your body.
			  
			  _"Fitness is like a relationship. You can't cheat and expect it to work" –_ **Unknown**
			  
			  \#\# 18\. Take the hour you'd be watching Netflix and put it towards a workout.
			  
			  _"It'd not about having time, it's about making time" –_ **Unknown** 
			  
			  \#\# 19\. Who doesn't want to sparkle?
			  
			  _"I don't sweat, I sparkle" –_ **Unknown**
			  
			  \#\# 20\. Never let anyone dull your shine and motivation.
			  
			  _"People who say it can't be done shouldn't interrupt those who are doing it"_ **– Unknown**
			  
			  **RELATED:** [**How The Dangerous Lie Exposed By Nike's Plus Size Mannequin Keeps Fat People Stressed & Depressed**](https://www.yourtango.com/2019325589/nike-plus-size-mannequin-breaks-fat-shaming-stereotypes-about-healthy-bmi-obesity)
			  
			  _Carlie Fox is a writer who covers Bible and astrology, pop culture and relationship topics._
	- 5 Off The Beaten Track Las Vegas Attractions | News](https://omnivore.app/me/5-off-the-beaten-track-las-vegas-attractions-news-18bc48c6631)
	  collapsed:: true
	  site:: [Daily Hive](https://dailyhive.com/vancouver/5-beaten-track-las-vegas-attractions)
	  author:: DH Vancouver Staff
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Dec 19th, 2017]]
		- ### Content
			- ![5 Off The Beaten Track Las Vegas Attractions](https://proxy-prod.omnivore-image-cache.app/0x0,seBTgza9YeteMPJmQBiqqdPAiu6PqO6f_OEeWx-kZujg/https://assets.dailyhive.com/assets/2.3.16/static/pictures/nav/channels/news-nav-2x.jpg)
			  
			  While The Strip may be the end all to many Las Vegas getaways, past those glitzy few miles is a remarkably different city. I searched for the real Vegas, past the glamour and decadence, and found a fascinating mix of cultural renewal and urban deterioration within Sin City.
			  
			  Here are the top 5 cultural experiences and attractions that truly defined Las Vegas for me:
			  
			  \#\# 1\. Container Park
			  
			  [![2014-05-07 18.22.39](https://proxy-prod.omnivore-image-cache.app/1024x768,s1oOhoRVY2mEKbQGv29DQntgs2mGHfF8a2WWZYn5kknY/https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-18.22.39.jpg)](https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-18.22.39.jpg)
			  
			  Zappos CEO Tony Hsieh’s well-publicized [Downtown Project](http://downtowncontainerpark.com/) has culminated in this segregated block of downtown Las Vegas, amidst abandoned casinos and empty parking lots. Within an oasis of modified shipping containers and friendly guards, you can feel a new optimism for Vegas’ urban renewal. Hipster boutiques and quirky decor transported me to the promise of a new micro utopia – not quite realized, but impressive considering spirit and age of the place.
			  
			  \#\# 2\. Downtown Las Vegas
			  
			  [![2014-05-07 18.35.36](https://proxy-prod.omnivore-image-cache.app/1024x768,s2OGKx5QJkNSGD6W3emYvqYsoN6wqar0Z0xYDk0HHYZ8/https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-18.35.36.jpg)](https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-18.35.36.jpg)
			  
			  Compared to The Strip, walking through downtown Las Vegas was startling and surreal. A focus on tourism, coupled with collapse and slow recovery of the housing market, has resulted in a post-apocalyptic emptiness to the central core. While pockets of revitalization are emerging, the whole of downtown is dead, abandoned, and in varying stages of decay. A fascinating urban landscape.
			  
			  \#\# 3\. Fremont Street Experience
			  
			  [![2014-05-07 17.42.50](https://proxy-prod.omnivore-image-cache.app/1024x768,sif8fUxnJi5HgU_nwrnnRDJ5qQpnx1Oa-nF8Kg-GEM6Y/https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-17.42.50.jpg)](https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-17.42.50.jpg)
			  
			  Tourists missing the Vegas of yester-year may appreciate the [Fremont Street Experience](http://www.vegasexperience.com/), which is rough around the edges and filled with nostalgic references. From go-go dancers atop outdoor bars, to street performers of all types, the cheesy, seen-better-days entertainment side of Vegas has found new life here. The Fremont Street Experience culminates in a spectacular lights show every night at dusk.
			  
			  \#\# 4\. 18b Las Vegas Art District
			  
			  [![2014-05-07 17.58.03](https://proxy-prod.omnivore-image-cache.app/1024x768,sEPpWx1fSP7QICKgB4tqz9-WBhGrvxnqWjbVSRhxOPZg/https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-17.58.03.jpg)](https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-17.58.03.jpg)
			  
			  One of the other odd pockets of rejuvenation downtown is Las Vegas’ fledgling arts district, coined ’18b’ for its 18-block radius. Unlike Container Park, no hard limits or barriers separate this part of town, but you definitely know once you’re in it. Hip and casual dress, bumping music, and bars without signs mark prime hipster territory. The monthly “First Friday” festival draws up to 20,000 visitors to browse, buy, eat, drink and mingle in this new arts & culture hot spot.
			  
			  \#\# 5\. Viva Las Arepas
			  
			  [![2014-05-07 19.58.57](https://proxy-prod.omnivore-image-cache.app/1024x768,sxLrZiLTkg1bymkjDiCYjkAxicAcdMFE_3vy0goSAaFM/https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-19.58.57.jpg)](https://images.dailyhive.com/vancitybuzz/uploads/2014/05/2014-05-07-19.58.57.jpg)
			  
			  The fifth experience on my list has nothing to do with arts & culture – except for the fact that if you’re going to make the trek all the way downtown, then you’d better find yourself at [Viva Las Arepas](http://www.vivalasarepas.com/).
			  
			  Viva Las Arepas is a no-frills Venezuelan eatery bursting with flavour and authenticity. The place is named for its _arepas_, an amazing corn tortilla bursting with meaty fillings. It reminded me of the cross between a baked pie (crust) and a samosa (flavour). The arepas were just the tip of the iceberg – I also sampled empandas and spicy chicken wings to tasty success. In over-priced, over-touristy Vegas, Viva Las Arepas was a steal. And oh yes, was the food worth it all right.
			  
			  Photo Credit: Cecilia Lu.
	- How to harness Google Photos to transform your messy pictures | WIRED UK](https://omnivore.app/me/how-to-harness-google-photos-to-transform-your-messy-pictures-wi-18bc48c3a4b)
	  collapsed:: true
	  site:: [WIRED UK](https://www.wired.co.uk/article/google-photos-tips-storage)
	  author:: Matt Burgess
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Jul 27th, 2019]]
		- ### Content
			- ![How to harness Google Photos to transform your messy pictures](https://proxy-prod.omnivore-image-cache.app/0x0,skSFTSZK74g1NCFFtZSZ6IGxtQ7uGZf2kfcukoo3nBrQ/https://media.wired.co.uk/photos/606d9b2389f3babb1f0130b8/master/w_1600%2Cc_limit/wired-google-photos.jpg)
			  
			  Google
			  
			  Google's online power is immense. As well as having market dominance with its Chrome web browser and Gmail app, it's now managed to develop the world's most used photo storage website in Google Photos.
			  
			  The picture storage service, which is available through its website or app, has passed the one billion user threshold. In doing so it's become Google's ninth service to have upwards of a billion people using it. (As well as email and web browsing, Android, Drive, Maps, its Play Store, plus Search and YouTube easily pass the seven-digit monthly user figure).
			  
			  Photos' growth has been rapid, reaching the colossal user figure in just under four years. However, it has been given a helping hand from Google's existing dominance within other services.
			  
			  Android has played a crucial role in its rise as the Photos app comes installed on every device running the OS. And it probably helped that Google gives people unlimited photo uploads for free, as long as the pictures are under 16MP. Photos had a humble beginning, though. It was spun out of Google+'s (RIP) photo sharing option and consumed Picasa in 2016 when the service was shutdown.
			  
			  The app's popularity doesn't really help when it comes to your messy collection of photos though. The ways we use our smartphone cameras – for taking quick pictures of items we want to buy, a ton of selfies, and grabbing screenshots of memes – doesn't lend itself to a neat set of photos. Here's how to use Photos to take some control of your picture archives.
			  
			  \#\# Free up space
			  
			  The growth of cloud storage means there's less reliance on storing every document, picture and movie on your phone. However, this doesn't mean that our devices don't get full quickly. Most mid range phones now come with around 64GB of internal storage – including our [best budget phone](https://www.wired.co.uk/article/best-budget-phone) the Motorola Moto G7\. While this may sound a lot, with our increasing picture and video demands it can be quickly filled up.
			  
			  The first thing you should do with Google Photos on your phone is turn backups on. This means your pictures and videos will be saved in the cloud and accessible through the web. However, perhaps Google Photos' most useful tool for getting your digital life in order is the capability to reclaim some of the storage on your smartphone.
			  
			  Within the app's menu there's the option to 'free up space'. In reality, this means backing-up your pictures to Google Photos. Selecting the option will wipe the original photos from your phone and store them remotely.
			  
			  \#\# Get rid of your screenshots
			  
			  The screenshots we take often have a limited shelf-life. You want to quickly share a person's terrible Twitter opinion with friends without the risk of RTing the post. Once the moment has passed, the screenshots are largely useless. Through Photos' [search](https://photos.google.com/search?pli=1) you can select all images deemed to be a screengrab. Select each photo by tapping or clicking on it and then batch delete all the useless images for good.
			  
			  \#\# Make sure everything is rotated
			  
			  Landscape or portrait? Whatever you pick for your images – never film video vertically – some will end-up upside down. Within Photos' [settings](https://photos.google.com/settings) there's the option for its AI bot, the [Google Assistant](https://www.wired.co.uk/article/google-assistant-voice-commands-actions-google-home), to notify you about photos that aren't quite right. Turn it on and you'll get prompts to rotate pictures that haven't turned-out the way you envisioned.
			  
			  The Assistant will also, if you turn it on, make collages, animations and attempt to apply filters to your pictures. They're all a little cheesy though, so for your notifications' sake it's probably best to disable the feature.
			  
			  \#\# Search for anything
			  
			  Apple and Google both apply their machine learning to your photos. As well as detecting faces, they're also able to identify pretty much any object you've taken a picture of. Search for fire, you'll get pictures of candles; look up wine and you'll be shown all those blurry memories. At the end of 2017 there was [mild outrage](https://www.theguardian.com/technology/shortcuts/2017/oct/31/apple-can-see-bra-photos-app-recognises-brassiere) when people discovered the companies knew what a bra looked like.
			  
			  The power of Photos' search is impressive. It is able to identify pretty much any object or activity you search for. Can't remember when you went camping? Tap tents into the search box and the pictures will come up. The search also works with locations if your pics have the correct metadata attached to them. If you select all the photos with dogs in, it's possible to create an album (using the + icon) to easily group all your favourite puppers in one place.
			  
			  There's one big difference between Google and Apple's photo classification. Because Apple controls its hardware platforms as well as iOS, it can use its machine learning algorithms on your device. The data doesn't directly go to Apple's servers; this isn't the case with Google which analyses your pictures in the cloud.
			  
			  \#\# Automatically share your photos
			  
			  We take photos because we want other people to see them. To encourage this, Google Photos allows people to automatically share photos with your friends and family. Within the iOS and Android app, the sharing options sit at the bottom of the screen within their own tab: once you enter the mode you'll be given recommendations, based on faces or places, of the photos you may want to share.
			  
			  Within the settings menu there's the option to share your uploaded pictures with other people. Select who you want to share pictures with and you're then given the option to let them view everything, photos of certain people, or photos since a certain date. They can download the pictures or add them to their own albums.
			  
			  While you're in the settings menu you should turn on the 'remove geo-location in items shared by link' option to strip location data from the pictures you're passing to friends.
			  
			  \#\# Find people's faces
			  
			  Google no longer thinks of itself as a search company – instead it self-defines as an artificial intelligence business. As a result, it's applied its machine learning to pretty much all areas of its business: translation, self-driving cars, and automatically generated email responses, to name a few.
			  
			  Photos is no different. Uploading your pictures to Google's platform means that the company's algorithms will pour through them to detect faces. If you allow 'face grouping' in Google Photos' settings it will automatically collect all the pictures of the same person together.
			  
			  To see this in action, visit the [photos search page](https://photos.google.com/search?pli=1) and you'll be presented with a row of your friends, family and pets' faces. From here it is possible to label the people (and animals) that have been identified. This means when you search in future, you can type in a name and see all the images of that person.
			  
			  This power is also the big downside of Google Photos. While you're getting the service for free, your images are helping to train and improve the Silicon Valley firm's algorithms. Google's data security is good, but it's not the best for your privacy. You and your pictures are the product.
			  
			  All the information you provide Google feeds into its data-hungry business model of helping to sell personalised adverts. If Google knows you visited London because of your pictures, it can help it to sell more specific adverts. The more information it has on you, the more money it can make from the adverts it sells. (It is possible to [delete your Google history](https://www.wired.co.uk/article/how-to-delete-google-search-history-tracking) to help prevent tracking).
			  
			  _Updated August 1, 2019 10:50: Storage capacities of phones have been updated_
			  
			  More great stories from WIRED
			  
			  **🕵🏿 It's time you ditched Chrome for a [privacy-first web browser](https://www.wired.co.uk/article/best-privacy-browsers-and-chrome-alternatives?utm%5Fsource=More%20Stories&utm%5Fmedium=internal)**
			  
			  **🚕 London's minicabs have a [cunning plan to beat Uber](https://www.wired.co.uk/article/london-minicabs-taxis-uber-igo-merger?utm%5Fsource=More%20Stories&utm%5Fmedium=internal)**
			  
			  **🎉 A vaccine for Alzheimer's is [on the verge of reality](https://www.wired.co.uk/article/alzheimers-vaccine-united-neuroscience?utm%5Fsource=More%20Stories&utm%5Fmedium=internal)**
			  
			  **🤦🏽 Reddit’s ‘Am I the Asshole’ is your [new guilty pleasure](https://www.wired.co.uk/article/reddit-aita-obsessions?utm%5Fsource=More%20Stories&utm%5Fmedium=internal)**
			  
			  **📧 Get the [best tech deals and gadget news in your inbox](https://www.wired.co.uk/newsletters?utm%5Fsource=More%20Stories&utm%5Fmedium=internal)**
	- CrossFit Star Mat Fraser's Tips Will Help You Own Any WOD](https://omnivore.app/me/cross-fit-star-mat-fraser-s-tips-will-help-you-own-any-wod-18bc48c304b)
	  collapsed:: true
	  site:: [Men's Health](https://www.menshealth.com/fitness/a28367324/mat-fraser-crossfit-tips/)
	  author:: Ebenezer Samuel, C.S.C.S.
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Jul 28th, 2019]]
		- ### Content
			- ![Barechested, Muscle, Physical fitness, Chest, Abdomen, Arm, Bodybuilding, Human body, Trunk, Model, ](https://proxy-prod.omnivore-image-cache.app/2667x4000,sWQuW88d9dvPjirZUmtuc7tUPqS6sCqFTTl7QXNOzgJo/https://hips.hearstapps.com/hmg-prod/images/hlh070119bodymatfraser01-1562872855.jpg?crop=1.00xw:0.668xh;0,0.146xh&resize=640:* "Barechested, Muscle, Physical fitness, Chest, Abdomen, Arm, Bodybuilding, Human body, Trunk, Model, ")
			  
			  Hamish Brown
			  
			  Every summer, top CrossFitters compete in “the Games,” a five-day, 14-event CrossFit challenge that’s like the decathlon, American Ninja Warrior, and World’s Strongest Man mashed into one ab-tastic sweatfest. 
			  
			  Last year, the final event was called Aeneas because ... well, Aeneas was a war hero, and presumably ripped. It involved five pegboard climbs, requiring you to hold a dowel in each hand and jab your way up an eight-foot board; 40 thrusters, front squats to overhead presses with an 85-pound barbell; and three 33-foot loaded yoke carries, in which you shoulder a crossbar attached to a base. Weight is added for each subsequent carry, starting at 425 pounds and going to 665 pounds. The event separates the very fit from the impossibly fit. 
			  
			  It also separates the impossibly fit from Mathew Fraser. Fraser is the three-time defending CrossFit Games champion, and he’ll chase his fourth straight title when the Games begin in Wisconsin later this month. And the 5-7, 195-pound bowling ball of a man tears it up in all CrossFit events, from the Olympic lifts like the snatch to the gymnastics maneuvers like the muscle up, to all the work capacity events there are.
			  
			  That’s exactly why Men’s Health asked Fraser to give us his best CrossFit wisdom for some of the sport’s trickiest lifts. His tips will help you destroy your next WOD. 
			  
			  \#\# The Snatch
			  
			  ![Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ](https://proxy-prod.omnivore-image-cache.app/4000x1300,sfq86ScAV7JWzseQsYVAv4cRnqKc83kVTYfnlYdnJQVk/https://hips.hearstapps.com/hmg-prod/images/matt-fraser-final2-1562874489.jpg?crop=0.240xw:0.783xh;0.207xw,0.101xh&resize=980:* "Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ")
			  
			  Pete Sucheski
			  
			  **What Is It?** The most technically demanding move in CrossFit (and Olympic weightlifting), the snatch has you lifting a loaded barbell from the ground to overhead, all in one motion. The move can be also be done with kettlebells or dumbbells. Fraser’s career-best barbell snatch: 315 pounds. 
			  
			  **What Fraser Says:** “Keep your arms absolutely straight and locked out through the first part of the lift so all the power from your hips will translate upward.” 
			  
			  \#\# The Cardio Row
			  
			  ![Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ](https://proxy-prod.omnivore-image-cache.app/4000x1300,stL_84TcH1wCptnB0dSWhSdOv7wx8eOUOSA1WHaujvNA/https://hips.hearstapps.com/hmg-prod/images/matt-fraser-final2-1562874574.jpg?crop=0.351xw:1.00xh;0.625xw,0&resize=980:* "Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ")
			  
			  Pete Sucheski
			  
			  **What Is It?** The cardio row machine mimics what it’s like to row a boat, and it’s become a staple exercise in most CrossFit boxes. It’s typically done on Concept2 rowers. A typical CrossFit workout has you row a set distance or number of calories. In 2018, the CrossFit Games included a marathon row, which had athletes rowing a whopping 42,195 meters. (Need a comparison? Try rowing a mere 500 meters and see how your body feels. Fraser completed the marathon row in 2:48.36\. 
			  
			  **What Fraser Says:** “Start coiled close to the rower, then extend your legs, then your hips, then follow through with your arms. It will feel unnatural.” (Need a rowing workout? Check [these](https://www.menshealth.com/fitness/g19543037/4-rowing-workouts-that-burn-fat-build-muscle/) out.) 
			  
			  \#\# The Assault Bike
			  
			  ![Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ](https://proxy-prod.omnivore-image-cache.app/4000x1300,sJpj3yHI65Ohl9EkJBFEN75AGOHJM2veIyztGsEcsgE4/https://hips.hearstapps.com/hmg-prod/images/matt-fraser-final2-1562874608.jpg?crop=0.216xw:0.804xh;0,0.0635xh&resize=980:* "Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ")
			  
			  Pete Sucheski
			  
			  **What Is It?** This cardio crusher has your feet pedaling and your arms pulling against variable resistance, typically for a set number of calories. The harder you push, the harder the bike is to pedal, creating a unique movement that never feels easily. It’s best used in short bursts. Fraser’s one-minute Airbike record is 84 calories in 60 seconds. 
			  
			  **What Fraser Says:** “Embrace the pain and just go hard. Keep telling yourself the pain will leave in a little bit, because you’re never on there for long. There is no form.” 
			  
			  \#\# The Muscle-Up
			  
			  ![Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ](https://proxy-prod.omnivore-image-cache.app/4000x1300,sPB4ilqg1oO7O3l5A7qFAZ8ARovzQ-1m1-xcPuM8j3Fw/https://hips.hearstapps.com/hmg-prod/images/matt-fraser-final2-1562874647.jpg?crop=0.187xw:1.00xh;0.442xw,0&resize=980:* "Physical fitness, Weightlifting, Strength athletics, Exercise, Lunge, Strength training, Sports equipment, ")
			  
			  Pete Sucheski
			  
			  **What Is It?** An advanced movement, the muscle-up requires that you start hanging from a bar or rings then pull your entire torso above it, locking your arms out at the top. There are several variations of the muscle-up. CrossFit is known for having athletes do kipping, explosive muscle-ups. There’s also the strict muscle-up shown in the video below. Fraser can rip through 30 kipping muscle-ups in 2:00.16.
			  
			  **What Fraser Says:** “Do not try ring muscle-ups until you are very good at pullups, strict pullups. Learn pullups first, then muscle-ups will come.”   
			  
			  ![preview for The Muscleup | Hero Moves](https://proxy-prod.omnivore-image-cache.app/2000x1000,sw-erY3kXOfOvY5QfxWpqHd_P_KcuMz569oWK7QcQuEQ/https://hips.hearstapps.com/vidthumb/images/2019-menshealth-heromoves-ep04-muscleup-tx-v10-prores-1548952377.jpg?crop=1.00xw:1.00xh;0,0&resize=1200:* "Video player poster image")
			  
			  ![Headshot of Ebenezer Samuel,  C.S.C.S.](https://proxy-prod.omnivore-image-cache.app/0x0,s0IyRD4z_7Xw6DQnwMTDpBTYs1p-JE8w3JnLwMvKTesc/https://hips.hearstapps.com/rover/profile_photos/7f8a292d-0a87-46e6-aa03-8181dd17b09d_1543349049.file?fill=1:1&resize=120:* "Headshot of Ebenezer Samuel,  C.S.C.S.")
			  
			  Ebenezer Samuel, C.S.C.S., is the fitness director of _Men's Health_ and a certified trainer with more than 10 years of training experience. He's logged training time with NFL athletes and track athletes and his current training regimen includes weight training, HIIT conditioning, and yoga. Before joining Men's Health in 2017, he served as a sports columnist and tech columnist for the New York Daily News.
	- 17-year-old shark bite victim plans to be 'same old Paigey' despite injuries](https://omnivore.app/me/17-year-old-shark-bite-victim-plans-to-be-same-old-paigey-despit-18bc48c2c58)
	  collapsed:: true
	  site:: [WRAL.com](https://www.wral.com/17-year-old-shark-bite-victim-plans-to-be-same-old-paigey-despite-injuries/18452081/)
	  author:: WRAL
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Jun 14th, 2019]]
		- ### Content
			- ---
			  
			  By 
			  
			  Matthew Burns
			  
			  , WRAL.com senior producer/politics editor
			  
			  **GREENVILLE, N.C.** — A North Carolina teen who was attacked by a shark two weeks ago says she doesn't plan to let her injuries get in the way of living her life.
			  
			  Paige Winter, 17, had her left leg amputated above the knee and suffered extensive injuries to her hands following the [June 2 attack near Fort Macon State Park](https://www.wral.com/story/officials-17-year-old-bitten-by-marine-animal-off-atlantic-beach/18426852/). She will likely be released next week from Vidant Medical Center in Greenville, where surgeons, her nurse and her father discussed the incident and her recovery during a Friday afternoon news conference.
			  
			  Charlie Winter, her father, recounted the attack in harrowing detail, describing how a fun day at the beach with the family suddenly turned tragic and, if not for a precise chain of events, could have become deadly.
			  
			  Paige was among a group of teens and children playing in waist-deep water, and Winter said he ran out to try to scare them. Moments later, he saw that his daughter was missing and that the water looked pink, he said, so he dove under at the site, grabbed her with his left arm as she was being pulled backward and lifted her out of the water.
			  
			  "It was just an immediate dad thing," he said of diving in. "When I pulled her up, a shark came up with her, and it was a big shark. The head was, it was a big shark. It kind of thrashed a little bit, and it had a big, just a big eye staring at you."
			  
			  Winter said he beat the shark repeatedly "with everything I could" while holding on to Paige, and the shark finally let go. He said he could see her leg was seriously injured, so he used his training as a paramedic and squeezed her thigh as tightly as possible to stanch the bleeding as he ran with her back to the beach.
			  
			  "The shark was at arm's length behind me, chasing us. I didn't know that until later. I also didn't know that there were two sharks out there," he said. "I've been told \[the shark\] was the length of a car. I don't know, I didn't see it. All I saw was its head, and it was impressive."
			  
			  During the attack, he said, his 15-year-old daughter grabbed a 7-year-old boy and pushed him toward the shore and away from the shark.
			  
			  Paige never screamed or cried at any point during the ordeal, he said, describing her as "completely calm." She merely said, "Dad" every few seconds, he said, to which he replied, "I got you. I got you."
			  
			  When Winter was about 10 feet from the shore, he said, other people rushed in to start helping, carrying her to the beach, offering towels and calling for help. A man walking by even gave up his belt for a tourniquet, he said.
			  
			  "Names I don't know. Names I'll never know. Just a link in a chain," he said, his voice cracking. "It's very odd that every single thing that had to happen when it had to happen happened. I don't know why it happened, but it happened, and she's here."
			  
			  Winter said he was no longer a paramedic at this point but was merely a father trying to squeeze as much as possible out of what he feared were his final minutes with his oldest daughter.
			  
			  "I grabbed her by her right hand, and I don't think I've ever told any of my children I loved them so much so many times. I wanted her to know," he said. "I listened to every word she said, every movement of her lip, every way the wind blew her hair. I just took it all in."
			  
			  ![Charlie Winter displays a T-short made in honor of his 17-year-old daughter, Paige, who was attacked by a shark.](https://proxy-prod.omnivore-image-cache.app/0x0,s6SiS1zOJ2B_Y4Fd_EdCY9onDQHO-Icbbijn1uiEbD74/https://wwwcache.wral.com/asset/news/local/2019/06/14/18452360/tshirt-DMID1-5j51oy8zu-640x360.jpg "Paige Winter T-shirt")
			  
			  Winter had that phrase on the back of a T-shirt he wore to the news conference that had an image of a girl and a shark shaking hands on the front and a silhouette of a shark on the back with "Paige - 1, Shark - 0" on it.
			  
			  Paige didn't attend the news conference, but Vidant officials videotaped a statement from her.
			  
			  "I want people to know that I'm doing all right and to know I'm still going to be able to do all the stuff that they can do," she said. "I'm going to be able to do just kind of like everything, just same old Paigey."
			  
			  She said she was grateful for the support of other shark bite victims and amputees and said she plans to turn a tragic situation into a positive.
			  
			  "When I was in that water, I was praying, 'I'm 17 years old, and I've got so much to do,'" she said. "Seventeen-year-old lost a leg, and we're popping. I think I can transform this into something good for me and good for sharks and good for the environment."
			  
			  "Sharks are still good people, and that's just kind of the truth," she said. "They're still so good, and they're so cool. They're pretty cool, good people."
			  
			  Winter and Paige's surgeons and nurse marveled at the teen's positive attitude and determination following what trauma surgeon Dr. Eric Toschlog called a "devastating injury" to her leg.
			  
			  "She puts a smile on my face every single time I walk in the room," nurse Brianna Springer said, adding that she learned from Paige how to give herself pep talks after hard days.
			  
			  Dr. Richard Zeri, a plastic surgeon who helped rebuild Paige's hands through a series of surgeries, opened the news conference by forwarding a message from Paige to "Ironman" actor Robert Downey Jr. that she wanted him to follow her on Instagram.
			  
			  "She's quite a young lady," Zeri said.
			  
			  In addition to her left leg, Paige lost the ring finger and pinkie on her left hand, and Zeri had to transplant tissue onto the hand to restore some of what was lost.
			  
			  Winter said the hand injuries came from the shark's teeth as Paige tried to wrench the creature's mouth open after she was bitten.
			  
			  "Those teeth are very, very sharp. It looks like there was a knife was taken to \[her hands\]," Zeri said, noting her tendons, nerves and other tissue were shredded.
			  
			  Vidant surgeons see two to three shark attack victims a year, but Toschlog said he's never seen injuries as severe as Paige's. Based on the circumstances of the incident, he said, he suspects it was a bull shark that attacked her.
			  
			  Despite the attack, Toschlog and Winter said people should live their lives and not be afraid to go to the beach and play in the water if that's what they want to do. The chance of dying from a shark attack is one in 1.37 million, compared with one in 84 for dying in a car crash, Toschlog said.
			  
			  "These are extraordinarily rare events," he said, noting only four shark-related deaths were recorded worldwide last year. "I would advise: Get in the water."
			  
			  "You've got to live your life. Paige is," Winter said.
			  
			  Paige is expected to be released from Vidant next week, Toschlog said, but will spend a couple of weeks in a rehabilitation facility to get fitted for a prosthetic leg and to begin to learn to walk with it before heading home. Zeri said getting function back in her hands will take six to 12 months of therapy.
			  
			  Winter said he can't wait to get his daughter home so she can get on with her life.
			  
			  "Paige is destined for great things, and she's going to do great things," he said. "I don't know what her future holds. ... I do know it's extremely bright, and it has to be bright because someone like that, it's the only thing it can be."
			  
			  \#\# More On This
			  
			  \#\# • Credits 
			  
			  Copyright 2023 by Capitol Broadcasting Company. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.
	- Linux for beginners: 10 more commands for manipulating files | Enable Sysadmin](https://omnivore.app/me/linux-for-beginners-10-more-commands-for-manipulating-files-enab-18bc48c03c4)
	  collapsed:: true
	  site:: [Enable Sysadmin](https://www.redhat.com/sysadmin/10-more-commands-terminal)
	  author:: tcarriga
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 11th, 2020]]
		- ### Content
			- In case you missed the first segment of this two-part series, we looked at [10 commands to get you started at the terminal](https://www.redhat.com/sysadmin/10-commands-terminal). Now, we are going to explore 10 additional commands that you can use to continue your command line journey. Specifically, I want to look at the commands used to manipulate files. We are going to copy, move, and rename files, plus a few commands for reading files in different ways.
			  
			  \#\# Commands to know
			  
			  \#\#\# 1\. cp file1 file2
			  
			  The **copy** command is used to copy the contents of one file to another. You can also use this command to copy a file from one directory into another.
			  
			  ```angelscript
			  [tcarrigan@server community_content]$ ls -l
			  total 36
			  -rw-rw-r--. 1 tcarrigan tcarrigan    5 Feb  6  2020 article
			  -rw-rw-r--. 1 root      tcarrigan    5 Feb  6  2020 article2
			  -rw-rw-r--. 1 root      tcarrigan    5 Feb  6  2020 article3
			  -rw-rw-r--. 1 tcarrigan tcarrigan    5 Feb  6  2020 article4
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Apr  7  2020 podman_pulling
			  -rw-rw-r--. 1 tcarrigan tcarrigan 8824 Apr  7  2020 real_sysadmins
			  [tcarrigan@server community_content]$ cp podman_pulling article
			  [tcarrigan@server community_content]$ ls -l
			  total 40
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:11 article
			  -rw-rw-r--. 1 root      tcarrigan    5 Feb  6  2020 article2
			  -rw-rw-r--. 1 root      tcarrigan    5 Feb  6  2020 article3
			  -rw-rw-r--. 1 tcarrigan tcarrigan    5 Feb  6  2020 article4
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Apr  7  2020 podman_pulling
			  -rw-rw-r--. 1 tcarrigan tcarrigan 8824 Apr  7  2020 real_sysadmins
			  
			  ```
			  
			  As you see, I copied the **podman\_pulling** article contents to the **article** file. Now, the other thing you can do with this command is copy an existing file into another directory. For example, if I wanted to copy the **podman\_pulling** article to my home directory, I would do something like this:
			  
			  ```angelscript
			  [tcarrigan@server community_content]$ cp podman_pulling /home/tcarrigan/
			  ** Navigate to /home/tcarrigan **
			  [tcarrigan@server ~]$ ls -l
			  total 8
			  drwxrwxr-x. 4 tcarrigan tcarrigan   50 Feb  6  2020 article_submissions
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Desktop
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Documents
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Downloads
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Music
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Pictures
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:17 podman_pulling
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Public
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Templates
			  drwxr-xr-x. 2 tcarrigan tcarrigan    6 Jan 27  2020 Videos
			  
			  ```
			  
			  You can see that **podman\_pulling** is now available in my `/home/tcarrigan` directory.
			  
			  _**\[ You might also like: [10 handy systemd commands: A reference](https://www.redhat.com/sysadmin/systemd-commands) \]**_
			  
			  \#\#\# 2\. mv file1 file2
			  
			  The **move** command allows a user to move or rename a file. To move a file, you would use the following:
			  
			  ```angelscript
			  [tcarrigan@server ~]$ mv podman_pulling article_submissions/my_articles/
			  [tcarrigan@server ~]$ ls -l article_submissions/my_articles/
			  total 20
			  -rw-rw-r--. 1 tcarrigan tcarrigan 4442 Apr  7  2020 Creating_physical_volumes
			  -rw-rw-r--. 1 tcarrigan tcarrigan 2744 Apr  7  2020 Creating_volume_groups
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:17 podman_pulling
			  
			  ```
			  
			  I moved the **podman\_pulling** article from my home directory over to `/article_submissions/my_articles`. Now, let’s say we needed to rename the article from "**podman\_pulling**" to "**rootless podman**." How would we do that?
			  
			  ```angelscript
			  [tcarrigan@server my_articles]$ mv podman_pulling rootless_podman
			  [tcarrigan@server my_articles]$ ls -l
			  total 20
			  -rw-rw-r--. 1 tcarrigan tcarrigan 4442 Apr  7  2020 Creating_physical_volumes
			  -rw-rw-r--. 1 tcarrigan tcarrigan 2744 Apr  7  2020 Creating_volume_groups
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:17 rootless_podman
			  
			  ```
			  
			  \#\#\# 3\. ln -s file link
			  
			  The **link** command in this context will allow us to create a soft link or symbolic link to another file or directory. For more information on this topic, check out my previous article, _[Linking Linux](https://www.redhat.com/sysadmin/linking-linux-explained)_. For now, I am just going to demonstrate how to use the soft link command.
			  
			  ```angelscript
			  [tcarrigan@server ~]$ ln -s article_submissions/community_content/podman_pulling podman_article
			  [tcarrigan@server ~]$ ls -l
			  total 0
			  drwxrwxr-x. 4 tcarrigan tcarrigan 50 Feb  6  2020 article_submissions
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Desktop
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Documents
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Downloads
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Music
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Pictures
			  lrwxrwxrwx. 1 tcarrigan tcarrigan 52 Oct 27 13:51 podman_article -> article_submissions/community_content/podman_pulling
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Public
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Templates
			  drwxr-xr-x. 2 tcarrigan tcarrigan  6 Jan 27  2020 Videos
			  
			  ```
			  
			  This creates a link in my home directory to the `/home/tcarrigan/article_submissions/community_content/podman_pulling` file, which means that you can read **podman\_article**_,_ and it will open the file inside the **community\_content** directory.
			  
			  \#\#\# 4\. touch file
			  
			  Use the **touch** command to create or update a file (here, I am creating **new\_file** in the **my\_articles** directory).
			  
			  ```angelscript
			  [tcarrigan@server my_articles]$ touch new_file
			  [tcarrigan@server my_articles]$ ls -l
			  total 20
			  -rw-rw-r--. 1 tcarrigan tcarrigan 4442 Apr  7  2020 Creating_physical_volumes
			  -rw-rw-r--. 1 tcarrigan tcarrigan 2744 Apr  7  2020 Creating_volume_groups
			  -rw-rw-r--. 1 tcarrigan tcarrigan    0 Oct 28 16:47 new_file
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:17 rootless_podman
			  
			  ```
			  
			  Note the timestamp of the file created is 16:47\. I am now going to use the **touch** command to update the timestamp to the current time (7:35pm).
			  
			  ```angelscript
			  [tcarrigan@server my_articles]$ touch new_file 
			  [tcarrigan@server my_articles]$ ls -l
			  total 20
			  -rw-rw-r--. 1 tcarrigan tcarrigan 4442 Apr  7  2020 Creating_physical_volumes
			  -rw-rw-r--. 1 tcarrigan tcarrigan 2744 Apr  7  2020 Creating_volume_groups
			  -rw-rw-r--. 1 tcarrigan tcarrigan    0 Oct 28 19:35 new_file
			  -rw-rw-r--. 1 tcarrigan tcarrigan 6404 Oct 26 19:17 rootless_podman
			  
			  ```
			  
			  \#\#\# 5\. cat > file
			  
			  This command is used to input standard output into a file. I will input "Hello World" into the file **test\_file**_._
			  
			  ```elixir
			  [tcarrigan@server ~]$ cat > test_file
			  Hello World 
			  
			  ```
			  
			  Use **Ctrl+D\*** to finish editing the file. To view your work, you can use the standard `cat filename` syntax.
			  
			  ```elixir
			  [tcarrigan@server ~]$ cat test_file 
			  Hello World
			  
			  ```
			  
			  \#\#\# 6\. more file
			  
			  The **more** command will allow the user to view the contents of a file one screenful at a time. Here we are going to look at the `/article_submissions/my_articles/creating_physical_volumes` file.
			  
			  ```applescript
			  \#  How to create a physical volume in Linux using LVM
			  by Tyler Carrigan
			  
			  Physical volumes `PV` are the base 'block' that you need in order to manipulate a disk using Logical Volume Manager `LVM`. Now, let's not rush ahead. What exactly is a physical volume? What in the world i
			  s LVM? In short, LVM is a type of storage virtualization that allows operators far more flexibility in storage management than standard partitioning. A physical volume is any physical storage device, such
			  as an Hard Disk Drive `HDD`, Solid State Drive `SSD`, or partition, that has been initialized as a physical volume with LVM. Without properly initialized physical volumes, you cannot create Volume Groups
			  or logical volumes. 
			  
			  So lets get started! First, there are a few considerations.
			  
			  Dont try to pinpoint the exact amount of space you need down to the nearest byte. The reason for this is that LVM places labels on the physical volumes `UUID` as well as metadata storage. While this doesn
			  t take up very much space, understand that if you initialize a 1Gb PV, you do **not** have 1Gb of *usable* space.
			  ** Output Omitted **
			  
			  ```
			  
			  You can go line by line with the **Enter** key or page by page with **Space**. Another helpful option is the **\-number** option that allows you to specify the number of lines displayed per page.
			  
			  To view 10 lines at a time, you might use `more -10 filename`. You can use the **+** sign to specify which line to start from: `more +10 filename`.
			  
			  \#\#\# 7\. less file
			  
			  The **less** command allows the user the same functionality as **more**. However, it is faster because it does not load the entire file but instead allows the user to parse the file using the arrow keys. This is especially noticeable in large log files.
			  
			  ```routeros
			  [tcarrigan@server my_articles]$ sudo less /var/log/messages
			  Oct 26 19:50:47 server dbus-daemon[939]: [system] Activating via systemd: service name='org.freedesktop.nm_dispatcher' unit='dbus-org.freedesktop.nm-dispatcher.service' requested by ':1.15' (uid=0 pid=1155 comm="/usr/sbin/NetworkManager --no-daemon " label="system_u:system_r:NetworkManager_t:s0")
			  Oct 26 19:50:47 server systemd[1]: Starting Network Manager Script Dispatcher Service...
			  Oct 26 19:50:47 server dbus-daemon[939]: [system] Successfully activated service 'org.freedesktop.nm_dispatcher'
			  Oct 26 19:50:47 server systemd[1]: Started Network Manager Script Dispatcher Service.
			  ** Output Omitted **
			  
			  ```
			  
			  \#\#\# 8\. head file
			  
			  The **head** command allows a user to output the first 10 lines of any file. I will use the example of `creating_physical_volumes`.
			  
			  ```elixir
			  [tcarrigan@server my_articles]$ head Creating_physical_volumes
			  ```
			  
			  ```sql
			  \#  How to create a physical volume in Linux using LVM
			  by Tyler Carrigan
			  
			  Physical volumes `PV` are the base 'block' that you need in order to manipulate a disk using Logical Volume Manager `LVM`. Now, let's not rush ahead. What exactly is a physical volume? What in the world is LVM? In short, LVM is a type of storage virtualization that allows operators far more flexibility in storage management than standard partitioning. A physical volume is any physical storage device, such as an Hard Disk Drive `HDD`, Solid State Drive `SSD`, or partition, that has been initialized as a physical volume with LVM. Without properly initialized physical volumes, you cannot create Volume Groups or logical volumes. 
			  
			  So lets get started! First, there are a few considerations.
			  
			  Dont try to pinpoint the exact amount of space you need down to the nearest byte. The reason for this is that LVM places labels on the physical volumes `UUID` as well as metadata storage. While this doesnt take up very much space, understand that if you initialize a 1Gb PV, you do **not** have 1Gb of *usable* space. 
			  
			  Also, although LVM allows you to create physical volumes using multiple partitions, it is recommended that you use a single partition for a PV. This is for a couple of reasons. 
			  [tcarrigan@server my_articles]$
			  
			  ```
			  
			  \#\#\# 9\. tail file
			  
			  The **tail** command allows the user to view the last 10 lines of any file. Here we will look at `/var/log/messages`.
			  
			  ```routeros
			  [tcarrigan@server my_articles]$ sudo tail /var/log/messages
			  [sudo] password for tcarrigan:  
			  Oct 28 20:17:47 server org.gnome.Shell.desktop[2053]: Window manager warning: last_user_time (9080279) is greater than comparison timestamp (9080269).  This most likely represents a buggy client sending inaccurate timestamps in messages such as _NET_ACTIVE_WINDOW.  Trying to work around...
			  Oct 28 20:17:47 server org.gnome.Shell.desktop[2053]: Window manager warning: W1 appears to be one of the offending windows with a timestamp of 9080279.  Working around...
			  Oct 28 20:17:48 server dbus-daemon[948]: [system] Activating via systemd: service name='net.reactivated.Fprint' unit='fprintd.service' requested by ':1.500' (uid=0 pid=5259 comm="sudo tail /var/log/messages " label="unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023")
			  Oct 28 20:17:48 server systemd[1]: Starting Fingerprint Authentication Daemon...
			  Oct 28 20:17:49 server dbus-daemon[948]: [system] Successfully activated service 'net.reactivated.Fprint'
			  Oct 28 20:17:49 server systemd[1]: Started Fingerprint Authentication Daemon.
			  Oct 28 20:17:53 server org.gnome.Shell.desktop[2053]: Window manager warning: last_user_time (9086680) is greater than comparison timestamp (9086677).  This most likely represents a buggy client sending inaccurate timestamps in messages such as _NET_ACTIVE_WINDOW.  Trying to work around...
			  Oct 28 20:17:53 server org.gnome.Shell.desktop[2053]: Window manager warning: W1 appears to be one of the offending windows with a timestamp of 9086680.  Working around...
			  Oct 28 20:18:00 server org.gnome.Shell.desktop[2053]: Window manager warning: last_user_time (9093426) is greater than comparison timestamp (9093424).  This most likely represents a buggy client sending inaccurate timestamps in messages such as _NET_ACTIVE_WINDOW.  Trying to work around...
			  Oct 28 20:18:00 server org.gnome.Shell.desktop[2053]: Window manager warning: W1 appears to be one of the offending windows with a timestamp of 9093426.  Working around...
			  [tcarrigan@server my_articles]$ 
			  
			  ```
			  
			  \#\#\# 10\. tail -f file
			  
			  The **\-f** variant of the **tail** command is an entirely different take on the original command. This flag allows the user to see the file as it’s being written. This is incredibly useful for troubleshooting startup/shutdown issues for applications and systems alike.
			  
			  \#\# Now you know
			  
			  If you followed along with this tutorial, you should have another 10 commands in your terminal arsenal. From moving, copying, creating, and updating to various ways to read file contents, you should feel comfortable moving about the filesystem and manipulating the files and directories that you come across. If this stuff seems complicated, just keep practicing, and you’ll be a more confident user in no time. For more tips, tricks, and tools, check back with us and Enable Sysadmin.
			  
			  _**\[ Download now: [A sysadmin's guide to Bash scripting](https://opensource.com/downloads/bash-scripting-ebook?intcmp=701f20000012ngPAAQ). \]**_ 
			  
			  Don't fear the command line. Embrace it with these 10 starter commands.
			  
			  Your bash history maintains a record of the commands you've entered. Here's how to make good use of that record.
			  
			  Here are 11 Linux commands that one sysadmin cannot live without.
	- JSON | PyCharm Documentation](https://omnivore.app/me/json-py-charm-documentation-18bc48c00b4)
	  collapsed:: true
	  site:: [PyCharm Help](https://www.jetbrains.com/help/pycharm/json.html)
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 21st, 2023]]
		- ### Content
			- \#\# JSON 
			  
			  Last modified: 23 August 2023
			  
			  The [JSON](https://www.json.org/) format is commonly used for storing data and for configuration files. PyCharm helps you work with JSON files — it checks their syntax and formatting. In popular types of configuration files, PyCharm provides code completion, thanks to the [JSON Schema](http://json-schema.org/), which is a special format for describing the structure and contents of such files. You can also use custom JSON Schemas to enable code completion in your JSON files and validate them.
			  
			  \#\# Enabling JSON5 
			  
			  PyCharm recognizes a number of most popular JSON standards including [JSON5](https://json5.org/). PyCharm by default treats files with the json5 extension as JSON5 files and supports this new syntax in them.
			  
			  ![Configuration file that uses JSON5 with the default extension .json5](https://proxy-prod.omnivore-image-cache.app/600x220,sKp6sbXEzFzW_ilznkqLA3_DTgp3nr_xU6Kjqjt-D0FY/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_five.png "Configuration file that uses JSON5 with the default extension .json5")
			  
			  \#\#\# Extend the JSON5 syntax to all JSON files 
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Editor | File Types.
			  2. In the Recognized File Types list, select JSON5.
			  3. In the File Name Patterns area, click and type `*.json` in the Add Wildcard dialog that opens.
			  
			  \#\# Using schemas from JSON Schema Store 
			  
			  PyCharm can automatically download and use schemas from the [JSON Schema Store](http://schemastore.org/json/) that hosts schema files for many popular configuration files. As soon as you open a file whose name is associated with one of the available schemas (for example, tslint.json), PyCharm downloads and uses this schema for it. The name of the applied schema is shown on the Status bar.
			  
			  ![JSON schema downloaded from JSON Schema Store, the name of the applied schema is shown in the Status bar](https://proxy-prod.omnivore-image-cache.app/600x145,sumvT-NebRGo8XICIhxOzv0yskijYhfWRyEZm46hTB-k/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_schema_from_store_status_bar.png "JSON schema downloaded from JSON Schema Store, the name of the applied schema is shown in the Status bar")
			  
			  If your configuration file has a custom name, or you are working with a [scratch file](https://www.jetbrains.com/help/pycharm/scratches.html), click No JSON schema on the Status bar and select the required schema from the list or click New Schema Mapping to open the JSON Schema Mappings page and [configure a new custom schema](https://www.jetbrains.com/help/pycharm/json.html\#ws%5Fjson%5Fschema%5Fadd%5Fcustom).
			  
			  ![No JSON schema for the current file](https://proxy-prod.omnivore-image-cache.app/600x137,sjSuHMYhTKYg9XfLdNVUw6p_oFLFLAcHNYrDnJct8JeQ/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_schema_no_schema_status_bar.png "No JSON schema for the current file")
			  
			  > \#\#\# tip
			  > 
			  > Schemas from the JSON Schema Store can be applied to YAML files as well.
			  
			  By default, automatic download of Schemas from the JSON Schema Store is enabled. If it was turned off, you can enable it again at any time.
			  
			  \#\#\# Enable automatic download schemas from the JSON Schema Store 
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Languages & Frameworks | Schemas and DTDs | Remote JSON Schemas.
			  2. Select the Allow downloading JSON schemas from remote sources and the Use schemastore.org JSON Schema catalog checkboxes.
			  
			  PyCharm comes bundled with a number of popular schemas. Although these schemas are automatically updated on a regular basis they still may happen to be outdated.
			  
			  \#\#\# Use the up-to-date versions of bundled schemas 
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Languages & Frameworks | Schemas and DTDs | Remote JSON Schemas.
			  2. Select the Always download the most recent version of schemas checkbox.
			  
			  \#\# Using custom JSON schemas 
			  
			  Besides [schemas from JSON Schema Store](https://www.jetbrains.com/help/pycharm/json.html\#ws%5Fjson%5Fusing%5Fschemas), PyCharm lets you configure and use custom schemas from other storages. You can download the required schema and store it under the project root or specify the URL of the resource so PyCharm can download the schema automatically.
			  
			  > \#\#\# tip
			  > 
			  > Custom schemas must meet the JSON schema standards. Currently, PyCharm supports schemas [draft-07](https://json-schema.org/draft-07/json-schema-release-notes.html) and earlier.
			  
			  \#\#\# Configure a custom JSON schema 
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Languages & Frameworks | Schemas and DTDs | JSON Schema Mappings.
			  2. In the central pane, that shows all your previously configured custom Schemas, click on the toolbar.
			  3. Specify the name of the Schema and the [Schema Specification version](http://json-schema.org/specification-links.html) with which your Schema complies. In the Schema file or URL field, specify the location of a previously downloaded Schema file or type the URL at which the required schema is available.  
			  > \#\#\# tip  
			  >  
			  > If you specify a URL, make sure the Allow downloading JSON schemas from remote sources checkbox on the Remote JSON Schemas page is selected.
			  4. Create a list of files or folders that you want to be validated against this Schema. Based on the list, PyCharm internally detects the files to be validated.  
			  The list may contain the names of specific files, the names of entire directories, and filename patterns. Use the following rules to specify filename patterns:  
			   * `role-*` matches all files with the names that start with `role-`.  
			   * `role-*/**/*.yaml` matches all .yaml files with names that contain `role`, `/`, and `/`.  
			   * `role-**.yaml` matches all .yaml files with names that start with `role-`.  
			  To add an item to the list, click and specify the path to a file or folder or type a file pattern.  
			  > \#\#\# tip  
			  >  
			  > PyCharm searches for files and folders with the specified names only within the current project, so you do not need to specify full paths to files and folders.
			  
			  \#\#\# Enable automatic download of JSON schemas from remote sources 
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Languages & Frameworks | Schemas and DTDs | Remote JSON Schemas.
			  2. Select the Allow downloading JSON schemas from remote sources.  
			  When the checkbox is cleared, any network activity around JSON Schemas, including schemas from the JSON Schema Store, is disabled.
			  
			  \#\# Handling conflicts among scopes of schemas 
			  
			  A conflict arises when a file, or a folder, or a pattern belongs to the scopes of two or more schemas. PyCharm analyzes scopes in two modes:
			  
			  * Static Analysis detects conflicts in scopes of custom schemas. If a conflict is detected, PyCharm displays a warning in the Schema Details pane. To view the overlapping scopes, click the Show details link. PyCharm shows a popup with a message where the conflicting scopes and schemas are listed:  
			  ![Notification about conflicting schema scopes in Settings dialog](https://proxy-prod.omnivore-image-cache.app/600x299,srvlLI0B8US7GS1LacuUHNQAABTxio2iKtD6KkHoZcSs/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_schemas_conflict_of_scopes.png "Notification about conflicting schema scopes in Settings dialog")
			  * Dynamic Analysis detects conflicts in scopes of both system and custom schemas. This type of analysis is started when you open a file that belongs to a certain scope. If a conflict is detected, PyCharm displays a warning at the top of the editor tab:  
			  ![Notification about conflicting schema scopes in the  editor](https://proxy-prod.omnivore-image-cache.app/600x173,sl_5AvlT7-WfuMp9bV7J3PJPtxyZuCOAhB7UO9qV8d4M/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_schemas_conflict_of_scopes_dynamic.png "Notification about conflicting schema scopes in the  editor")  
			  Click the link to open the JSON Schema Mappings page and edit the scope of the conflicting custom schema. Note that you cannot edit the scope of system schemas.
			  
			  \#\#  Using HTML descriptions in JSON schema 
			  
			  By default, PyCharm escapes HTML characters when displaying documentation for JSON schema definitions in documentation popups. To get nice looking documentation with rich HTML markup, store the HTML description in the `x-intellij-html-description` extension property instead of `description`.
			  
			  ```json
			  {
			  "id":  "http://some.site.somewhere/entry-schema\#",
			  "$schema":  "http://json-schema-org/draft-06/schema\#",
			  "type":  "object",
			  "required":  [ "options" ],
			  "properties":  {
			    "options":  {
			      "type":  "array",
			      "description": "Interesting details: Fresh New Awesome",
			      "minItems":  1,
			      "items":  { "type":  "string" },
			      "uniqueItems":  true,
			    },
			    "readonly":  { "type":  "boolean" }
			  }
			  }
			  ```
			  
			  ![No formatting in documentation for JSON schema definitions with description property](https://proxy-prod.omnivore-image-cache.app/600x181,sb1xBV9A2I8nrBflA0XjXULiON8UwT2fKb2kYZxqn1SA/https://resources.jetbrains.com/help/img/idea/2023.2/ws_json_quick_doc_description_property.png "No formatting in documentation for JSON schema definitions with description property")
			  
			  \#\# Configuring syntax highlighting 
			  
			  You can configure JSON-aware syntax highlighting according to your preferences and habits.
			  
			  1. In the Settings dialog (CtrlAlt0S), go to Editor | Color Scheme | JSON.
			  2. Select the color scheme, accept the highlighting settings inherited from the defaults or customize them as described in [Colors and fonts](https://www.jetbrains.com/help/pycharm/configuring-colors-and-fonts.html).
	- Data dashboarding tools | Streamlit v.s. Dash v.s. Shiny vs. Voila vs. Flask vs. Jupyter](https://omnivore.app/me/data-dashboarding-tools-streamlit-v-s-dash-v-s-shiny-vs-voila-vs-18bc48bff6f)
	  collapsed:: true
	  site:: [datarevenue.com](https://www.datarevenue.com/en-blog/data-dashboarding-streamlit-vs-dash-vs-shiny-vs-voila)
	  author:: byMarkus Schmitt
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- Comparing data dashboarding tools and frameworks
			  
			  [](\#)![](https://proxy-prod.omnivore-image-cache.app/0x0,snfD9XQApRoJLE6__zrMPOoT39xGQe9RUS3-r8owZEYM/https://assets-global.website-files.com/5d3ec351b1eba4332d213004/5f772982445b5615eb871491_Dashboard.png)![](https://proxy-prod.omnivore-image-cache.app/0x0,s36JrUVxMLcvf_AzV4AsaURhHG6ezNTek4WS-Jc1IbPA/https://global-uploads.webflow.com/5d3af5ca4e11726adbd659b2/5d3b06736e3cf0299a0dde71_shadow-pattern.svg)
			  
			  ![A graph showing the GitHub star history of Viola, Dash, Shiny, Streamlit, Jupyter, and Panel.](https://proxy-prod.omnivore-image-cache.app/0x0,sxmMIWHWBtZ9_NLSfNw5G04nrLAmiZ7CZYqdvfNrK3UA/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5f99e10dafbd69a99c875340_C8_qX8dvzv60T4LVZ9GftX-ZH-VJzq3sjUroWWH5XSWw8RFHnCCPPrC6jB3EFVuQdwiqhoEMQKFV-dFz7t6fqaRpSZGvBKI0i1Utj38_j9a54GXMuzi1BiepdIMjOK4ATVdF2131.png)
			  
			  Over the last three years, Dash and Streamlit have surged in popularity as all-in-one dashboarding solutions.
			  
			  \#\# **Data dashboards – Tooling and libraries**
			  
			  Nearly every company is sitting on valuable data that internal teams need to access and analyze. Non-technical teams often request tooling to make this easier. Instead of having to poke a data scientist for every request, these teams want dynamic dashboards where they can easily run queries and see custom, interactive visualizations.
			  
			  ![A representation of an unhappy person looking at code and a happy person looking at a neat dashboard.](https://proxy-prod.omnivore-image-cache.app/0x0,sUsKqs7G1o-LScIxisgib3LjClA_eSkgNwLv6tmzNWZ4/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5f77265696bfa94c739ba0c2_TLv7TGplTgg3oqi47Oc042fEPb-c-maCuH8Y2IiXI4P8Yv8pPTlkeM7IT6zhwp0OmTk_gWn0B9c_0wmDgcsKh3b4koNqH-WA32VqaZJzyTW-aUhf9Kz8a_PcP-e0VlepkzfS3qbo.png)
			  
			  Data dashboards can make data more accessible to your non-technical teams
			  
			  A data dashboard consists of many different components. It needs to:
			  
			  * **Analyze:** Manipulate and summarize data using a backend library such as Pandas.
			  * **Visualize:** Create plots and graphs of the data using a graphing library such as Bokeh.
			  * **Interact:** Accept user input using a frontend library such as React.
			  * **Serve:** Listen for user requests and return webpages using a web server such as Flask.
			  
			  In the past, you’d have had to waste a significant amount of time writing all the “glue” code to join these components together. But with newer libraries like Streamlit and Dash, these components come in a single package.
			  
			  Still, figuring out which library to use can be challenging. Here’s how they compare as well as some guidance on how to choose which one is best for your project.
			  
			  Do you want more detailed tooling comparisons that cut through the marketing-speak?
			  
			  Sign up to our weekly newsletter.
			  
			  Thank you! Your submission has been received!
			  
			  Oops! Something went wrong while submitting the form.
			  
			  \#\# **Just tell me which one to use**
			  
			  As always, “it depends” – but if you’re looking for a quick answer, you should probably use:
			  
			  * **Dash** if you already use Python for your analytics and you want to build production-ready data dashboards for a larger company.
			  * **Streamlit** if you already use Python for your analytics and you want to get a prototype of your dashboard up and running as quickly as possible.
			  * **Shiny** if you already use R for your analytics and you want to make the results more accessible to non-technical teams.
			  * **Jupyter** if your team is very technical and doesn’t mind installing and running developer tools to view analytics.
			  * **Voila** if you already have Jupyter Notebooks and you want to make them accessible to non-technical teams.
			  * **Flask** if you want to build your own solution from the ground up.
			  * **Panel** if you already have Jupyter Notebooks, and Voila is not flexible enough for your needs.
			  
			  \#\# **Quick overview**
			  
			  Not all the libraries are directly comparable. For example, Dash is built on top of Flask, and Flask is a more general framework for web application development. Similarly, each library focuses on a slightly different area.
			  
			  * **Streamlit, Dash**, and **Panel** are full dashboarding solutions, focused on Python-based data analytics and running on the **Tornado** and **Flask** web frameworks.
			  * **Shiny** is a full dashboarding solution focused on data analytics with R.
			  * **Jupyter** is a notebook that data scientists use to analyze and manipulate data. You can also use it to visualize data.
			  * **Voila** is a library that turns individual Jupyter notebooks into interactive web pages.
			  * **Flask** is a Python web framework for building websites and apps – not necessarily with a data science focus.
			  
			  Some of these libraries have been around for a while, and some are brand new. Some are more rigid, and have their own structure, while others are flexible and can adapt to yours. Some focus on specific languages. Here’s a table showing the tradeoffs:
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,szOtmLF1Jr11EOsXd98NMKnsgTmwJNV35FsALfyIMg-o/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5fa3b0d4a2043bcf84d49134_z87mnMfsPGOF7L3sGULQBusJnJTWGZHWtoizufuDR1q1A6JggFWO9IYHXf8wFyqgKhuG6hEGOPM4Acb-qmNXxwCFW95DPX9r7Syewkejb7itbmm8I_os2XI8bightYGJq7Gg-FXo.png)
			  
			  We’ve compared these libraries on:
			  
			  * **Maturity:** Based on the age of the project and how stable it is.
			  * **Popularity:** Based on adoption and GitHub stars.
			  * **Simplicity:** Based on how easy it is to get started using the library.
			  * **Adaptability:** Based on how flexible and opinionated the library is.
			  * **Focus:** Based on what problem the library solves.
			  * **Language support:** The main languages the library supports.
			  
			  These are not rigorous or scientific benchmarks, but they’re intended to give you a quick overview of how the tools overlap and how they differ from each other. For more details, see the head-to-head comparison below.
			  
			  \#\# **Streamlit vs. Dash**
			  
			  Streamlit and Dash are the two most similar libraries in this set. They are both full dashboarding solutions built with Python, and both include components for data analysis, visualization, user interaction, and serving. 
			  
			  Although they’re both open source, Dash is more focused on the enterprise market and doesn’t include all the features (such as job queues) in the open source version. By contrast, Streamlit is fully open source. 
			  
			  Streamlit is more structured and focused more on simplicity. It only supports Python-based data analysis and has a limited set of widgets (for example, sliders) to choose from.
			  
			  Dash is more adaptable. Although it’s built with Python and pushes users towards its own plotting library (Plotly), it’s also compatible with other plotting libraries and even other languages, such as R or Julia. 
			  
			  * **Use** **Streamlit** if you want to get going as quickly possible and don’t have strong opinions or many custom requirements.
			  * **Use Dash** if you need something more flexible and mature, and you don’t mind spending the extra engineering time.
			  
			  \#\# **Streamlit vs. Shiny**
			  
			  Streamlit is a dashboard tool based on Python, while Shiny uses R. Both tools focus on turning data analysis scripts into full, interactive web applications. 
			  
			  Because Python is a general-purpose language while R is focused solely on data analytics, the web applications you build with Streamlit (based on the Tornado web server) are more powerful and easier to scale to production environments than those built with Shiny. 
			  
			  Shiny integrates well with plotting libraries in the R ecosystem, such as ggplot2, while Streamlit integrates with Python plotting libraries such as Bokeh or Altair.
			  
			  * **Use** **Shiny** if you prefer doing data analysis in R and have already invested in the R ecosystem.
			  * **Otherwise use Streamlit** (or Dash – see above).
			  
			  \#\# **Streamlit vs. Voila** 
			  
			  Streamlit is a complete data dashboarding solution, while Voila is a simpler and more limited tool that lets you convert existing Jupyter Notebooks into basic data dashboards and serve them as web applications to non-technical users.
			  
			  Like Streamlit, Voila is built on top of the Tornado web framework, so you can use Jupyter notebooks along with Voila to get something broadly similar to Streamlit. But Streamlit is more flexible (it doesn’t require you to use Jupyter), while Voila can be simpler (provided you already have Jupyter Notebooks you want to present).
			  
			  Voila uses Jupyter’s widget library, while Streamlit uses custom widgets – so if you’re already familiar with Jupyter, you’ll find Voila easier to work with.
			  
			  * **Use Streamlit** If you’re looking for an all-in-one solution.
			  * **Use Voila** if you already have Jupyter Notebooks and are looking for a way to serve them.
			  
			  \#\# **Streamlit vs. Panel**
			  
			  **Streamlit** and **Panel** are both data dashboarding solutions, but **Panel** \- like Voila - integrates better with Jupyter Notebooks.
			  
			  Engineers can use either Streamlit or Panel to develop interactive data dashboards for non-technical users, but they might use Panel for internal data exploration as well.
			  
			  * **Use** **Streamlit** if you are looking for a more mature data dashboarding solution and your primary goal is to develop dashboards for non-technical people.
			  * **Use Panel** if you already use Jupyter Notebooks and need something more powerful than **Voila** to turn them into dashboards.
			  
			  \#\# **Streamlit vs. Jupyter Notebooks**
			  
			  Streamlit is a full data dashboarding solution, while Jupyter Notebooks are primarily useful to engineers who want to develop software and visualizations. Engineers use Streamlit to build dashboards for non-technical users, and they use Jupyter Notebooks to develop code and share it with other engineers.
			  
			  Combined with add-ons such as Voila, Jupyter Notebooks can be used similarly to Streamlit, but data dashboarding is not their core goal.
			  
			  * **Use Streamlit** if you need dashboards that non-technical people can use.
			  * ‍**Jupyter Notebooks** are best if your team is mainly technical and you care more about functionality than aesthetics.
			  
			  \#\# **Streamlit vs. Flask**
			  
			  Streamlit is a data dashboarding tool, while Flask is a web framework. Serving pages to users is an important but small component of data dashboards. Flask doesn’t have any data visualization, manipulation, or analytical capabilities (though since it’s a general Python library, it can work well with other libraries that perform these tasks). Streamlit is an all-in-one tool that encompases web serving as well as data analysis.
			  
			  * **Use Streamlit** if you want a structured data dashboard with many of the components you’ll need already included. Use Streamlitif you want to build a data dashboard with common components and don’t want to reinvent the wheel.
			  * **Use Flask** if you want to build a highly customized solution from the ground up and you have the engineering capacity.
			  
			  \#\# **Dash vs Panel**
			  
			  **Dash** and **Panel** are both data dashboarding solutions, but Panel integrates better with Jupyter Notebooks. While Dash lets developers customize the front-end by writing HTML and CSS, Panel automatically creates front-ends based only on Python syntax.
			  
			  Engineers can use either to build interactive dashboards for non-technical users, but they might use Panel for data exploration too.
			  
			  * **Use** **Dash** if you want to build more customized data dashboards for non-technical users.
			  * **Use** **Panel** if you already use Jupyter Notebooks and want more flexibility than is offered by Voila.
			  
			  \#\# **Dash vs. Shiny**
			  
			  Dash and Shiny are both complete data dashboarding tools, but Dash lives mainly in the Python ecosystem, while Shiny is exclusive to R. 
			  
			  Dash has more features than Shiny, especially in its enterprise version, and it's more flexible. Python is a general-purpose programming language, while R is focused solely on data analytics. Some data scientists prefer R for its mature libraries and (often) more concise code. Engineers usually prefer Python, since it conforms more closely to other languages.
			  
			  * **Use Dash** if your team prefers Python.
			  * **Use Shiny** if your team prefers R.
			  
			  \#\# **Dash vs. Voila and Jupyter Notebooks**
			  
			  Dash is an all-in-one dashboarding solution, while Voila can be combined with Jupyter Notebooks to get similar results. Dash is more powerful and flexible, and it’s built specifically for creating data dashboards, while Voila is a thin layer built on top of Jupyter Notebooks to convert them into stand-alone web applications.
			  
			  * **Use Dash** if you want to build a scalable, flexible data dashboarding tool.
			  * **Use Voila** if you have existing Jupyter Notebooks you want your non-technical teams to be able to use.
			  
			  \#\# **Dash vs. Flask**
			  
			  Dash is built on top of Flask and uses Flask as its web routing component, so it’s not very meaningful to compare them head-to-head. Dash is a data dashboarding tool, while Flask is a minimalist, generic web framework. Flask has no data analytics tools included, although it can work with other Python libraries that do analytics.
			  
			  * **Use Dash** if you want to build a data dashboard.
			  * **Use** **Flask** if you want to build a far more generic web application and to choose every component in it.
			  
			  \#\# **Shiny vs. Voila + Jupyter Notebooks**
			  
			  Shiny is a data dashboarding solution for R. While you can use Voila and Jupyter Notebooks with R, these are tools that focus primarily on the Python ecosystem.
			  
			  * **Use Shiny** if you already do your data analytics in R.
			  * **Use Voila** if you already have Jupyter Notebooks you want to make more accessible.
			  
			  \#\# **Shiny vs. Flask**
			  
			  Shiny is a data dashboarding tool built in R. Flask is a web framework built in Python. Shiny works well with R plotting libraries, such as ggplot2\. Flask doesn’t have any data analysis tools built in by default.
			  
			  * **Use Shiny** if you’re building a data dashboard and you want to do your data analysis with R.
			  * **Use Flask** if you want to build a generic web application from the ground up.
			  
			  \#\# **Voila vs. Flask**
			  
			  Voila is a library to convert Jupyter Notebooks to stand-alone web applications and serve them using Tornado. Like Tornado, Flask is a generic web framework. While it would be possible to use Flask to serve Jupyter Notebooks, you would have to reimplement most of the Voila library – so unless you have a very specific reason, it’s better to simply use Voila.
			  
			  \#\# **Final remarks**
			  
			  All the tools we’ve covered here can help you access the value locked away in your existing data. One common mistake we see teams make is getting too tied up in choosing which tools to use, rather than focusing on the data itself. While using the wrong tools can definitely hinder your analysis, it’s more common for teams to get bogged down by so-called [Bikeshedding](https://exceptionnotfound.net/bikeshedding-the-daily-software-anti-pattern/): spending too much time debating details that aren’t very important.
			  
			  If you’d like to chat about exploring your data and turning it into more revenue, [book a free call with our CEO](https://datarevenue.com/en-contact).
			  
			  \#\#\# Get Notified of New Articles
			  
			  Leave your email to get our weekly newsletter.
			  
			  Thank you! Your submission has been received!
			  
			  Oops! Something went wrong while submitting the form.
			  
			  \#\# Keep reading
	- A beginner’s guide to web scraping with Python and Scrapy](https://omnivore.app/me/a-beginner-s-guide-to-web-scraping-with-python-and-scrapy-18bc48bf65c)
	  collapsed:: true
	  site:: [The Next Web](https://thenextweb-com.cdn.ampproject.org/v/s/thenextweb.com/syndication/2020/11/23/a-beginners-guide-to-web-scraping-with-python-and-scrapy/amp/?amp_gsa=1&amp_js_v=a6&usqp=mq331AQFKAGwASA%3D)
	  author:: Live Code Stream
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 23rd, 2020]]
		- ### Content
			- Since their inception, websites are used to share information. Whether it is a Wikipedia article, YouTube channel, Instagram account, or a Twitter handle. They all are packed with interesting data that is available for everyone with access to the internet and a web browser.
			  
			  But, what if we want to get any specific data programmatically?
			  
			  There are two ways to do that:
			  
			  1. Using official API
			  2. Web Scraping
			  
			  The concept of API (Application Programming Interface) was introduced to exchange data between different systems in a standard way. But, most of the time, website owners don’t provide any API. In that case, we are only left with the possibility to extract the data using web scraping.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s5wXktLMLI54kjwEKwL99WOjicsppweOuRhCr0F9TDvo/https://s3.amazonaws.com/events.tnw/hardfork-2018/uploads/visuals/tnw-newsletter.png)
			  
			  The <3 of EU tech
			  
			  The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now!
			  
			  Basically, every web page is returned from the server in an HTML format, meaning that our actual data is nicely packed inside HTML elements. It makes the whole process of retrieving specific data very easy and straightforward.
			  
			  This tutorial will be an ultimate guide for you to learn web scraping using Python programming language. At first, I’ll walk you through some basic examples to make you familiar with web scraping. Later on, we’ll use that knowledge to extract data of football matches from Livescore.cz .
			  
			  _\[Read: [Neural’s market outlook for artificial intelligence in 2021 and beyond](https://thenextweb.com/news/neurals-market-outlook-for-artificial-intelligence-in-2021-and-beyond)\]_
			  
			  \#\# Getting Started
			  
			  To get us started, you will need to start a new Python3 project with and install Scrapy (a web scraping and web crawling library for Python). I’m using pipenv for this tutorial, but you can use pip and venv, or conda.
			  
			  _pipenv install scrapy_
			  
			  At this point, you have Scrapy, but you still need to create a new web scraping project, and for that scrapy provides us with a command line that does the work for us.
			  
			  Let’s now create a new project named web\_scraper by using the scrapy cli.
			  
			  If you are using pipenv like me, use:
			  
			  _pipenv run scrapy startproject web\_scraper ._
			  
			  Otherwise, from your virtual environment, use:
			  
			  _scrapy startproject web\_scraper ._
			  
			  This will create a basic project in the current directory with the following structure:
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/811x418,smg2Jebh-jKUWfNCTiDlFr62CBpr0K8-ohRAOtWFMcSU/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.39.37.png)
			  
			  \#\# Building our first Spider with XPath queries
			  
			  We will start our web scraping tutorial with a very simple example. At first, we’ll locate the logo of the [Live Code Stream](https://livecodestream.dev/) website inside HTML. And as we know, it is just a text and not an image, so we’ll simply extract this text.
			  
			  **The code**
			  
			  To get started we need to create a new spider for this project. We can do that by either creating a new file or using the CLI.
			  
			  Since we know already the code we need we will create a new Python file on this path **/web\_scraper/spiders/live\_code\_stream.py**
			  
			  Here are the contents of this file.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/825x578,sNW3sCOSwieeiniwedS30MZuEteXrVZxLOuPXlpbm_HA/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.40.43.png)
			  
			  **Code explanation:**
			  
			  * First of all, we imported the Scrapy library because we need its functionality to create a Python web spider. This spider will then be used to crawl the specified website and extract useful information from it.
			  * We created a class and named it LiveCodeStreamSpider. Basically, it inherits from scrapy.Spider and that’s why we passed it as a parameter.
			  * Now, an important step is to define a unique name for your spider using a variable called name. Remember that you are not allowed to use the name of an existing spider. Similarly, you can not use this name to create new spiders. It must be unique throughout this project.
			  * After that, we passed the website URL using the start\_urls list.
			  * Finally, create a method called parse() that will locate the logo inside HTML code and extract its text. In Scrapy, there are two methods to find HTML elements inside source code. These are mentioned below.
			  * CSS
			  * XPath
			  
			  You can even use some external libraries like BeautifulSoup and lxml . But, for this example, we’ve used XPath.  
			  A quick way to determine the XPath of any HTML element is to open it inside the Chrome DevTools. Now, simply right-click on the HTML code of that element, hover the mouse cursor over “Copy” inside the popup menu that just appeared. Finally, click the “Copy XPath” menu item.
			  
			  Have a look at the below screenshot to understand it better.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/700x356,sDy7ChIKrH9QBKFuzGrTc2UCeLt5U2xM49ZMT6Gxo_y4/https://livecodestream.dev/post/2020-11-18-how-to-turn-the-web-into-data-with-python-and-scrapy/find-xpath_hub7e3e64a73ee4298452ddd712fc8bae5_469803_700x0_resize_q75_box.jpg)
			  
			  Find XPath using Chrome Dev Tools
			  
			  By the way, I used `/text()` after the actual XPath of the element to only retrieve the text from that element instead of the full element code.
			  
			  **Note:** You’re not allowed to use any other name for the variable, list, or function as mentioned above. These names are pre-defined in Scrapy library. So, you must use them as it is. Otherwise, the program will not work as intended.
			  
			  **Run the Spider:**
			  
			  As we are already inside the **web\_scraper** folder in command prompt. Let’s execute our spider and fill the result inside a new file **lcs.json** using the below code. Yes, the result we get will be well-structured using JSON format.
			  
			  ```dockerfile
			  pipenv run scrapy crawl lcs -o lcs.json
			  
			  ```
			  
			  ```mipsasm
			  scrapy crawl lcs -o lcs.json
			  
			  ```
			  
			  **Results:**
			  
			  When the above code executes, we’ll see a new file **lcs.json** in our project folder.
			  
			  Here are the contents of this file.
			  
			  ```json
			  [
			  {"logo": "Live Code Stream"}
			  ]
			  
			  ```
			  
			  \#\# Another Spider with CSS query selectors
			  
			  Most of us love sports, and when it comes to Football, it is my personal favorite.
			  
			  Football tournaments are organized frequently throughout the world. There are several websites that provide a live feed of match results while they are being played. But, most of these websites don’t offer any official API.
			  
			  In turn, it creates an opportunity for us to use our web scraping skills and extract meaningful information by directly scraping their website.
			  
			  For example, let’s have a look at [Livescore.cz](https://www.livescore.cz/) website.
			  
			  On their home page, they have nicely displayed tournaments and their matches that will be played today (the date when you visit the website).
			  
			  We can retrieve information like:
			  
			  * Tournament Name
			  * Match Time
			  * Team 1 Name (e.g. Country, Football Club, etc.)
			  * Team 1 Goals
			  * Team 2 Name (e.g. Country, Football Club, etc.)
			  * Team 2 Goals
			  * etc.
			  
			  In our code example, we will be extracting tournament names that have matches today.
			  
			  \#\# The code
			  
			  Let’s create a new spider in our project to retrieve the tournament names. I’ll name this file as **livescore\_t.py**
			  
			  Here is the code that you need to enter inside **/web\_scraper/web\_scraper/spiders/livescore\_t.py**
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/809x603,sKjcNANKI9sChnJf4KPtiqihqf4ayhbsTB5GOGH3eMUE/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.42.05.png)
			  
			  \#\# Code explanation:
			  
			  * As usual, import Scrapy.
			  * Create a class that inherits the properties and functionality of **scrapy.Spider**.
			  * Give a unique name to our spider. Here, I used `LiveScoreT` as we will only be extracting the tournament names.
			  * The next step is to provide the URL of Livescore.cz.
			  * At last, the `parse()` function loop through all the matched elements that contains the **tournament name** and join it together using `yield`. Finally, we receive all the tournament names that have matches today. A point to be noted is that this time I used **CSS** selector instead of **XPath**.
			  
			  It’s time to see our spider in action. Run the below command to let the spider crawl the home page of Livescore.cz website. The web scraping result will then be added inside a new file called **ls\_t.json** in JSON format.
			  
			  ```arduino
			  pipenv run scrapy crawl LiveScoreT -o ls_t.json
			  
			  ```
			  
			  By now you know the drill.
			  
			  **Results:**
			  
			  This is what our web spider has extracted on 18 November 2020 from [Livescore.cz](https://www.livescore.cz/) . Remember that the output may change every day.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/795x448,slVZ-ZZRIGyGg4lvB36NdIwVUDToPMSNYksRPiSK9w2s/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.43.49.png)
			  
			  \#\# A more advanced use case
			  
			  In this section, instead of just retrieving the tournament name, we will go the next mile and get complete details of tournaments and their matches.
			  
			  Create a new file inside **/web\_scraper/web\_scraper/spiders/** and name it as **livescore.py**. Now, enter the below code in it.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/706x1536,spPpKYcWvTTQhu3nYsr_9_t65P420heuWIkVJoTvnfu4/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.44.47.png)
			  
			  \#\#\# Code explanation:
			  
			  The code structure of this file is the same as our previous examples. Here, we just updated the `parse()` method with a new functionality.
			  
			  Basically, we extracted all the HTML `<tr></tr>` elements from the page. Then, we loop through them to find out whether it is a tournament or a match. If it is a tournament, we extracted its name. In the case of a match, we extracted its “time,” “state,” and “name and score of both teams.”
			  
			  \#\#\# Run the example:
			  
			  Type the following command inside the console and execute it.
			  
			  ```dockerfile
			  pipenv run scrapy crawl LiveScore -o ls.json
			  
			  ```
			  
			  \#\#\# Results:
			  
			  Here is a sample of what has been retrieved:
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/702x990,s5s-isBfD-zAivOIRE0ZiDJtIivPrsm179KHNtJII4GQ/https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/11/Screenshot-2020-11-23-at-10.45.33.png)
			  
			  Now with this data, we can do anything we want, like use it to train our own neural network to predict future games.
			  
			  \#\# Conclusion
			  
			  Data Analysts often use **web scraping** because it helps them in collecting data to predict the future. Similarly, businesses use it to extract emails from web pages as it is an effective way of lead generation. We can even use it to monitor the prices of products.
			  
			  In other words, web scraping has many use cases and **Python** is completely capable to do that.
			  
			  So, what are you waiting for? Try scraping your favorite websites now.
			  
			  _This_ [_article_](https://livecodestream.dev/post/2020-11-18-how-to-turn-the-web-into-data-with-python-and-scrapy/) _was originally published on_ [_Live Code Stream_](https://livecodestream.dev/) _by_ [_Juan Cruz Martinez_](https://www.linkedin.com/in/bajcmartinez/) _(twitter:_ [_@bajcmartinez_](https://twitter.com/bajcmartinez)_), founder and publisher of Live Code Stream, entrepreneur, developer, author, speaker, and doer of things._
			  
			  [_Live Code Stream_](https://livecodestream.dev/subscribe) _is also available as a free weekly newsletter. Sign up for updates on everything related to programming, AI, and computer science in general._
			  
			  \#\# Get the TNW newsletter
			  
			  Get the most important tech news in your inbox each week.
			  
			  \#\# Also tagged with
	- Build 12 Data Science Apps with Python and Streamlit](https://omnivore.app/me/build-12-data-science-apps-with-python-and-streamlit-18bc48beb77)
	  collapsed:: true
	  site:: [freeCodeCamp.org](https://www-freecodecamp-org.cdn.ampproject.org/v/s/www.freecodecamp.org/news/build-12-data-science-apps-with-python-and-streamlit/amp/?amp_gsa=1&amp_js_v=a6&usqp=mq331AQFKAGwASA%3D)
	  author:: Beau Carnes
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Jan 7th, 2021]]
		- ### Content
			- [![freeCodeCamp.org](https://proxy-prod.omnivore-image-cache.app/0x0,sT4i4oHOfeu4CDWat6FXNsHAnYvTbWDrvTJcIUzj4r3Y/https://cdn.freecodecamp.org/platform/universal/fcc_primary.svg)](https://www.freecodecamp.org/news) 
			  
			  [ Learn to code — free 3,000-hour curriculum ](https://www.freecodecamp.org/)
			  
			  / [ \#Data Science](https://www-freecodecamp-org.cdn.ampproject.org/news/tag/data-science/) 
			  
			  ![Build 12 Data Science Apps with Python and Streamlit](https://proxy-prod.omnivore-image-cache.app/2133x1199,s_ZtyY_NFZf3jS_peiklJUhghGWKBmtTia1_F33I8smA/https://www.freecodecamp.org/news/content/images/size/w2000/2021/01/Streamlit_freeCodeCamp.png) 
			  
			  If you want to get better at Data Science, you need to practice. 
			  
			  We've released a course on the freeCodeCamp.org YouTube channel that will help you improve your data science skills by teaching you how to build 12 different interactive, data-driven web apps.
			  
			  Chanin Nantasenamat teaches this course. Besides being known as the Data Professor on his YouTube channel, Chanin is an Associate Professor of Bioinfomatics where he teaches and does research at the intersection of machine learning and computational drug discovery.
			  
			  You will learn how to create the data science apps with Streamlit. Streamlit is an open-source Python library for machine learning and data science. Streamlit makes it easy to create and share custom data science web apps.
			  
			  Here are the 12 different apps you will create in this course:
			  
			  * Simple Stock Price
			  * Simple Bioinformatics DNA Count
			  * EDA Basketball
			  * EDA Football
			  * EDA SP500 Stock Price
			  * EDA Cryptocurrency
			  * Classification Iris
			  * Classification Penguins
			  * Regression Boston Housing
			  * Regression Bioinformatics Solubility
			  * Deploy to Heroku
			  * Deploy to Streamlit Sharing
			  
			  Watch the [full course on the freeCodeCamp.org YouTube channel](https://youtu.be/JwSS70SZdyM) (3-hour watch).
			  
			  ---
			  
			  ![Beau Carnes](https://proxy-prod.omnivore-image-cache.app/250x250,s5BQWRXrSGwgOi8x0L8T3XktBimbS1Xvp38Xo2s6cCLc/https://www.freecodecamp.org/news/content/images/size/w60/2021/05/beau-carnes-gravatar.jpeg) 
			  
			  [ Beau Carnes](https://www-freecodecamp-org.cdn.ampproject.org/news/author/beau/) 
			  
			  I'm a teacher and developer with freeCodeCamp.org. I run the freeCodeCamp.org YouTube channel.
			  
			  ---
			  
			  If you read this far, thank the author to show them you care. 
			  
			  Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. [Get started](https://www.freecodecamp.org/learn/)
	- Show progress in your Python apps with tqdm | Opensource.com](https://omnivore.app/me/show-progress-in-your-python-apps-with-tqdm-opensource-com-18bc48beb53)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/20/12/tqdm-python)
	  author:: Moshe Zadka
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Dec 28th, 2020]]
		- ### Content
			- If your program takes a while to show results, avoid frustrating users by showing the progress it's making.
			  
			  ![arrows cycle symbol for failing faster](https://proxy-prod.omnivore-image-cache.app/520x292,sjcLGg4GHK92uB8Eqgbo83MyMUwOB3lFdT-UwmZhrPvQ/https://opensource.com/sites/default/files/lead-images/fail_progress_cycle_momentum_arrow.png "arrows cycle symbol for failing faster") 
			  
			  The Semitic root _q-d-m_ in Aramaic, Hebrew, and Arabic is usually associated with moving forward or making progress. The Arabic word _taqaddum_ (تقدّم) means "progress." Progress is important. As every feel-good movie will tell you, the journey is as important as the destination.
			  
			  Most programs have a clear goal, a desired end state. Sometimes, calculating that end state can take a long time. While computers don't care, not having feelings, people do. Humans are not happy sitting around waiting without any visible sign of progress. Doubt creeps in. Has the program crashed? Is the disk thrashing? Did the operating system allocate all its computing resources to other tasks?
			  
			  Like justice, progress must be seen, not merely done. The [tqdm](https://pypi.org/project/tqdm/) Python library helps make progress explicit.
			  
			  The tqdm module works with the console, but it also has special support for one of my favorite environments: Jupyter. To use tqdm in Jupyter, you need to import the `notebook` submodule and have [ipywidgets](https://opensource.com/article/20/11/daily-journal-jupyter) installed. The `notebook` submodule is interface-compatible with tqdm.
			  
			  This means you can do some import-time shenanigans to import the correct module while keeping tqdm usage the same. The trick is to check if the `__main__` module has the global variable `get_ipython`. While this is a heuristic, it is a reasonably accurate one:
			  
			  ```xl
			  import sys
			  if hasattr(sys.modules["__main__"], "get_ipython"):
			    from tqdm import notebook as tqdm
			  else:
			    import tqdm
			  ```
			  
			  The simplest case is when something needs to run for a certain number of iterations (known in advance), and each of those iterations takes about the same amount of time. For example, there is an algorithm to calculate the square root of any number by starting with 1 as a guess and then calculating an improved guess:
			  
			  ```ruby
			  def improve_guess(rt, n):
			    return (rt + n/rt) / 2
			  ```
			  
			  A small number of improvements gets you pretty close. For example, you can calculate the square root of two:
			  
			  ```makefile
			  guess = 1
			  target = 2
			  for i in tqdm.trange(10):
			    guess = improve_guess(guess, target)
			  ```
			  
			  ![tqdm output](https://proxy-prod.omnivore-image-cache.app/675x45,s-jO9TbnC6AW5wZoGmfBhiOyRaCJhI7mDUlHPOGi3Jy8/https://opensource.com/sites/default/files/uploads/output_8_0.png "tqdm output")
			  
			  It's correct to 10 decimal places!
			  
			  ```stylus
			  round(2 - guess*guess, 10)
			  ```
			  
			  ```angelscript
			  0.0
			  ```
			  
			  A slightly more complicated example is when the number of elements is known, and processing each element takes a similar amount of time. As an example, you can calculate the product of some numbers. For that, you'll want some random numbers:
			  
			  ```angelscript
			  import random
			  numbers = [random.uniform(0, 2.8) for i in range(100)]
			  numbers[:5]
			  ```
			  
			  ```json
			  [2.6575636572230916,
			  0.1286674965830302,
			  1.0634250104041332,
			  1.1760969844376505,
			  0.45192978568125486]
			  ```
			  
			  Now that the numbers are in, it's time to multiply them. The easiest way to use tqdm is by wrapping a Python iterable. The values will be the same, but tqdm will also display a progress bar:
			  
			  ```livecodeserver
			  result = 1
			  for num in tqdm.tqdm(numbers):
			    result *= num
			  result
			  ```
			  
			  ![tqdm output](https://proxy-prod.omnivore-image-cache.app/675x49,sWfVA_iJwM8YmtKV23LQAntERMEBBaBsBy3n7lvz9YXI/https://opensource.com/sites/default/files/uploads/output_15_0.png "tqdm output")
			  
			  However, not all things are predictable. One of the least predictable things is network speed. When you download a big file, the only way to measure progress is to explicitly check how much has been downloaded:
			  
			  ```qml
			  url = "https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgz"
			  import httpx
			  with httpx.stream("GET", url) as response:
			    total = int(response.headers["Content-Length"])
			    with tqdm.tqdm(total=total) as progress:
			        for chunk in response.iter_bytes():
			            progress.update(len(chunk))
			  ```
			  
			  ![tqdm output](https://proxy-prod.omnivore-image-cache.app/675x32,sTeXPoEv9dyYlO48TuO6dUP4-HZIxOMBm3cl4Kld6m8k/https://opensource.com/sites/default/files/uploads/output_18_0.png "tqdm output")
			  
			  Sometimes, it makes sense to "nest" progress bars. For example, if you are downloading a directory, you'll want a progress bar tracking the files and a progress bar per file.
			  
			  Here is an example (but without actually downloading a directory):
			  
			  ```angelscript
			  files = [f"vid-{i}.mp4" for i in range(4)]
			  for fname in tqdm.tqdm(files, desc="files"):
			    total = random.randrange(10**9, 2 * 10**9)
			    with tqdm.tqdm(total=total, desc=fname) as progress:
			        current = 0
			        while current < total:
			            chunk_size = min(random.randrange(10**3, 10**5), total - current)
			            current += chunk_size
			            if random.uniform(0, 1) < 0.01:
			                time.sleep(0.1)
			            progress.update(chunk_size)
			  ```
			  
			  ![tqdm output](https://proxy-prod.omnivore-image-cache.app/675x102,siaiP67Kht5cKasXxjxOY_ok7rdXA4zWnJvby1cvdyiw/https://opensource.com/sites/default/files/uploads/output_21_0.png "tqdm output")
			  
			  So, if your program takes a while to show final results, avoid frustrating your users: Show the progress it's making!
			  
			  ![Moshe sitting down, head slightly to the side. His t-shirt has Guardians of the Galaxy silhoutes against a background of sound visualization bars.](https://proxy-prod.omnivore-image-cache.app/150x150,s9Qfp74cj8H4XChqFVMWHlU33PrKyfSoEaPCdCvl6-34/https://opensource.com/sites/default/files/styles/150x150/public/pictures/m48a0550-half.jpg?itok=jlWNwEOM) 
			  
			  Moshe has been involved in the Linux community since 1998, helping in Linux "installation parties". He has been programming Python since 1999, and has contributed to the core Python interpreter. Moshe has been a DevOps/SRE since before those terms existed, caring deeply about software reliability, build reproducibility and other such things.
			  
			  \#\# Related Content
			  
			  [ ![Creative Commons License](https://proxy-prod.omnivore-image-cache.app/0x0,s4aXUWHfwSQ2Woh5RX82tXHIVoMMMXKO7tdegsKArCy8/https://opensource.com/themes/osdc/assets/img/cc-by-sa-4.png "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.")](http://creativecommons.org/licenses/by-sa/4.0/)This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.
	- Fuzzy Name Matching in Postgres](https://omnivore.app/me/fuzzy-name-matching-in-postgres-18bc48bea62)
	  collapsed:: true
	  site:: [Crunchy Data](https://blog.crunchydata.com/blog/fuzzy-name-matching-in-postgresql)
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Feb 22nd, 2021]]
		- ### Content
			- A surprisingly common problem in both application development and analysis is: given an input name, find the database record it most likely refers to. It's common because databases of names and people are common, and it's a problem because names are a very irregular identifying token.
			  
			  The page "[Falsehoods Programmers Believe About Names](https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/)" covers some of the ways names are hard to deal with in programming. This post will ignore most of those complexities, and deal with the problem of matching up loose user input to a database of names.
			  
			  \#\# [Sample Data](\#sample-data)
			  
			  The data for this post are 50,000 Westernized names created by the[Fake Name Generator](https://www.fakenamegenerator.com/), then loaded into PostgreSQL using the COPY command.
			  
			  ```pgsql
			  CREATE TABLE names (
			    number integer primary key,
			    givenname text,
			    middleinitial text,
			    surname text
			    );
			  
			  \COPY names FROM 'FakeNameGenerator.csv' WITH (FORMAT csv, HEADER true);
			  
			  ```
			  
			  The names are a mix of common American given names and surnames.
			  
			  ```coq
			  SELECT * FROM names LIMIT 10;
			  
			  number | givenname | middleinitial |  surname
			  --------+-----------+---------------+------------
			      1 | Helen     | S             | Godinez
			      2 | Sabrina   | L             | Glanz
			      3 | Tiffany   | P             | Hernandez
			      4 | Willie    | C             | Williams
			      5 | Michael   | S             | Jones
			      6 | Jaime     | L             | Sanderson
			      7 | Luis      | L             | Broderick
			      8 | Debby     | R             | Thorp
			      9 | Cynthia   | N             | Figueroa
			     10 | Amanda    | K             | Schnieders
			  
			  
			  ```
			  
			  \#\# [Prefix Matching](\#prefix-matching)
			  
			  Suppose we are working with a form input field, and want to return names that match user keystrokes in real time. If a user types "Back" what query can find all the surnames that start with "Back"?
			  
			  Easy, a "LIKE" query:
			  
			  ```sql
			  SELECT * FROM names WHERE surname LIKE 'Back%';
			  
			  ```
			  
			  On the test data of 50,000 names, the query returns **8 names, in about 11ms**.
			  
			  ```coq
			  number | givenname | middleinitial | surname
			  --------+-----------+---------------+----------
			   6788 | Milton    | E             | Backer
			   7748 | Earl      | L             | Backlund
			  11787 | Estela    | J             | Backlund
			  28516 | Lillian   | J             | Backus
			  31087 | John      | F             | Backer
			  35760 | Joyce     | D             | Backman
			  43301 | Ronald    | S             | Backman
			  44967 | Lisa      | J             | Backman
			  
			  
			  ```
			  
			  So far we have no indexes, so what happens if we index the "surnames" column?
			  
			  ```sql
			  CREATE INDEX names_surname_txt ON names (surname);
			  
			  ```
			  
			  Now, when we run the "LIKE" query, the result is ... **8 names in about 11ms**. Wait, what? The index didn't make the query any faster!
			  
			  The problem is that the default[operator class](https://www.postgresql.org/docs/current/indexes-opclass.html)for the "text" data type does not support pattern matching. If you are going to do prefix matching, you need to create an index using the "text\_pattern\_ops" operator class.
			  
			  ```sql
			  CREATE INDEX names_surname_txt ON names (surname text_pattern_ops);
			  
			  ```
			  
			  Now the same prefix query returns **8 names in about 1ms**. Much better!
			  
			  \#\# [Case Insensitivity](\#case-insensitivity)
			  
			  You cannot count on users applying any particular rules to upper and lowercase characters in the input. So the only sensible thing to do is coerce all input into a single casing.
			  
			  ```sql
			  SELECT * FROM names WHERE lower(surname) LIKE lower('Back%');
			  
			  ```
			  
			  Unfortunately, now our text prefix index is no longer being used.
			  
			  PostgreSQL supports "functional" indexes, where the index is built on a functional transformation of the stored data. As long as the query uses the same transformation as the function, you can index transforms of your data without having to store them directly in your table.
			  
			  ```sql
			  CREATE INDEX names_surname_txt ON names (lower(surname) text_pattern_ops);
			  
			  ```
			  
			  Now, the query is back to returning **8 names in about 1ms** again, but is fully case-insensitive.
			  
			  \#\# [Levenshtein Matching](\#levenshtein-matching)
			  
			  So far, our imaginary user has been very good at entering data that match the database. But what if they are looking for "Robert Harrington" and type in "Robert Harington" (one "r")... is there a way to still find them the name(s) they are looking for, without sacrificing performance?
			  
			  Enter the PostgreSQL[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)extension! This extension provides some utility functions for matching similar-but-not-quite-the-same strings.
			  
			  The first function we will use calculates the[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein%5Fdistance)between two strings. The Levenshtein distance is the sum of the number of character transpositions and the number of character insertions/deletions.
			  
			  * levenshtein('mister','mister') == 0
			  * levenshtein('mister','master') == 1
			  * levenshtein('mister','maser') == 2
			  * levenshtein('mister','laser') == 3
			  
			  We can use Levenshtein distance to write a query that finds all the database entries that are within one character of the input string ("Robert Harington"). To keep things simpler we compare the "full name", a concatenation of the surname and given name.
			  
			  ```sql
			  WITH q AS (
			  SELECT 'Robert' AS qgn, 'Harington' AS qsn
			  )
			  SELECT
			  levenshtein(lower(concat(surname,givenname)),lower(concat(qsn, qgn))) AS leven,
			  names.*
			  FROM names, q
			  WHERE levenshtein(lower(concat(surname,givenname)),lower(concat(qsn, qgn))) < 2
			  ORDER BY leven;
			  
			  ```
			  
			  And we get back the two "Harrington" records!
			  
			  ```coq
			  leven | number | givenname | middleinitial |  surname
			  -------+--------+-----------+---------------+------------
			     1 |   1186 | Robert    | H             | Harrington
			     1 |  21256 | Robert    | B             | Harrington
			  
			  
			  ```
			  
			  The only trouble is, it's really slow (**over 100ms**), because it is calculating Levenshtein distances between our candidate string and **every name in the database**. This approach will not scale without some indexing help.
			  
			  \#\# [Soundex Indexing](\#soundex-indexing)
			  
			  What we want to do is use an index filter to reduce the number of candidate records to a manageable size, and then only perform the expensive Levenshtein calculation on those records.
			  
			  Fortunately the[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) has a perfect answer: [Soundex](https://en.wikipedia.org/wiki/Soundex)!
			  
			  The Soundex algorithm reduces a word to a "phonetic code", basically a short string that approximates the pronunciation of the initial syllable. This allows us to avoid all kinds of common misspelling mistakes, since the Soundex is the same as long as the pronunciation of the mistake is similar.
			  
			  * soundex('Harrington') = H652
			  * soundex('Harington') = H652
			  * soundex('Herington') = H652
			  * soundex('Heringtan') = H652
			  
			  Our full database is 50,000 records: how big is the set of records that match the soundex of the last name?
			  
			  ```sql
			  SELECT count(*)
			  FROM names
			  WHERE soundex(surname) = soundex('Harrington');
			  
			  ```
			  
			  Only **46** records match the soundex!
			  
			  \#\# [Soundex + Levenshtein](\#soundex--levenshtein)
			  
			  So we can add a soundex functional index, and re-write our Levenshtein query to make use of soundex as a prefilter, to get a levenshtein calculation at fully indexed speed.
			  
			  ```sql
			  CREATE INDEX surname_soundex ON names (soundex(surname));
			  
			  ```
			  
			  With a functional index, we can add a soundex clause and re-run the query.
			  
			  ```sql
			  WITH q AS (
			  SELECT 'Robert' AS qgn, 'Harington' AS qsn
			  )
			  SELECT
			  levenshtein(lower(concat(surname,givenname)),lower(concat(qsn, qgn))) AS leven,
			  names.*
			  FROM names, q
			  WHERE soundex(surname) = soundex(qsn)
			  AND levenshtein(lower(concat(surname,givenname)),lower(concat(qsn, qgn))) < 2
			  
			  ```
			  
			  Now we get the same answer as before, but in **just 1 ms**, one hundred times faster.
			  
			  \#\# [Conclusion](\#conclusion)
			  
			  With the right indexes, and the[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)toolkit, it's possible to build very fast loose text matching queries.
			  
			  * Remember `text_pattern_ops` for indexed prefix filtering.
			  * Use `lower()` and functional indexes for case-insensitive queries.
			  * Combine indexes on `soundex()` with expensive tests like `levenshtein()` to get fast searches for fuzzy queries.
			  
			  ![Avatar for Paul Ramsey](https://proxy-prod.omnivore-image-cache.app/0x0,sxl0-TpenINwN-OkKPC4Oy-oYBUN87vY08PU7Kyvb7bE/https://blog.crunchydata.com/build/_assets/paul-ramsey.png-POMCJCK4.webp)
	- Tips for Selecting Columns in a DataFrame - Practical Business Python](https://omnivore.app/me/tips-for-selecting-columns-in-a-data-frame-practical-business-py-18bc48be791)
	  collapsed:: true
	  site:: [pbpython.com](https://pbpython.com/selecting-columns.html)
	  author:: Chris Moffitt
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 25th, 2019]]
		- ### Content
			- ![article header image](https://proxy-prod.omnivore-image-cache.app/0x0,ssb3cHjifGUG1dtWkfPoywwWCF5if4IYFZyD1ipCB-xw/https://pbpython.com/images/squirrel.jpg)
			  
			  \#\# Introduction
			  
			  This article will discuss several tips and shortcuts for using `iloc` to work with a data set that has a large number of columns. Even if you have some experience with using `iloc` you should learn a couple of helpful tricks to speed up your own analysis and avoid typing lots of column names in your code.
			  
			  \#\# Why Do We Care About Selecting Columns?
			  
			  In many standard data science examples, there are a relatively small number of columns. For example, Titanic has 8, Iris has 4, and Boston Housing has 14\. Real-life data sets are messy and often include a lot of extra (potentially unnecessary) columns.
			  
			  In data science problems you may need to select a subset of columns for one or more of the following reasons:
			  
			  * Filtering the data to only include the relevant columns can help shrink the memory footprint and speed up data processing.
			  * Limiting the number of columns can reduce the mental overhead of keeping the data model in your head.
			  * When exploring a new data set, it might be necessary to break to task into manageable chunks.
			  * In some cases, you may need to loop through columns and perform calculations or cleanups in order to get the data in the format you need for further analysis.
			  * Your data may just contain extra or duplicate information which is not needed.
			  
			  Regardless of the reason, you may not need these techniques all the time. When you do, though, the tricks outlined below can reduce the amount of time you spend wrangling columns of data.
			  
			  Also, if you like this type of content, I encourage you to check out Kevin Markham’s [pandas tricks](https://www.dataschool.io/python-pandas-tips-and-tricks/) which served as an inspiration for a couple of the tips below.
			  
			  \#\# The Data
			  
			  In order to illustrate some examples, I’m going to use a quirky data set from the [Central Park Squirrel Census](https://www.thesquirrelcensus.com/). Yes, apparently there was an effort to count and catalog squirrels in Central Park. I thought this would be a fun example to work through. It also gave me a chance to include a squirrel image (credit: [GeorgeB2](https://pixabay.com/photos/squirrel-gray-tree-wild-wildlife-3703312/)) in my post :) .
			  
			  This data [set](https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw) includes 3,023 rows of data and 31 columns. While 31 columns is not a tremendous number of columns, it is a useful example to illustrate the concepts you might apply to data with many more columns.
			  
			  If you want to follow along, you can view the [notebook](https://nbviewer.jupyter.org/github/chris1610/pbpython/blob/master/notebooks/Selecting%5FColumns%5Fin%5FDataFrame.ipynb) or pull it directly from [github](https://github.com/chris1610/pbpython/blob/master/notebooks/Selecting%5FColumns%5Fin%5FDataFrame.ipynb).
			  
			  Let’s get started by reading in the data.
			  
			  import pandas as pd
			  import numpy as np
			  
			  df = pd.read_csv(
			    'https://data.cityofnewyork.us/api/views/vfnx-vebw/rows.csv?accessType=DOWNLOAD&bom=true&format=true'
			  )
			  
			  Sometimes it gets tricky to remember each column name and where it is by index. Here is a simple list comprehension to build up a reference list of all columns and their index.
			  
			  col_mapping = [f"{c[0]}:{c[1]}" for c in enumerate(df.columns)]
			  
			  Which creates a list like this:
			  
			  ['0:X',
			  '1:Y',
			  '2:Unique Squirrel ID',
			  '3:Hectare',
			  '4:Shift',
			  '5:Date',
			  ...
			  '33:Borough Boundaries',
			  '34:City Council Districts',
			  '35:Police Precincts']
			  
			  In some cases if you might want to rename a bunch of columns, you can use a dictionary comprehension to create a dictionary view of the data:
			  
			  col_mapping_dict = {c[0]:c[1] for c in enumerate(df.columns)}
			  
			  Which creates this dictionary:
			  
			  {0: 'X',
			  1: 'Y',
			  2: 'Unique Squirrel ID',
			  3: 'Hectare',
			  4: 'Shift',
			  5: 'Date',
			  ...
			  33: 'Borough Boundaries',
			  34: 'City Council Districts',
			  35: 'Police Precincts'}
			  
			  Having these variables defined can be useful as you progress through your analysis. Instead of repeatedly looking at your original file, you can just double check the variable name during your analysis.
			  
			  One other common task I frequently have is to rename a bunch of columns that are inconsistently named across files. I use a dictionary to easily rename all the columns using something like `df.rename(columns=col_mapping)`Typing all the column names can be an error prone task. A simple trick is to copy all the columns in excel and use `pd.read_clipboard()` to build a small DataFrame and turn the columns into a dictionary. I can then manually type in the new names, if need be.
			  
			  Here is a quick example with this data set. Note that we pass in the `sep` to parse a tab delimited string:
			  
			  df_cols = pd.read_clipboard(sep='\t')
			  col_mapping = {c[1]:'' for c in enumerate(df_cols.columns)}
			  
			  Which creates a dictionary that is relatively easy to populate with new names:
			  
			  {'X': '',
			  'Y': '',
			  'Unique': '',
			  'Squirrel': '',
			  'ID': '',
			  'Hectare': '',
			  'Shift': '',
			  ...
			  'Police': '',
			  'Precincts': ''}
			  
			  As an added bonus, you could even use an Excel file to set up the column renaming and automate the whole process. That tip is outside the scope of this article. If you are interested though, let me know in the comments.
			  
			  \#\# Using iloc
			  
			  The primary [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html) we will walk through is panda’s `iloc` which is used for integer-location based indexing. New users may be slightly confused because `iloc` and `loc` can take a boolean-array which leads to more powerful indexing. Since both functions can take a boolean array as input, there are times when these functions produce the same output. However, for the scope of this post, I will focus only on `iloc` column selection.
			  
			  Here’s a simple graphic to illustrate the primary usage for `iloc`:
			  
			  ![iloc illustrated](https://proxy-prod.omnivore-image-cache.app/0x0,sJi48CIsTrDz8qKWrnZgLkPqxdGALY__00mhZ1B1n3Z0/https://pbpython.com/images/iloc.png) 
			  
			  For instance, if you want to look at just the Squirrel ID column of data for all rows:
			  
			  0       37F-PM-1014-03
			  1       37E-PM-1006-03
			  2        2E-AM-1010-03
			  3        5D-PM-1018-05
			  4       39B-AM-1018-01
			             ...
			  3018    30B-AM-1007-04
			  3019    19A-PM-1013-05
			  3020    22D-PM-1012-07
			  3021    29B-PM-1010-02
			  3022     5E-PM-1012-01
			  Name: Unique Squirrel ID, Length: 3023, dtype: object
			  
			  If you want to look at the X and Y location as well as the ID, you can pass in a list of integers `[0,1,2]`:
			  
			  | X    | Y           | Unique Squirrel ID |                 |
			  | ---- | ----------- | ------------------ | --------------- |
			  | 0    | \-73.956134 | 40.794082          | 37F-PM\-1014-03 |
			  | 1    | \-73.957044 | 40.794851          | 37E-PM\-1006-03 |
			  | 2    | \-73.976831 | 40.766718          | 2E-AM\-1010-03  |
			  | 3    | \-73.975725 | 40.769703          | 5D-PM\-1018-05  |
			  | 4    | \-73.959313 | 40.797533          | 39B-AM\-1018-01 |
			  | …    | …           | …                  | …               |
			  | 3018 | \-73.963943 | 40.790868          | 30B-AM\-1007-04 |
			  | 3019 | \-73.970402 | 40.782560          | 19A-PM\-1013-05 |
			  | 3020 | \-73.966587 | 40.783678          | 22D-PM\-1012-07 |
			  | 3021 | \-73.963994 | 40.789915          | 29B-PM\-1010-02 |
			  | 3022 | \-73.975479 | 40.769640          | 5E-PM\-1012-01  |
			  
			  3023 rows × 3 columns
			  
			  Typing all the columns is not the most efficient, so we can use slicing notation to make this a little easier to understand:
			  
			  Which will generate the same output as above.
			  
			  If you have some experience with python lists, and have used pandas a bit; all of this usage should make sense. These points are pandas 101 concepts but we will build up from here.
			  
			  While both of these approaches are straightforward, what if you want to combine the list of integers with the slice notation? You might try something like this:
			  
			  File "<ipython-input-56-6f5b3426f412>", line 1
			    df.iloc[:, [0:3,15:19]]
			                 ^
			  SyntaxError: invalid syntax
			  
			  Or, you could try something like this:
			  
			  IndexingError: Too many indexers
			  
			  Hmmm. That obviously doesn’t work but seems like it would be useful for selecting ranges as well as individual columns.
			  
			  Fortunately there is a [numpy object](https://docs.scipy.org/doc/numpy/reference/generated/numpy.r%5F.html) that can help us out. The `r_` object will “Translate slice objects to concatenation along the first axis.” It might not make much sense from the documentation but it does exactly what we need.
			  
			  Here’s a slightly more elaborate example to show how it works on a combination of individual list items and sliced ranges:
			  
			  array([ 0,  1,  2, 15, 16, 17, 18, 24, 25])
			  
			  That’s kind of cool. This object has converted the combination of integer lists and slice notation into a single list which we can pass to `iloc`:
			  
			  df.iloc[:, np.r_[0:3,15:19,24,25]]
			  
			  | Y    | Unique Squirrel ID | Date            | Hectare Squirrel Number | Age      | Running | Chasing | Climbing | Eating | Foraging | Other Activities | Kuks  | Quaas | Moans | Tail flags | Tail twitches | Approaches | Indifferent | Runs from | Other Interactions |   |
			  | ---- | ------------------ | --------------- | ----------------------- | -------- | ------- | ------- | -------- | ------ | -------- | ---------------- | ----- | ----- | ----- | ---------- | ------------- | ---------- | ----------- | --------- | ------------------ | - |
			  | 0    | 40.794082          | 37F-PM\-1014-03 | 3                       | NaN      | False   | False   | False    | False  | False    | NaN              | False | False | False | False      | False         | False      | False       | False     | NaN                |   |
			  | 1    | 40.794851          | 37E-PM\-1006-03 | 3                       | Adult    | True    | False   | False    | False  | False    | NaN              | False | False | False | False      | False         | False      | False       | True      | me                 |   |
			  | 2    | 40.766718          | 2E-AM\-1010-03  | 3                       | Adult    | False   | False   | True     | False  | False    | NaN              | False | False | False | False      | False         | False      | True        | False     | NaN                |   |
			  | 3    | 40.769703          | 5D-PM\-1018-05  | 5                       | Juvenile | False   | False   | True     | False  | False    | NaN              | False | False | False | False      | False         | False      | False       | True      | NaN                |   |
			  | …    | …                  | …               | …                       | …        | …       | …       | …        | …      | …        | …                | …     | …     | …     | …          | …             | …          | …           | …         | …                  | … |
			  | 3019 | 40.782560          | 19A-PM\-1013-05 | 5                       | Adult    | False   | False   | False    | False  | True     | NaN              | False | False | False | False      | False         | False      | True        | False     | NaN                |   |
			  | 3020 | 40.783678          | 22D-PM\-1012-07 | 7                       | Adult    | False   | False   | False    | True   | True     | NaN              | False | False | False | False      | False         | False      | True        | False     | NaN                |   |
			  
			  3023 rows × 20 columns
			  
			  Here is another tip. You may also use this notation when reading data using `read_csv`:
			  
			  df_2 = pd.read_csv(
			    'https://data.cityofnewyork.us/api/views/vfnx-vebw/rows.csv?accessType=DOWNLOAD&bom=true&format=true',
			    usecols=np.r_[1,2,5:8,15:30],
			  )
			  
			  I find this notation helpful when you have a data set where you want to keep non-sequential columns and do not want to type out the full names of all the columns.
			  
			  One caveat I would make is that you need to be careful when using the slice notation and keep in mind that the last number in the range will not be included in the generated list of numbers.
			  
			  For example, if we specify the range `2:4`, we only get a list of 2 and 3:
			  
			  If you want to include column index 4, use `np.r_[2:5]`.
			  
			  One final comment on `np.r_` is that there is an optional step argument. In this example, we can specify that this list will increment by 2:
			  
			  This is a bit of a more advanced option and is not going to be intuitively obvious to a new pandas user. However if you ever find yourself parsing a lot of columns by index, this might be a useful tool to navigate a tricky scenario.
			  
			  \#\# iloc and boolean arrays
			  
			  One of the most powerful ways to filter columns is to pass a boolean array to `iloc` to select a subset of columns. This sounds a little complex but a couple of examples should make this understandable.
			  
			  The most important concept is that we don’t generate a boolean array by hand but use the output from another pandas function to generate the array and feed it to `iloc`.
			  
			  In this case, we can use the `str` accessor on a column index just like any other column of pandas data. This will generate the necessary boolean array that `iloc` expects. An example should help make this clear.
			  
			  If we want to see which columns contain the word “run”:
			  
			  run_cols = df.columns.str.contains('run', case=False)
			  print(run_cols)
			  
			  array([False, False, False, False, False, False, False, False, False,
			    False, False, False, False, False, False,  True, False, False,
			    False, False, False, False, False, False, False, False, False,
			    False,  True, False, False, False, False, False, False, False])
			  
			  Then we can pass this new array of boolean values to select only two columns:
			  
			  df.iloc[:, run_cols].head()
			  
			  | Running | Runs from |       |
			  | ------- | --------- | ----- |
			  | 0       | False     | False |
			  | 1       | True      | True  |
			  | 2       | False     | False |
			  | 3       | False     | True  |
			  | 4       | False     | False |
			  
			  In practice, many people will use a `lambda` function to do this in one line:
			  
			  df.iloc[:, lambda df:df.columns.str.contains('run', case=False)]
			  
			  The benefits of using `str` functions are that you can get sophisticated with the potential filter options. For instance, if we want all the columns with “district,” “precinct” or “boundaries” in the name:
			  
			  df.iloc[:, lambda df: df.columns.str.contains('district|precinct|boundaries',
			                                              case=False)].head()
			  
			  | Community Districts | Borough Boundaries | City Council Districts | Police Precincts |    |
			  | ------------------- | ------------------ | ---------------------- | ---------------- | -- |
			  | 0                   | 19                 | 4                      | 19               | 13 |
			  | 1                   | 19                 | 4                      | 19               | 13 |
			  | 2                   | 19                 | 4                      | 19               | 13 |
			  | 3                   | 19                 | 4                      | 19               | 13 |
			  | 4                   | 19                 | 4                      | 19               | 13 |
			  
			  We can even combine all these concepts together by using the results of the boolean array to get the index then use `np.r_` to combine these lists together.
			  
			  The example below can be simplified by using `filter`. Please review the next section for an explanation of `filter` and how to use it with a regular expression.
			  
			  Here is an example where we want to get all the location related columns as well as the squirrel ID into a DataFrame:
			  
			  location_cols = df.columns.str.contains('district|precinct|boundaries',
			                                        case=False)
			  location_indices = [i for i, col in enumerate(location_cols) if col]
			  df.iloc[:, np.r_[0:3,location_indices]].head()
			  
			  | X | Y           | Unique Squirrel ID | Community Districts | Borough Boundaries | City Council Districts | Police Precincts |    |
			  | - | ----------- | ------------------ | ------------------- | ------------------ | ---------------------- | ---------------- | -- |
			  | 0 | \-73.956134 | 40.794082          | 37F-PM\-1014-03     | 19                 | 4                      | 19               | 13 |
			  | 1 | \-73.957044 | 40.794851          | 37E-PM\-1006-03     | 19                 | 4                      | 19               | 13 |
			  | 2 | \-73.976831 | 40.766718          | 2E-AM\-1010-03      | 19                 | 4                      | 19               | 13 |
			  | 3 | \-73.975725 | 40.769703          | 5D-PM\-1018-05      | 19                 | 4                      | 19               | 13 |
			  | 4 | \-73.959313 | 40.797533          | 39B-AM\-1018-01     | 19                 | 4                      | 19               | 13 |
			  
			  This code is a little complicated since we are using a conditional list comprehension and might be overkill for selecting 7 columns. The important concept is that you know it is possible and can refer back to this article when you need it for your own analysis.
			  
			  \#\# Filter
			  
			  In the original article, I did not include any information about using pandas DataFrame [filter](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html)to select columns. I think this mainly because `filter` sounds like it should be used to filter data not column names. Fortunately you _can_ use pandas `filter` to select columns and it is very useful.
			  
			  If you want to select the columns that have “Districts” in the name, you can use `like`:
			  
			  df.filter(like='Districts')
			  
			  | Community Districts | City Council Districts |    |
			  | ------------------- | ---------------------- | -- |
			  | 0                   | 19                     | 19 |
			  | 1                   | 19                     | 19 |
			  | 2                   | 19                     | 19 |
			  | 3                   | 19                     | 19 |
			  | 4                   | 19                     | 19 |
			  | …                   | …                      | …  |
			  | 3018                | 19                     | 19 |
			  | 3019                | 19                     | 19 |
			  | 3020                | 19                     | 19 |
			  | 3021                | 19                     | 19 |
			  | 3022                | 19                     | 19 |
			  
			  You can also use a regex so it is easy to look for columns that contain one or more patterns:
			  
			  df.filter(regex='ing|Date')
			  
			  | Date | Running | Chasing | Climbing | Eating | Foraging |   |
			  | ---- | ------- | ------- | -------- | ------ | -------- | - |
			  | …    | …       | …       | …        | …      | …        | … |
			  
			  The more complex `lambda` example shown above could be much more succinctly created using `filter`:
			  
			  df.filter(regex='District|Precinct|Boundaries')
			  
			  \#\# Caveats
			  
			  One item to keep in mind when dealing with numerical indexing of columns is that you need to understand where your data comes from. If you expect you ID column to always be in a specific location and it changes order in the data, you could face problems with your subsequent data processing. This situation is where your domain knowledge and expertise comes into play to make sure the solution is robust enough for the given situation.
			  
			  \#\# Summary
			  
			  Most of my data analysis involves filtering and selecting data at the row level. However there are times when it is helpful to work with data in a column-wise fashion. Pandas` iloc` and `filter` can be a useful tool for quickly and efficiently working with data sets that have many columns of data. I hope this article provided a couple of tips that will help you with your own analysis.
			  
			  \#\# Changes
			  
			  * 1-Dec-2019: Updated typos and clarified read\_clipboard usage to use tab delimiter.
			  * 24-July-2020: Include using `filter` to select columns.
	- Are You Still Using Pandas to Process Big Data in 2021? Here are two better options - KDnuggets](https://omnivore.app/me/are-you-still-using-pandas-to-process-big-data-in-2021-here-are--18bc48bdfe2)
	  collapsed:: true
	  site:: [KDnuggets](https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html)
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- When its time to handle a lot of data -- so much that you are in the realm of Big Data -- what tools can you use to wrangle the data, especially in a notebook environment? Pandas doesn’t handle really Big Data very well, but two other libraries do. So, which one is better and faster?
			  
			  ---
			  
			  
			  **By [Roman Orac](https://www.linkedin.com/in/romanorac/), Data Scientist**.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,slgCO9BWUmWNfx1RWaf8AJeEsU-G_kmLOzxXKWMzcuxE/https://www.kdnuggets.com/wp-content/uploads/nasa-image-for-big-data-theme.jpg)
			  
			  _Photo by [NASA](https://unsplash.com/@nasa?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  I recently wrote two introductory articles about processing Big Data with [Dask](https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a) and [Vaex](https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447) — libraries for processing bigger than memory datasets. While writing, a question popped up in my mind:
			  
			  _**Can these libraries really process bigger than memory datasets, or is it all just a sales slogan?**_
			  
			  This intrigued meto do a practical experiment with Dask and Vaex and try to process a bigger than memory dataset. The dataset was so big that you cannot even open it with pandas.
			  
			  \#\#\# What do I mean by Big Data?
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,soi-3JTZ0eKwyiXAHo-ri26XZWsHl53j7aaiaDCg5X4k/https://miro.medium.com/max/875/0*Ao9I4CsTxA9Zd-uZ)
			  
			  _Photo by [ev](https://unsplash.com/@ev?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  Big Data is a loosely defined term, which has as many definitions as there are hits on Google. In this article, I use the term to describe a dataset that is so big that we need specialized software to process it. With Big, I am referring to “bigger than the main memory on a single machine.”
			  
			  _Definition from Wikipedia:_
			  
			  > _Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software._
			  
			  \#\#\# What are Dask and Vaex?
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s9BnYBfOqhBMRnaHtGqRrjG_d-LL4mEvK8CNuFjeSw9Y/https://miro.medium.com/max/875/0*rlzJyQLjigA-3HTJ)
			  
			  _Photo by [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  **Dask** provides advanced parallelism for analytics, enabling performance at scale for the tools you love. This includes numpy, pandas, and sklearn. It is open-source and freely available. It uses existing Python APIs and data structures to make it easy to switch between Dask-powered equivalents.
			  
			  **Vaex** is a high-performance Python library for lazy Out-of-Core DataFrames (similar to Pandas) to visualize and explore big tabular datasets. It can calculate basic statistics for more than a billion rows per second. It supports multiple visualizations allowing interactive exploration of big data.
			  
			  Dask and Vaex Dataframes are not fully compatible with Pandas Dataframes, but some most common “data wrangling” operations are supported by both tools. Dask is more focused on scaling the code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine.
			  
			  \#\#\# The Experiment
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sKVH6jTZlGl2h_WnXs5LjcUebVW6kwqHxxAPKNdND1v8/https://miro.medium.com/max/875/0*XsiFMRdMK5OTtmpx)
			  
			  _Photo by [Louis Reed](https://unsplash.com/@%5Flouisreed?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  I generated two CSV files with 1 million rows and 1000 columns. The size of a file was 18.18 GB, which is 36.36 GB combined. Files have random numbers from a Uniform distribution between 0 and 100.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sh5vgZ9z5ziSQJUgpU6UMhUwg382CgZXK83d8Qg2bwDw/https://miro.medium.com/max/875/1*AlkOauFhiLK1PyaxUFRK7Q.png)
			  
			  _Two CSV files with random data. Photo made by the author._
			  
			  import pandas as pd
			  import numpy as np
			  from os import path
			  n_rows = 1_000_000
			  n_cols = 1000
			  for i in range(1, 3):
			    filename = 'analysis_%d.csv' % i
			    file_path = path.join('csv_files', filename)
			    df = pd.DataFrame(np.random.uniform(0, 100, size=(n_rows, n_cols)), columns=['col%d' % i for i in range(n_cols)])
			    print('Saving', file_path)
			    df.to_csv(file_path, index=False)
			  df.head()
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sth6SfDyM9uKxVnnfZql7fWPPGfj6mafIWyUoYAYaqzo/https://miro.medium.com/max/875/1*in_UallF5DDsxiBHuCIMWw.png)
			  
			  _Head of a file. Photo made by the author._
			  
			  The experiment was run on a MacBook Pro with 32 GB of main memory — quite a beast. When testing the limits of a pandas Dataframe, I surprisingly found that reaching a Memory Error on such a machine is quite a challenge!
			  
			  macOS starts dumping data from the main memory to SSD when the memory is running near its capacity. The upper limit for pandas Dataframe was 100 GB of free disk space on the machine.
			  
			  > When your Mac needs memory, it will push something that isn’t currently being used into a swapfile for temporary storage. When it needs access again, it will read the data from the swap file and back into memory.
			  
			  I’ve spent some time thinking about how I should address this issue so that the experiment would be fair. The first idea that came to my mind was to disable swapping so that each library would have only the main memory available — good luck with that on macOS. After spending a few hours, I wasn’t able to disable swapping.
			  
			  The second idea was to use a brute force approach. I’ve filled the SSD to its full capacity so that the operating system couldn’t use swap as there was no free space left on the device.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sA1JH2Hnesjf4xPJk1l-TrwjExglhC4RB-rkj1c05mz0/https://miro.medium.com/max/875/1*GzQv3Hi5NqJPGRlLmDkLDQ.png)
			  
			  _Your disk is almost full notification during the experiment. Photo made by the author._
			  
			  This worked! pandas couldn’t read two 18 GB files, and Jupyter Kernel crashed.
			  
			  If I performed this experiment again, I would create a virtual machine with less memory. That way, it would be easier to show the limits of these tools.
			  
			  Can Dask or Vaex help us and process these large files? Which one is faster? Let’s find out.
			  
			  \#\#\# Vaex vs. Dask
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sLonRc0bM2CGPJmQvImzZ6XDEluKQ3a945bYbJVy_YYs/https://miro.medium.com/max/875/0*asf1BDHdxjbq41-3)
			  
			  _Photo by [Frida Bredesen](https://unsplash.com/@fridooh?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://www.kdnuggets.com/2021/02/data-science-learning-roadmap-2021.html)._
			  
			  When designing the experiment, I thought about basic operations when performing Data Analysis, like grouping, filtering, and visualizing data. I came up with the following operations:
			  
			  * calculating 10th quantile of a column,
			  * adding a new column,
			  * filtering by column,
			  * grouping by column and aggregating,
			  * visualizing a column.
			  
			  All of the above operations perform a calculation using a single column, e.g.:
			  
			  \# filtering with a single column
			  df[df.col2 > 10]
			  
			  So I was intrigued to try an operation, which requires all data to be processed:
			  
			  * calculate the sum of all of the columns.
			  
			  This can be achieved by breaking down the calculation into smaller chunks. E.g., reading each column separately and calculating the sum, and in the last step calculating the overall sum. These types of computational problems are known as [Embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly%5Fparallel) — no effort is required to separate the problem into separate tasks.
			  
			  \#\#\# Vaex
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sHY3UYmkKt0TJnFM8TcJ9LTDFytvniqcBcfh_GbKtm68/https://miro.medium.com/max/875/0*JiVrNI8tHm1yEyNA)
			  
			  _Photo by [Photos by Lanty](https://unsplash.com/@photos%5Fby%5Flanty?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  Let’s start with Vaex. The experiment was designed in a way that follows best practices for each tool — this is using binary format HDF5 for Vaex. So we need to convert CSV files to HDF5 format (The Hierarchical Data Format version 5).
			  
			  import glob
			  import vaex
			  csv_files = glob.glob('csv_files/*.csv')
			  for i, csv_file in enumerate(csv_files, 1):
			    for j, dv in enumerate(vaex.from_csv(csv_file, chunk_size=5_000_000), 1):
			        print('Exporting %d %s to hdf5 part %d' % (i, csv_file, j))
			        dv.export_hdf5(f'hdf5_files/analysis_{i:02}_{j:02}.hdf5')
			  
			  Vaex needed 405 seconds to covert two CSV files (36.36 GB) to two HDF5 files, which have 16 GB combined. Conversion from text to binary format reduced the file size.
			  
			  **Open HDF5 dataset with Vaex:**
			  
			  dv = vaex.open('hdf5_files/*.hdf5')
			  
			  Vaex needed 1218 seconds to read the HDF5 files. I expected it to be faster as Vaex claims near-instant opening of files in binary format.
			  
			  _[From Vaex documentation](https://vaex.readthedocs.io/en/latest/example%5Fio.html\#Binary-file-formats):_
			  
			  > _Opening such data is instantenous regardless of the file size on disk: Vaex will just memory-map the data instead of reading it in memory. This is the optimal way of working with large datasets that are larger than available RAM._
			  
			  **Display head with Vaex:**
			  
			  Vaex needed 1189 seconds to display head. I am not sure why displaying the first 5 rows of each column took so long.
			  
			  **Calculate 10th quantile with Vaex:**
			  
			  Note, Vaex has percentile\_approx function, which calculates an approximation of quantile.
			  
			  quantile = dv.percentile_approx('col1', 10)
			  
			  Vaex needed 0 seconds to calculate the approximation of the 10th quantile for the col1 column.
			  
			  **Add a new column with Vaex:**
			  
			  dv[‘col1_binary’] = dv.col1 > dv.percentile_approx(‘col1’, 10)
			  
			  Vaex has a concept of virtual columns, which stores an expression as a column. It does not take up any memory and is computed on the fly when needed. A virtual column is treated just like a normal column. As expected, Vaex needed 0 seconds to execute the command above.
			  
			  **Filter data with Vaex:**
			  
			  Vaex has a concept of [selections](https://vaex.readthedocs.io/en/latest/tutorial.html\#Selections-and-filtering), which I didn’t use as Dask doesn’t support selections, which would make the experiment unfair. The filter below is similar to filtering with pandas, except that Vaex does not copy the data.
			  
			  Vaex needed 0 seconds to execute the filter above.
			  
			  **Grouping and aggregating data with Vaex:**
			  
			  The command below is slightly different from pandas as it combines grouping and aggregation. The command groups the data by col1\_binary and calculate the mean for col3:
			  
			  group_res = dv.groupby(by=dv.col1_binary, agg={'col3_mean': vaex.agg.mean('col3')})
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sh07QhfzmqWIXBsmnZJCQLvCt1_h38LXjjGLm7e287a8/https://miro.medium.com/max/875/1*ZQVYuxSE3ZvLf7Ex-1uknA.png)
			  
			  _Calculating mean with Vaex. Photo made by the author._
			  
			  Vaex needed 0 seconds to execute the command above.
			  
			  **Visualize the histogram:**
			  
			  Visualization with bigger datasets is problematic as traditional tools for data analysis are not optimized to handle them. Let’s try if we can make a histogram of col3 with Vaex.
			  
			  plot = dv.plot1d(dv.col3, what='count(*)', limits=[0, 100])
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sijL1sB6T2CAKUwZeArUonT7w7v9fW7YB9wbk2WAhcfc/https://miro.medium.com/max/875/1*FPcoGub4_AH_1Oe_86iaLg.png)
			  
			  _Visualizing data with Vaex. Photo made by the author._
			  
			  Vaex needed 0 seconds to display the plot, which was surprisingly fast.
			  
			  **Calculate the sum of all columns**
			  
			  Memory is not an issue when processing a single column at a time. Let’s try to calculate the sum of all the numbers in the dataset with Vaex.
			  
			  suma = np.sum(dv.sum(dv.column_names))
			  
			  Vaex needed 40 seconds to calculate the sum of all columns.
			  
			  \#\#\# Dask
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sQeGdieUDliLd5aG1scMJXe4bsx6wFRa3Vxn8A2z6OYI/https://miro.medium.com/max/875/0*CjC-ygYYj2tzyrry)
			  
			  _Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  Now, let’s repeat the operations above but with Dask. The Jupyter Kernel was restarted before running Dask commands.
			  
			  Instead of reading CSV files directly with Dask’s read\_csv function, we convert the CSV files to HDF5 to make the experiment fair.
			  
			  import dask.dataframe as dd
			  ds = dd.read_csv('csv_files/*.csv')
			  ds.to_hdf('hdf5_files_dask/analysis_01_01.hdf5', key='table')
			  
			  Dask needed 763 seconds for conversion. Let me know in the comments if there is a faster way to convert the data with Dask. I tried to read the HDF5 files that were converted with Vaex with no luck.
			  
			  _[Best practices with Dask](https://docs.dask.org/en/latest/dataframe-best-practices.html\#store-data-in-apache-parquet-format):_
			  
			  > _HDF5 is a popular choice for Pandas users with high performance needs. We encourage Dask DataFrame users to store and load data using Parquet instead._
			  
			  **Open HDF5 dataset with Dask:**
			  
			  import dask.dataframe as dd
			  
			  ds = dd.read_csv('csv_files/*.csv')
			  
			  Dask needed 0 seconds to open the HDF5 file. This is because I didn’t explicitly run the compute command, which would actually read the file.
			  
			  **Display head with Dask:**
			  
			  Dask needed 9 seconds to output the first 5 rows of the file.
			  
			  **Calculate the 10th quantile with Dask:**
			  
			  Dask has a quantile function, which calculates actual quantile, not an approximation.
			  
			  quantile = ds.col1.quantile(0.1).compute()
			  
			  Dask wasn’t able to calculate quantile as Juptyter Kernel crashed.
			  
			  **Define a new column with Dask:**
			  
			  The function below uses the quantile function to define a new binary column. Dask wasn’t able to calculate it because it uses quantile.
			  
			  ds['col1_binary'] = ds.col1 > ds.col1.quantile(0.1)
			  
			  **Filter data with Dask:**
			  
			  The command above needed 0 seconds to execute as Dask uses the delayed execution paradigm.
			  
			  **Grouping and aggregating data with Dask:**
			  
			  group_res = ds.groupby('col1_binary').col3.mean().compute()
			  
			  Dask wasn’t able to group and aggregate the data.
			  
			  **Visualize the histogram of col3:**
			  
			  plot = ds.col3.compute().plot.hist(bins=64, ylim=(13900, 14400))
			  
			  Dask wasn’t able to visualize the data.
			  
			  **Calculate the sum of all columns:**
			  
			  suma = ds.sum().sum().compute()
			  
			  Dask wasn’t able to sum all the data.
			  
			  \#\#\# Results
			  
			  The table below shows the execution times of the Vaex vs. Dask experiment. NA means that the tool couldn’t process the data, and Jupyter Kernel crashed.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,slmi1SowmGCYNWcJZI3yu81pRha6hPYEvmcVNnDDn4TI/https://miro.medium.com/max/875/1*1lha93QSqHt9TunVECjRKA.png)
			  
			  _Summary of execution times in the experiment. Photo made by the author._
			  
			  \#\#\# Conclusion
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sMwAfUvWoGZZhQrSPPcYNmBkCJBbWFLhL8raV3Wivuo8/https://miro.medium.com/max/875/0*jB0lzD1N3LCLmLgY)
			  
			  _Photo by [Joshua Golde](https://unsplash.com/@joshgmit?utm%5Fsource=medium&utm%5Fmedium=referral) on [Unsplash](https://unsplash.com/?utm%5Fsource=medium&utm%5Fmedium=referral)._
			  
			  Vaex requires conversion of CSV to HDF5 format, which doesn’t bother me as you can go to lunch, come back, and the data will be converted. I also understand that in harsh conditions (like in the experiment) with little or no main memory reading data will take longer.
			  
			  What I don’t understand is the time that Vaex needed to display the head of the file (1189 seconds for the first 5 rows!). Other operations in Vaex are heavily optimized, which enables us to do interactive data analysis on bigger than main memory datasets.
			  
			  I kinda expected the problems with Dask as it is more optimized for compute clusters instead of a single machine. Dask is built on top of pandas, which means that operations that are slow in pandas, stay slow in Dask.
			  
			  The winner of the experiment is clear. Vaex was able to process bigger than the main memory file on a laptop while Dask couldn’t. This experiment is specific as I am testing performance on a single machine, not a compute cluster.
			  
			  [Original](https://towardsdatascience.com/are-you-still-using-pandas-to-process-big-data-in-2021-850ab26ad919). Reposted with permission.
			  
			  **Related:**
			  
			  * [Pandas on Steroids: End to End Data Science in Python with Dask](https://www.kdnuggets.com/2020/11/pandas-steroids-dask-python-data-science.html)
			  * [Why and How to Use Dask with Big Data](https://www.kdnuggets.com/2020/04/dask-big-data.html)
			  * [Good-bye Big Data. Hello, Massive Data!](https://www.kdnuggets.com/2020/10/sqream-massive-data.html)
	- Use Sentiment Analysis With Python to Classify Movie Reviews – Real Python](https://omnivore.app/me/use-sentiment-analysis-with-python-to-classify-movie-reviews-rea-18bc48bdfb5)
	  collapsed:: true
	  site:: [Real Python](https://realpython.com/sentiment-analysis-python/)
	  author:: Real Python
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 9th, 2020]]
		- ### Content
			- ![Use Sentiment Analysis With Python to Classify Movie Reviews](https://proxy-prod.omnivore-image-cache.app/1920x1080,smD5v4x41mlKzyD9fxCTbhhtszS79DEyCTjU43ftz4mU/https://files.realpython.com/media/Use-Sentiment-Analysis-With-Python-to-Classify-Reviews_Watermarked.e73ba441d870.jpg) 
			  
			  **Sentiment analysis** is a powerful tool that allows computers to understand the underlying subjective tone of a piece of writing. This is something that humans have difficulty with, and as you might imagine, it isn’t always so easy for computers, either. But with the right tools and Python, you can use sentiment analysis to better understand the **sentiment** of a piece of writing.
			  
			  Why would you want to do that? There are a lot of uses for sentiment analysis, such as understanding how stock traders feel about a particular company by using social media data or aggregating reviews, which you’ll get to do by the end of this tutorial. 
			  
			  **In this tutorial, you’ll learn:**
			  
			  * How to use **natural language processing (NLP)** techniques
			  * How to use machine learning to **determine the sentiment** of text
			  * How to use spaCy to **build an NLP pipeline** that feeds into a sentiment analysis classifier
			  
			  This tutorial is ideal for beginning machine learning practitioners who want a project-focused guide to building sentiment analysis pipelines with spaCy. 
			  
			  You should be familiar with basic [machine learning techniques](https://realpython.com/tutorials/machine-learning) like binary classification as well as the concepts behind them, such as training loops, data batches, and weights and biases. If you’re unfamiliar with machine learning, then you can kickstart your journey by learning about [logistic regression](https://realpython.com/logistic-regression-python/).
			  
			  When you’re ready, you can follow along with the examples in this tutorial by downloading the source code from the link below:
			  
			  \#\# Using Natural Language Processing to Preprocess and Clean Text Data[](\#using-natural-language-processing-to-preprocess-and-clean-text-data "Permanent link")
			  
			  Any sentiment analysis workflow begins with loading data. But what do you do once the data’s been loaded? You need to process it through a **natural language processing pipeline** before you can do anything interesting with it.
			  
			  The necessary steps include (but aren’t limited to) the following:
			  
			  1. **Tokenizing sentences** to break text down into sentences, words, or other units
			  2. **Removing stop words** like “if,” “but,” “or,” and so on
			  3. **Normalizing words** by condensing all forms of a word into a single form
			  4. **Vectorizing text** by turning the text into a numerical representation for consumption by your classifier
			  
			  All these steps serve to reduce the **noise** inherent in any human-readable text and improve the accuracy of your classifier’s results. There are lots of great tools to help with this, such as the [Natural Language Toolkit](https://www.nltk.org/), [TextBlob](https://textblob.readthedocs.io/en/dev/index.html), and [spaCy](https://spacy.io/). For this tutorial, you’ll use spaCy.
			  
			  Before you go further, make sure you have spaCy and its English model installed:
			  
			  The first command installs spaCy, and the second uses spaCy to download its English language model. spaCy supports a number of different languages, which are listed on the [spaCy website](https://spacy.io/usage/models).
			  
			  Next, you’ll learn how to use spaCy to help with the preprocessing steps you learned about earlier, starting with tokenization.
			  
			  \#\#\# Tokenizing[](\#tokenizing "Permanent link")
			  
			  **Tokenization** is the process of breaking down chunks of text into smaller pieces. spaCy comes with a default processing pipeline that begins with tokenization, making this process a snap. In spaCy, you can do either sentence tokenization or word tokenization:
			  
			  * **Word tokenization** breaks text down into individual words.
			  * **Sentence tokenization** breaks text down into individual sentences.
			  
			  In this tutorial, you’ll use word tokenization to separate the text into individual words. First, you’ll load the text into spaCy, which does the work of tokenization for you:
			  
			  In this code, you set up some example text to tokenize, load spaCy’s English model, and then tokenize the text by passing it into the `nlp` constructor. This model includes a default processing pipeline that you can customize, as you’ll see later in the project section.
			  
			  After that, you generate a list of tokens and print it. As you may have noticed, “word tokenization” is a slightly misleading term, as captured tokens include punctuation and other nonword strings. 
			  
			  Tokens are an important container type in spaCy and have a very rich set of features. In the next section, you’ll learn how to use one of those features to filter out stop words.
			  
			  \#\#\# Removing Stop Words[](\#removing-stop-words "Permanent link")
			  
			  **Stop words** are words that may be important in human communication but are of little value for machines. spaCy comes with a default list of stop words that you can customize. For now, you’ll see how you can use token attributes to remove stop words:
			  
			  In one line of Python code, you filter out stop words from the tokenized text using the `.is_stop` token attribute.
			  
			  What differences do you notice between this output and the output you got after tokenizing the text? With the stop words removed, the token list is much shorter, and there’s less context to help you understand the tokens.
			  
			  \#\#\# Normalizing Words[](\#normalizing-words "Permanent link")
			  
			  **Normalization** is a little more complex than tokenization. It entails condensing all forms of a word into a single representation of that word. For instance, “watched,” “watching,” and “watches” can all be normalized into “watch.” There are two major normalization methods: 
			  
			  1. Stemming
			  2. Lemmatization
			  
			  With **stemming**, a word is cut off at its **stem**, the smallest unit of that word from which you can create the descendant words. You just saw an example of this above with “watch.” Stemming simply truncates the string using common endings, so it will miss the relationship between “feel” and “felt,” for example.
			  
			  **Lemmatization** seeks to address this issue. This process uses a data structure that relates all forms of a word back to its simplest form, or **lemma**. Because lemmatization is generally more powerful than stemming, it’s the only normalization strategy offered by spaCy.
			  
			  Luckily, you don’t need any additional code to do this. It happens automatically—along with a number of other activities, such as **part of speech tagging** and **named entity recognition**—when you call `nlp()`. You can inspect the lemma for each token by taking advantage of the `.lemma_` attribute:
			  
			  All you did here was generate a readable list of tokens and lemmas by iterating through the filtered list of tokens, taking advantage of the `.lemma_` attribute to inspect the lemmas. This example shows only the first few tokens and lemmas. Your output will be much longer.
			  
			  The next step is to represent each token in way that a machine can understand. This is called **vectorization**.
			  
			  \#\#\# Vectorizing Text[](\#vectorizing-text "Permanent link")
			  
			  **Vectorization** is a process that transforms a token into a **vector**, or a numeric array that, in the context of NLP, is unique to and represents various features of a token. Vectors are used under the hood to find word similarities, classify text, and perform other NLP operations.
			  
			  This particular representation is a **dense array**, one in which there are defined values for every space in the array. This is in opposition to earlier methods that used **sparse arrays**, in which most spaces are empty.
			  
			  Like the other steps, vectorization is taken care of automatically with the `nlp()` call. Since you already have a list of token objects, you can get the vector representation of one of the tokens like so:
			  
			  Here you use the `.vector` attribute on the second token in the `filtered_tokens` list, which in this set of examples is the word `Dave`.
			  
			  Now that you’ve learned about some of the typical text preprocessing steps in spaCy, you’ll learn how to classify text.
			  
			  \#\# Using Machine Learning Classifiers to Predict Sentiment[](\#using-machine-learning-classifiers-to-predict-sentiment "Permanent link")
			  
			  Your text is now processed into a form understandable by your computer, so you can start to work on classifying it according to its sentiment. You’ll cover three topics that will give you a general understanding of [machine learning classification](https://realpython.com/logistic-regression-python/\#classification) of text data:
			  
			  1. What machine learning tools are available and how they’re used
			  2. How classification works
			  3. How to use spaCy for text classification
			  
			  First, you’ll learn about some of the available tools for doing machine learning classification.
			  
			  \#\#\# Machine Learning Tools[](\#machine-learning-tools "Permanent link")
			  
			  There are a number of tools available in Python for solving classification problems. Here are some of the more popular ones:
			  
			  * [TensorFlow](https://www.tensorflow.org/)
			  * [PyTorch](https://pytorch.org/)
			  * [scikit-learn](https://scikit-learn.org/)
			  
			  This list isn’t all-inclusive, but these are the more widely used machine learning frameworks available in Python. They’re large, powerful frameworks that take a lot of time to truly master and understand.
			  
			  **TensorFlow** is developed by Google and is one of the most popular machine learning frameworks. You use it primarily to implement your own machine learning algorithms as opposed to using existing algorithms. It’s fairly low-level, which gives the user a lot of power, but it comes with a steep learning curve.
			  
			  **PyTorch** is Facebook’s answer to TensorFlow and accomplishes many of the same goals. However, it’s built to be more familiar to Python programmers and has become a very popular framework in its own right. Because they have similar use cases, [comparing TensorFlow and PyTorch](https://realpython.com/pytorch-vs-tensorflow/) is a useful exercise if you’re considering learning a framework.
			  
			  **scikit-learn** stands in contrast to TensorFlow and PyTorch. It’s higher-level and allows you to use off-the-shelf machine learning algorithms rather than building your own. What it lacks in customizability, it more than makes up for in ease of use, allowing you to quickly train classifiers in just a few lines of code.
			  
			  Luckily, spaCy provides a fairly straightforward built-in text classifier that you’ll learn about a little later. First, however, it’s important to understand the general workflow for any sort of classification problem.
			  
			  \#\#\# How Classification Works[](\#how-classification-works "Permanent link")
			  
			  Don’t worry—for this section you won’t go deep into [linear algebra](https://realpython.com/python-linear-algebra/), vector spaces, or other esoteric concepts that power machine learning in general. Instead, you’ll get a practical introduction to the workflow and constraints common to classification problems.
			  
			  Once you have your vectorized data, a basic workflow for classification looks like this:
			  
			  1. [Split your data into training and evaluation sets.](https://realpython.com/train-test-split-python-data/)
			  2. Select a model architecture.
			  3. Use training data to train your model.
			  4. Use test data to evaluate the performance of your model.
			  5. Use your trained model on new data to generate predictions, which in this case will be a number between -1.0 and 1.0.
			  
			  This list isn’t exhaustive, and there are a number of additional steps and variations that can be done in an attempt to improve accuracy. For example, machine learning practitioners often split their datasets into three sets: 
			  
			  1. Training
			  2. Validation
			  3. Test
			  
			  The **training set**, as the name implies, is used to train your model. The **validation set** is used to help tune the **hyperparameters** of your model, which can lead to better performance. 
			  
			  The **test set** is a dataset that incorporates a wide variety of data to accurately judge the performance of the model. Test sets are often used to compare multiple models, including the same models at different stages of training.
			  
			  Now that you’ve learned the general flow of classification, it’s time to put it into action with spaCy.
			  
			  \#\#\# How to Use spaCy for Text Classification[](\#how-to-use-spacy-for-text-classification "Permanent link")
			  
			  You’ve already learned how spaCy does much of the text preprocessing work for you with the `nlp()` constructor. This is really helpful since training a classification model requires many examples to be useful.
			  
			  Additionally, spaCy provides a pipeline functionality that powers much of the magic that happens under the hood when you call `nlp()`. The default pipeline is defined in a [JSON](https://realpython.com/python-json/) file associated with whichever preexisting model you’re using (`en_core_web_sm` for this tutorial), but you can also build one from scratch if you wish.
			  
			  What does this have to do with classification? One of the built-in pipeline components that spaCy provides is called `textcat` (short for `TextCategorizer`), which enables you to assign categories (or **labels**) to your text data and use that as training data for a neural network. 
			  
			  This process will generate a trained model that you can then use to predict the sentiment of a given piece of text. To take advantage of this tool, you’ll need to do the following steps:
			  
			  1. Add the `textcat` component to the existing pipeline.
			  2. Add valid labels to the `textcat` component.
			  3. Load, shuffle, and split your data.
			  4. Train the model, evaluating on each training loop.
			  5. Use the trained model to predict the sentiment of non-training data.
			  6. Optionally, save the trained model.
			  
			  In the next section, you’ll learn how to put all these pieces together by building your own project: a movie review sentiment analyzer.
			  
			  \#\# Building Your Own NLP Sentiment Analyzer[](\#building-your-own-nlp-sentiment-analyzer "Permanent link")
			  
			  From the previous sections, you’ve probably noticed four major stages of building a sentiment analysis pipeline: 
			  
			  1. Loading data
			  2. Preprocessing
			  3. Training the classifier
			  4. Classifying data
			  
			  For building a real-life sentiment analyzer, you’ll work through each of the steps that compose these stages. You’ll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) compiled by [Andrew Maas](http://www.andrew-maas.net/) to train and test your sentiment analyzer. Once you’re ready, proceed to the next section to load your data.
			  
			  \#\#\# Loading and Preprocessing Data[](\#loading-and-preprocessing-data "Permanent link")
			  
			  If you haven’t already, download and extract the Large Movie Review Dataset. Spend a few minutes poking around, taking a look at its structure, and sampling some of the data. This will inform how you load the data. For this part, you’ll use [spaCy’s textcat example](https://spacy.io/usage/examples\#textcat) as a rough guide. 
			  
			  You can (and should) decompose the loading stage into concrete steps to help plan your coding. Here’s an example:
			  
			  1. Load text and labels from the file and directory structures.
			  2. Shuffle the data.
			  3. Split the data into training and test sets.
			  4. Return the two sets of data.
			  
			  This process is relatively self-contained, so it should be its own function at least. In thinking about the actions that this function would perform, you may have thought of some possible parameters.
			  
			  Since you’re splitting data, the ability to control the size of those splits may be useful, so `split` is a good parameter to include. You may also wish to limit the total amount of documents you process with a `limit` parameter. You can open your favorite editor and add this function signature:
			  
			  With this signature, you take advantage of Python 3’s [type annotations](https://realpython.com/lessons/type-hinting/) to make it absolutely clear which types your function expects and what it will return. 
			  
			  The parameters here allow you to define the directory in which your data is stored as well as the ratio of training data to test data. A good ratio to start with is 80 percent of the data for training data and 20 percent for test data. All of this and the following code, unless otherwise specified, should live in the same file.
			  
			  Next, you’ll want to iterate through all the files in this dataset and load them into a list:
			  
			  While this may seem complicated, what you’re doing is constructing the directory structure of the data, looking for and opening text files, then appending a tuple of the contents and a label dictionary to the `reviews` list. 
			  
			  The label dictionary structure is a format required by the spaCy model during the training loop, which you’ll see soon.
			  
			  Since you have each review open at this point, it’s a good idea to replace the `<br />` HTML tags in the texts with newlines and to use `.strip()` to remove all leading and trailing whitespace. 
			  
			  For this project, you won’t remove stop words from your training data right away because it could change the meaning of a sentence or phrase, which could reduce the predictive power of your classifier. This is dependent somewhat on the stop word list that you use.
			  
			  After loading the files, you want to shuffle them. This works to eliminate any possible bias from the order in which training data is loaded. Since the `random` module makes this easy to do in one line, you’ll also see how to split your shuffled data:
			  
			  Here, you shuffle your data with a call to `random.shuffle()`. Then you optionally truncate and split the data using some math to convert the split to a number of items that define the split boundary. Finally, you [return](https://realpython.com/python-return-statement/) two parts of the `reviews` list using list slices.
			  
			  Here’s a sample output, truncated for brevity:
			  
			  To learn more about how `random` works, take a look at [Generating Random Data in Python (Guide)](https://realpython.com/python-random/).
			  
			  Now that you’ve got your data loader built and have some light preprocessing done, it’s time to build the spaCy pipeline and classifier training loop. 
			  
			  \#\#\# Training Your Classifier[](\#training-your-classifier "Permanent link")
			  
			  Putting the spaCy pipeline together allows you to rapidly build and train a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional%5Fneural%5Fnetwork) (CNN) for classifying text data. While you’re using it here for sentiment analysis, it’s general enough to work with any kind of [text classification](https://realpython.com/python-keras-text-classification/) task as long as you provide it with the training data and labels.
			  
			  In this part of the project, you’ll take care of three steps:
			  
			  1. Modifying the base spaCy pipeline to include the `textcat` component
			  2. Building a training loop to train the `textcat` component
			  3. Evaluating the progress of your model training after a given number of training loops
			  
			  First, you’ll add `textcat` to the default spaCy pipeline.
			  
			  \#\#\#\# Modifying the spaCy Pipeline to Include `textcat`[](\#modifying-the-spacy-pipeline-to-include-textcat "Permanent link")
			  
			  For the first part, you’ll load the same pipeline as you did in the examples at the beginning of this tutorial, then you’ll add the `textcat` component if it isn’t already present. After that, you’ll add the labels that your data uses (`"pos"` for positive and `"neg"` for negative) to `textcat`. Once that’s done, you’ll be ready to build the training loop:
			  
			  If you’ve looked at the spaCy documentation’s [textcat example](https://spacy.io/usage/examples\#textcat) already, then this should look pretty familiar. First, you load the built-in `en_core_web_sm` pipeline, then you check the `.pipe_names` attribute to see if the `textcat` component is already available.
			  
			  If it isn’t, then you create the component (also called a **pipe**) with `.create_pipe()`, passing in a configuration dictionary. There are a few options that you can work with described in the [TextCategorizer documentation](https://spacy.io/api/textcategorizer\#init). 
			  
			  Finally, you add the component to the pipeline using `.add_pipe()`, with the `last` parameter signifying that this component should be added to the end of the pipeline.
			  
			  Next, you’ll handle the case in which the `textcat` component is present and then add the labels that will serve as the categories for your text:
			  
			  If the component is present in the loaded pipeline, then you just use `.get_pipe()` to assign it to a variable so you can work on it. For this project, all that you’ll be doing with it is adding the labels from your data so that `textcat` knows what to look for. You’ll do that with `.add_label()`.
			  
			  You’ve created the pipeline and prepared the `textcat` component for the labels it will use for training. Now it’s time to write the training loop that will allow `textcat` to categorize movie reviews.
			  
			  \#\#\#\# Build Your Training Loop to Train `textcat`[](\#build-your-training-loop-to-train-textcat "Permanent link")
			  
			  To begin the training loop, you’ll first set your pipeline to train only the `textcat` component, generate **batches** of data for it with spaCy’s `minibatch()` and `compounding()` utilities, and then go through them and update your model.
			  
			  A **batch** is just a subset of your data. Batching your data allows you to reduce the memory footprint during training and more quickly update your hyperparameters.
			  
			  Here’s an implementation of the training loop described above:
			  
			  On lines 25 to 27, you create a list of all components in the pipeline that aren’t the `textcat` component. You then use the `nlp.disable()` context manager to disable those components for all code within the context manager’s scope.
			  
			  Now you’re ready to add the code to begin training:
			  
			  Here, you call `nlp.begin_training()`, which returns the initial optimizer function. This is what `nlp.update()` will use to update the weights of the underlying model.
			  
			  You then use the `compounding()` utility to create a generator, giving you an infinite series of `batch_sizes` that will be used later by the `minibatch()` utility.
			  
			  Now you’ll begin training on batches of data:
			  
			  Now, for each iteration that is specified in the `train_model()` signature, you create an empty dictionary called `loss` that will be updated and used by `nlp.update()`. You also shuffle the training data and split it into batches of varying size with `minibatch()`.
			  
			  For each batch, you separate the text and labels, then fed them, the empty `loss` dictionary, and the `optimizer` to `nlp.update()`. This runs the actual training on each example.
			  
			  The `dropout` parameter tells `nlp.update()` what proportion of the training data in that batch to skip over. You do this to make it harder for the model to accidentally just memorize training data without coming up with a generalizable model.
			  
			  This will take some time, so it’s important to periodically evaluate your model. You’ll do that with the data that you held back from the training set, also known as the **holdout set**.
			  
			  \#\#\#\# Evaluating the Progress of Model Training[](\#evaluating-the-progress-of-model-training "Permanent link")
			  
			  Since you’ll be doing a number of evaluations, with many calculations for each one, it makes sense to write a separate `evaluate_model()` function. In this function, you’ll run the documents in your test set against the unfinished model to get your model’s predictions and then compare them to the correct labels of that data.
			  
			  Using that information, you’ll calculate the following values:
			  
			  * **True positives** are documents that your model correctly predicted as positive. For this project, this maps to the positive sentiment but generalizes in binary classification tasks to the class you’re trying to identify.
			  * **False positives** are documents that your model incorrectly predicted as positive but were in fact negative.
			  * **True negatives** are documents that your model correctly predicted as negative.
			  * **False negatives** are documents that your model incorrectly predicted as negative but were in fact positive.
			  
			  Because your model will return a score between 0 and 1 for each label, you’ll determine a positive or negative result based on that score. From the four statistics described above, you’ll calculate precision and recall, which are common measures of classification model performance:
			  
			  * **Precision** is the ratio of true positives to all items your model marked as positive (true _and_ false positives). A precision of 1.0 means that every review that your model marked as positive belongs to the positive class.
			  * **Recall** is the ratio of true positives to all reviews that are _actually_ positive, or the number of true positives divided by the total number of true positives and false negatives.
			  
			  The **F-score** is another popular accuracy measure, especially in the world of NLP. Explaining it could take its own article, but you’ll see the calculation in the code. As with precision and recall, the score ranges from 0 to 1, with 1 signifying the highest performance and 0 the lowest.
			  
			  For `evaluate_model()`, you’ll need to pass in the pipeline’s `tokenizer` component, the `textcat` component, and your test dataset:
			  
			  In this function, you separate reviews and their labels and then use a [generator expression](https://realpython.com/introduction-to-python-generators/) to tokenize each of your evaluation reviews, preparing them to be passed in to `textcat`. The generator expression is a nice trick recommended in [the spaCy documentation](https://spacy.io/usage/examples\#textcat) that allows you to iterate through your tokenized reviews without keeping every one of them in memory. 
			  
			  You then use the `score` and `true_label` to determine true or false positives and true or false negatives. You then use those to calculate precision, recall, and f-score. Now all that’s left is to actually call `evaluate_model()`:
			  
			  Here you add a [print statement](https://realpython.com/python-print/) to help organize the output from `evaluate_model()` and then call it with the `.use_params()` context manager in order to use the model in its current state. You then call `evaluate_model()` and print the results.
			  
			  Once the training process is complete, it’s a good idea to save the model you just trained so that you can use it again without training a new model. After your training loop, add this code to save the trained model to a directory called `model_artifacts` located within your working directory:
			  
			  This snippet saves your model to a directory called `model_artifacts` so that you can make tweaks without retraining the model. Your final training function should look like this:
			  
			  In this section, you learned about training a model and evaluating its performance as you train it. You then built a function that trains a classification model on your input data. 
			  
			  \#\#\# Classifying Reviews[](\#classifying-reviews "Permanent link")
			  
			  Now that you have a trained model, it’s time to test it against a real review. For the purposes of this project, you’ll hardcode a review, but you should certainly try extending this project by reading reviews from other sources, such as files or a review aggregator’s API.
			  
			  The first step with this new function will be to load the previously saved model. While you could use the model in memory, loading the saved **model artifact** allows you to optionally skip training altogether, which you’ll see later. Here’s the `test_model()` signature along with the code to load your saved model:
			  
			  In this code, you define `test_model()`, which includes the `input_data` parameter. You then load your previously saved model. 
			  
			  The IMDB data you’re working with includes an `unsup` directory within the training data directory that contains unlabeled reviews you can use to test your model. Here’s one such review. You should save it (or a different one of your choosing) in a `TEST_REVIEW` constant at the top of your file:
			  
			  Next, you’ll pass this review into your model to generate a prediction, prepare it for display, and then display it to the user:
			  
			  In this code, you pass your `input_data` into your `loaded_model`, which generates a prediction in the `cats` attribute of the `parsed_text` variable. You then check the scores of each sentiment and save the highest one in the `prediction` variable.
			  
			  You then save that sentiment’s score to the `score` variable. This will make it easier to create human-readable output, which is the last line of this function.
			  
			  You’ve now written the `load_data()`, `train_model()`, `evaluate_model()`, and `test_model()` functions. That means it’s time to put them all together and train your first model. 
			  
			  \#\#\# Connecting the Pipeline[](\#connecting-the-pipeline "Permanent link")
			  
			  So far, you’ve built a number of independent functions that, taken together, will load data and train, evaluate, save, and test a sentiment analysis classifier in Python. 
			  
			  There’s one last step to make these functions usable, and that is to call them when the script is run. You’ll use the [if \_\_name\_\_ \== "\_\_main\_\_": idiom](https://realpython.com/if-name-main-python/) to accomplish this:
			  
			  Here you load your training data with the function you wrote in the [Loading and Preprocessing Data](\#loading-and-preprocessing-data) section and limit the number of reviews used to `2500` total. You then train the model using the `train_model()` function you wrote in [Training Your Classifier](\#training-your-classifier) and, once that’s done, you call `test_model()` to test the performance of your model.
			  
			  What did your model predict? Do you agree with the result? What happens if you increase or decrease the `limit` parameter when loading the data? Your scores and even your predictions may vary, but here’s what you should expect your output to look like:
			  
			  As your model trains, you’ll see the measures of loss, precision, and recall and the F-score for each training iteration. You should see the loss generally decrease. The precision, recall, and F-score will all bounce around, but ideally they’ll increase. Then you’ll see the test review, sentiment prediction, and the score of that prediction—the higher the better. 
			  
			  You’ve now trained your first sentiment analysis machine learning model using natural language processing techniques and neural networks with spaCy! Here are two charts showing the model’s performance across twenty training iterations. The first chart shows how the loss changes over the course of training:
			  
			  [![Loss over training iterations](https://proxy-prod.omnivore-image-cache.app/600x371,sEZDourCjH7MYqDrjMuGgvGrZpz8o6WBtd3DEGrriEBY/https://files.realpython.com/media/loss_chart.c0fecc2d5c49.png)](https://files.realpython.com/media/loss%5Fchart.c0fecc2d5c49.png)
			  
			  While the above graph shows loss over time, the below chart plots the precision, recall, and F-score over the same training period:
			  
			  [![The precision, recall, and f-score of the model over training iterations](https://proxy-prod.omnivore-image-cache.app/600x371,splw2c_8pJ5w7TlH7XtdVg46Z_dbfPjDQomAVI7xQF9I/https://files.realpython.com/media/precision_recall_fscore_chart.b232474e1a64.png)](https://files.realpython.com/media/precision%5Frecall%5Ffscore%5Fchart.b232474e1a64.png)
			  
			  In these charts, you can see that the loss starts high but drops very quickly over training iterations. The precision, recall, and F-score are pretty stable after the first few training iterations. What could you tinker with to improve these values?
			  
			  \#\# Conclusion[](\#conclusion "Permanent link")
			  
			  Congratulations on building your first sentiment analysis model in Python! What did you think of this project? Not only did you build a useful tool for data analysis, but you also picked up on a lot of the fundamental concepts of natural language processing and machine learning.
			  
			  **In this tutorial, you learned how to:**
			  
			  * Use **natural language processing** techniques
			  * Use a **machine learning classifier** to determine the sentiment of processed text data
			  * Build your own **NLP pipeline** with spaCy
			  
			  You now have the basic toolkit to build more models to answer any research questions you might have. If you’d like to review what you’ve learned, then you can download and experiment with the code used in this tutorial at the link below:
			  
			  What else could you do with this project? See below for some suggestions.
			  
			  \#\# Next Steps With Sentiment Analysis and Python[](\#next-steps-with-sentiment-analysis-and-python "Permanent link")
			  
			  This is a core project that, depending on your interests, you can build a lot of functionality around. Here are a few ideas to get you started on extending this project:
			  
			  * The data-loading process loads every review into memory during `load_data()`. Can you make it more memory efficient by using **generator functions** instead?
			  * Rewrite your code to **remove stop words** during preprocessing or data loading. How does the mode performance change? Can you incorporate this preprocessing into a pipeline component instead?
			  * Use a tool like [Click](https://realpython.com/python-click/) to generate an interactive **command-line interface**.
			  * **Deploy your model** to a cloud platform like [AWS](https://aws.amazon.com/about-aws/) and wire an API to it. This can form the basis of a web-based tool.
			  * Explore the **configuration parameters** for the `textcat` pipeline component and experiment with different configurations.
			  * Explore different ways to **pass in new reviews** to generate predictions.
			  * **Parametrize options** such as where to save and load trained models, whether to skip training or train a new model, and so on.
			  
			  This project uses the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/), which is maintained by [Andrew Maas](http://www.andrew-maas.net/). Thanks to Andrew for making this curated dataset widely available for use.
			  
			  Get a short & sweet **Python Trick** delivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team.
			  
			  ![Python Tricks Dictionary Merge](https://proxy-prod.omnivore-image-cache.app/738x490,sUTwRFkPkUttK_PnC-NI52F2YotqwovLa_-ll0U08urw/https://realpython.com/static/pytrick-dict-merge.4201a0125a5e.png) 
			  
			  [![Kyle Stratis](https://proxy-prod.omnivore-image-cache.app/400x400,s_APapmH_rvcGRHx1WjHu6ugcroizAuBbECcFTZJDU3c/https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/KEK9iuEG_400x400.28b60a4581c0.jpg&w=400&h=400&mode=crop&sig=445d048c1ba88637a77de71092496a250141f2ad)](https://realpython.com/team/kstratis/) [![Kyle Stratis](https://proxy-prod.omnivore-image-cache.app/400x400,s_APapmH_rvcGRHx1WjHu6ugcroizAuBbECcFTZJDU3c/https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/KEK9iuEG_400x400.28b60a4581c0.jpg&w=400&h=400&mode=crop&sig=445d048c1ba88637a77de71092496a250141f2ad)](https://realpython.com/team/kstratis/) 
			  
			  Kyle is a self-taught developer working as a senior data engineer at Vizit Labs. In the past, he has founded DanqEx (formerly Nasdanq: the original meme stock exchange) and Encryptid Gaming.
			  
			  [» More about Kyle](https://realpython.com/team/kstratis/) 
			  
			  ---
			  
			  _Each tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are:_
			  
			  [![Joanna Jablonski](https://proxy-prod.omnivore-image-cache.app/800x800,sTFAoEjzOcExxvoootOYCmG9xK1pWSCbHwSFeLvMNER4/https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg&w=800&h=800&mode=crop&sig=c363b704eeccb35f2247db13baff3d4383459858)](https://realpython.com/team/jjablonski/) 
			  
			  [![Jacob Schmitt](https://proxy-prod.omnivore-image-cache.app/400x400,sCMFP0akPcAR6_JmtSeWEsMTQIJGEDEnPPYcpKPgyX7A/https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/profile-small_js.2f4d0d8da1ca.jpg&w=400&h=400&mode=crop&sig=d10d9fc35ba4a6608969e71b4c24c1e61176ee2d)](https://realpython.com/team/jschmitt/) 
			  
			  Master Real-World Python Skills With Unlimited Access to Real Python
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/510x260,snMMRsPEo01z3ELqy6F7IFzzex5sbWuyoy_VCM42zuBU/https://realpython.com/static/videos/lesson-locked.f5105cfd26db.svg)
			  
			  **Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:**
			  
			  [Level Up Your Python Skills »](https://realpython.com/account/join/?utm%5Fsource=rp%5Farticle%5Ffooter&utm%5Fcontent=sentiment-analysis-python) 
			  
			  Master Real-World Python Skills  
			  With Unlimited Access to Real Python
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/510x260,snMMRsPEo01z3ELqy6F7IFzzex5sbWuyoy_VCM42zuBU/https://realpython.com/static/videos/lesson-locked.f5105cfd26db.svg)
			  
			  **Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:**
			  
			  [Level Up Your Python Skills »](https://realpython.com/account/join/?utm%5Fsource=rp%5Farticle%5Ffooter&utm%5Fcontent=sentiment-analysis-python)
	- Comprehensive Guide to Grouping and Aggregating with Pandas - Practical Business Python](https://omnivore.app/me/comprehensive-guide-to-grouping-and-aggregating-with-pandas-prac-18bc48bddad)
	  collapsed:: true
	  site:: [pbpython.com](https://pbpython.com/groupby-agg.html)
	  author:: Chris Moffitt
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 8th, 2020]]
		- ### Content
			- ![article header image](https://proxy-prod.omnivore-image-cache.app/0x0,s3E6o5kvOJEafeduHZ3MTocwAp3RaO0pZMWohXx9e6fk/https://pbpython.com/images/book-header.png)
			  
			  \#\# Introduction
			  
			  One of the most basic analysis functions is grouping and aggregating data. In some cases, this level of analysis may be sufficient to answer business questions. In other instances, this activity might be the first step in a more complex data science analysis. In pandas, the `groupby` function can be combined with one or more aggregation functions to quickly and easily summarize data. This concept is deceptively simple and most new pandas users will understand this concept. However, they might be surprised at how useful complex aggregation functions can be for supporting sophisticated analysis.
			  
			  This article will quickly summarize the basic pandas aggregation functions and show examples of more complex custom aggregations. Whether you are a new or more experienced pandas user, I think you will learn a few things from this article.
			  
			  \#\# Aggregating
			  
			  In the context of this article, an aggregation function is one which takes multiple individual values and returns a summary. In the majority of the cases, this summary is a single value.
			  
			  The most common aggregation functions are a simple average or summation of values. As of pandas 0.20, you may call an aggregation function on one or more columns of a DataFrame.
			  
			  Here’s a quick example of calculating the total and average fare using the Titanic dataset (loaded from seaborn):
			  
			  import pandas as pd
			  import seaborn as sns
			  
			  df = sns.load_dataset('titanic')
			  
			  df['fare'].agg(['sum', 'mean'])
			  
			  sum     28693.949300
			  mean       32.204208
			  Name: fare, dtype: float64
			  
			  This simple concept is a necessary building block for more complex analysis.
			  
			  One area that needs to be discussed is that there are multiple ways to call an aggregation function. As shown above, you may pass a list of functions to apply to one or more columns of data.
			  
			  What if you want to perform the analysis on only a subset of columns? There are two other options for aggregations: using a dictionary or a [named aggregation](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html).
			  
			  Here is a comparison of the the three options:
			  
			  ![Pandas aggregation options](https://proxy-prod.omnivore-image-cache.app/0x0,sh2uFJSTFDiv7U0dRyJYLcODnvCykILTVy0ivz045HCU/https://pbpython.com/images/agg-options.png) 
			  
			  It is important to be aware of these options and know which one to use when.
			  
			  Choosing an aggregation approach
			  
			  As a general rule, I prefer to use dictionaries for aggregations.
			  
			  The tuple approach is limited by only being able to apply one aggregation at a time to a specific column. If I need to rename columns, then I will use the `rename` function after the aggregations are complete. In some specific instances, the list approach is a useful shortcut. I will reiterate though, that I think the dictionary approach provides the most robust approach for the majority of situations.
			  
			  \#\# Groupby
			  
			  Now that we know how to use aggregations, we can combine this with `groupby` to summarize data.
			  
			  \#\#\# Basic math
			  
			  The most common built in aggregation functions are basic math functions including sum, mean, median, minimum, maximum, standard deviation, variance, mean absolute deviation and product.
			  
			  We can apply all these functions to the `fare` while grouping by the `embark_town`:
			  
			  agg_func_math = {
			    'fare':
			    ['sum', 'mean', 'median', 'min', 'max', 'std', 'var', 'mad', 'prod']
			  }
			  df.groupby(['embark_town']).agg(agg_func_math).round(2)
			  
			  ![Basic math functions](https://proxy-prod.omnivore-image-cache.app/0x0,sEZlNg5e32zeYOWKVK6fU5ceOBKN5bNoAjpORFqGzW9g/https://pbpython.com/images/agg_func_math.png) 
			  
			  This is all relatively straightforward math.
			  
			  As an aside, I have not found a good usage for the `prod` function which computes the product of all the values in a group. For the sake of completeness, I am including it.
			  
			  One other useful shortcut is to use `describe` to run multiple built-in aggregations at one time:
			  
			  agg_func_describe = {'fare': ['describe']}
			  df.groupby(['embark_town']).agg(agg_func_describe).round(2)
			  
			  ![Basic math functions](https://proxy-prod.omnivore-image-cache.app/0x0,s2kNHqy0YIEFU-rVLqw_SuRAdcP34q_aWCPvAvuHRVC4/https://pbpython.com/images/agg-describe.png) 
			  
			  \#\#\# Counting
			  
			  After basic math, counting is the next most common aggregation I perform on grouped data. In some ways, this can be a little more tricky than the basic math. Here are three examples of counting:
			  
			  agg_func_count = {'embark_town': ['count', 'nunique', 'size']}
			  df.groupby(['deck']).agg(agg_func_count)
			  
			  ![Basic math functions](https://proxy-prod.omnivore-image-cache.app/0x0,ssxe27eungQ3g5VPOHNPvAeRvAb-0yHwtZay9srqXSxk/https://pbpython.com/images/agg_func_count.png) 
			  
			  The major distinction to keep in mind is that `count` will not include `NaN`values whereas `size` will. Depending on the data set, this may or may not be a useful distinction. In addition, the `nunique` function will exclude `NaN` values in the unique counts. Keep reading for an example of how to include `NaN` in the unique value counts.
			  
			  \#\#\# First and last
			  
			  In this example, we can select the highest and lowest fare by embarked town. One important point to remember is that you must sort the data first if you want `first` and `last`to pick the max and min values.
			  
			  agg_func_selection = {'fare': ['first', 'last']}
			  df.sort_values(by=['fare'],
			            ascending=False).groupby(['embark_town'
			                                        ]).agg(agg_func_selection)
			  
			  ![Basic math functions](https://proxy-prod.omnivore-image-cache.app/0x0,s3psfC5e0yNgnZJLDVV69v5fKKLN7_A1pqvheDpzXz-Y/https://pbpython.com/images/agg_func_first_last.png) 
			  
			  In the example above, I would recommend using `max` and `min` but I am including` first` and `last` for the sake of completeness. In other applications (such as time series analysis) you may want to select the first and last values for further analysis.
			  
			  Another selection approach is to use `idxmax` and `idxmin` to select the index value that corresponds to the maximum or minimum value.
			  
			  agg_func_max_min = {'fare': ['idxmax', 'idxmin']}
			  df.groupby(['embark_town']).agg(agg_func_max_min)
			  
			  ![Max and Min index](https://proxy-prod.omnivore-image-cache.app/0x0,sv9TUraFzX01BSth5xciBSvaE_ZPkogSmcHCwFNf6IkI/https://pbpython.com/images/agg_idxmin_max.png) 
			  
			  We can check the results:
			  
			  ![Idxmax](https://proxy-prod.omnivore-image-cache.app/0x0,sUaQBuqcg5CSXCRHJAOrN883ls5IqJ5ga2HQ1quJm464/https://pbpython.com/images/idxmax_details.png) 
			  
			  Here’s another shortcut trick you can use to see the rows with the max `fare`:
			  
			  df.loc[df.groupby('class')['fare'].idxmax()]
			  
			  ![Idxmax](https://proxy-prod.omnivore-image-cache.app/0x0,s_8yrBJjcH48xrfusca4xiE4l_vJXqMr8Soyc-6J_Bts/https://pbpython.com/images/idxmax_details_shortcut.png) 
			  
			  The above example is one of those places where the list-based aggregation is a useful shortcut.
			  
			  \#\#\# Other libraries
			  
			  You are not limited to the aggregation functions in pandas. For instance, you could use stats functions from [scipy](https://docs.scipy.org/doc/scipy/reference/stats.html) or [numpy](https://numpy.org/doc/stable/reference/routines.statistics.html).
			  
			  Here is an example of calculating the mode and skew of the fare data.
			  
			  from scipy.stats import skew, mode
			  agg_func_stats = {'fare': [skew, mode, pd.Series.mode]}
			  df.groupby(['embark_town']).agg(agg_func_stats)
			  
			  ![Stats functions](https://proxy-prod.omnivore-image-cache.app/0x0,sQ8Gp-KfFk1xF9CjFXv91KzAlEnswek848l1cGy-MKSE/https://pbpython.com/images/agg_stats.png) 
			  
			  The mode results are interesting. The scipy.stats mode function returns the most frequent value as well as the count of occurrences. If you just want the most frequent value, use `pd.Series.mode.` 
			  
			  The key point is that you can use any function you want as long as it knows how to interpret the array of pandas values and returns a single value.
			  
			  \#\#\# Working with text
			  
			  When working with text, the counting functions will work as expected. You can also use scipy’s mode function on text data.
			  
			  One interesting application is that if you a have small number of distinct values, you can use python’s `set` function to display the full list of unique values.
			  
			  This summary of the `class` and `deck` shows how this approach can be useful for some data sets.
			  
			  agg_func_text = {'deck': [ 'nunique', mode, set]}
			  df.groupby(['class']).agg(agg_func_text)
			  
			  ![Stats functions](https://proxy-prod.omnivore-image-cache.app/0x0,s33oJbaCtjzuGb9kxgclHCnyQ1FMAnPuWt58vLUyXitI/https://pbpython.com/images/agg_text.png) 
			  
			  \#\#\# Custom functions
			  
			  The pandas standard aggregation functions and pre-built functions from the python ecosystem will meet many of your analysis needs. However, you will likely want to create your own custom aggregation functions. There are four methods for creating your own functions.
			  
			  To illustrate the differences, let’s calculate the 25th percentile of the data using four approaches:
			  
			  First, we can use a [partial](https://docs.python.org/3/library/functools.html) function:
			  
			  from functools import partial
			  \# Use partial
			  q_25 = partial(pd.Series.quantile, q=0.25)
			  q_25.__name__ = '25%'
			  
			  Next, we define our own function (which is a small wrapper around `quantile`):
			  
			  \# Define a function
			  def percentile_25(x):
			    return x.quantile(.25)
			  
			  We can define a lambda function and give it a name:
			  
			  \# Define a lambda function
			  lambda_25 = lambda x: x.quantile(.25)
			  lambda_25.__name__ = 'lambda_25%'
			  
			  Or, define the lambda inline:
			  
			  \# Use a lambda function inline
			  agg_func = {
			    'fare': [q_25, percentile_25, lambda_25, lambda x: x.quantile(.25)]
			  }
			  
			  df.groupby(['embark_town']).agg(agg_func).round(2)
			  
			  ![Custom agg functions](https://proxy-prod.omnivore-image-cache.app/0x0,savvfZEKW6mw2yu_1BG_pQVYWRhJbSrOY4u_YwnKvPSY/https://pbpython.com/images/agg_custom_funcs.png) 
			  
			  As you can see, the results are the same but the labels of the column are all a little different. This is an area of programmer preference but I encourage you to be familiar with the options since you will encounter most of these in online solutions.
			  
			  Choosing an custom function style
			  
			  I prefer to use custom functions or inline lambdas.
			  
			  Like many other areas of programming, this is an element of style and preference but I encourage you to pick one or two approaches and stick with them for consistency.
			  
			  \#\#\# Custom function examples
			  
			  As shown above, there are multiple approaches to developing custom aggregation functions. I will go through a few specific useful examples to highlight how they are frequently used.
			  
			  In most cases, the functions are lightweight wrappers around built in pandas functions. Part of the reason you need to do this is that there is no way to pass arguments to aggregations. Some examples should clarify this point.
			  
			  If you want to count the number of null values, you could use this [function](https://medium.com/escaletechblog/writing-custom-aggregation-functions-with-pandas-96f5268a8596):
			  
			  def count_nulls(s):
			    return s.size - s.count()
			  
			  If you want to include `NaN` values in your unique counts, you need to pass` dropna=False` to the `nunique` function.
			  
			  def unique_nan(s):
			    return s.nunique(dropna=False)
			  
			  Here is a summary of all the values together:
			  
			  agg_func_custom_count = {
			    'embark_town': ['count', 'nunique', 'size', unique_nan, count_nulls, set]
			  }
			  df.groupby(['deck']).agg(agg_func_custom_count)
			  
			  ![Custom agg functions](https://proxy-prod.omnivore-image-cache.app/0x0,sWDqxAhmGqVCgQY9cS5zuZpz4H_ee5eGTeipigOei1hQ/https://pbpython.com/images/agg_multiple_custom_funcs.png) 
			  
			  If you want to calculate the 90th percentile, use `quantile`:
			  
			  def percentile_90(x):
			    return x.quantile(.9)
			  
			  If you want to calculate a trimmed mean where the lowest 10th percent is excluded, use the scipy stats function `trim_mean`:
			  
			  def trim_mean_10(x):
			    return trim_mean(x, 0.1)
			  
			  If you want the largest value, regardless of the sort order (see notes above about `first` and` last`:
			  
			  def largest(x):
			    return x.nlargest(1)
			  
			  This is equivalent to `max` but I will show another example of `nlargest` below to highlight the difference.
			  
			  I wrote about sparklines [before](https://pbpython.com/styling-pandas.html). Refer to that article for install instructions. Here’s how to incorporate them into an aggregate function for a unique view of the data:
			  
			  def sparkline_str(x):
			    bins=np.histogram(x)[0]
			    sl = ''.join(sparklines(bins))
			    return sl
			  
			  Here they are all put together:
			  
			  agg_func_largest = {
			    'fare': [percentile_90, trim_mean_10, largest, sparkline_str]
			  }
			  df.groupby(['class', 'embark_town']).agg(agg_func_largest)
			  
			  ![Sparkline function](https://proxy-prod.omnivore-image-cache.app/0x0,scDn1_Dggm5a97sat3GUUhu-Sc9yaqFhwshxDPqaFi5E/https://pbpython.com/images/agg_sparkline.png) 
			  
			  The `nlargest` and `nsmallest` functions can be useful for summarizing the data in various scenarios. Here is code to show the total fares for the top 10 and bottom 10 individuals:
			  
			  def top_10_sum(x):
			    return x.nlargest(10).sum()
			  
			  def bottom_10_sum(x):
			    return x.nsmallest(10).sum()
			  
			  
			  agg_func_top_bottom_sum = {
			    'fare': [top_10_sum, bottom_10_sum]
			  }
			  df.groupby('class').agg(agg_func_top_bottom_sum)
			  
			  ![Custom agg functions](https://proxy-prod.omnivore-image-cache.app/0x0,sazUbEJ5JdqKyad6m0XkPIEa5GmRyMXMLqtP0JgO7Q20/https://pbpython.com/images/agg_top_bottom_10.png) 
			  
			  Using this approach can be useful when applying the [Pareto principle](https://en.wikipedia.org/wiki/Pareto%5Fprinciple) to your own data.
			  
			  \#\#\# Custom functions with multiple columns
			  
			  If you have a scenario where you want to run multiple aggregations across columns, then you may want to use the `groupby` combined with `apply` as described in this [stack overflow](https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns/47103408\#47103408) answer.
			  
			  Using this method, you will have access to all of the columns of the data and can choose the appropriate aggregation approach to build up your resulting DataFrame (including the column labels):
			  
			  def summary(x):
			    result = {
			        'fare_sum': x['fare'].sum(),
			        'fare_mean': x['fare'].mean(),
			        'fare_range': x['fare'].max() - x['fare'].min()
			    }
			    return pd.Series(result).round(0)
			  
			  df.groupby(['class']).apply(summary)
			  
			  ![Custom agg functions](https://proxy-prod.omnivore-image-cache.app/0x0,sslMQY0yCR4Kkwk_MAh9aBNwH3OEqTZibv1kYgtTJKSY/https://pbpython.com/images/agg-apply.png) 
			  
			  Using `apply` with `groupy` gives maximum flexibility over all aspects of the results. However, there is a downside. The `apply` function is slow so this approach should be used sparingly.
			  
			  \#\# Working with group objects
			  
			  Once you group and aggregate the data, you can do additional calculations on the grouped objects.
			  
			  For the first example, we can figure out what percentage of the total fares sold can be attributed to each `embark_town` and `class` combination. We use` assign` and a `lambda` function to add a `pct_total` column:
			  
			  df.groupby(['embark_town', 'class']).agg({
			    'fare': 'sum'
			  }).assign(pct_total=lambda x: x / x.sum())
			  
			  ![Percent of total](https://proxy-prod.omnivore-image-cache.app/0x0,sXb0WTZufnXxAisM-Cn-RaeU1z4OBcbso92VnQLmJNvw/https://pbpython.com/images/agg_pct_total.png) 
			  
			  One important thing to keep in mind is that you can actually do this more simply using a` pd.crosstab` as described in my [previous article](https://pbpython.com/pandas-crosstab.html):
			  
			  pd.crosstab(df['embark_town'],
			            df['class'],
			            values=df['fare'],
			            aggfunc='sum',
			            normalize=True)
			  
			  ![Crosstab example](https://proxy-prod.omnivore-image-cache.app/0x0,sJNCTmEzgdpCZQ701UuNmZssBuGQ6VR8g8gVyowW5WEA/https://pbpython.com/images/agg_crosstab.png) 
			  
			  While we are talking about `crosstab`, a useful concept to keep in mind is that agg functions can be combined with pivot tables too.
			  
			  Here’s a quick example:
			  
			  pd.pivot_table(data=df,
			            index=['embark_town'],
			            columns=['class'],
			            aggfunc=agg_func_top_bottom_sum)
			  
			  ![Custom agg functions with a pivot table](https://proxy-prod.omnivore-image-cache.app/0x0,shpc3twcqeSeqSk9mTH00nWH5bTE19BX8PGx0h2RvAJo/https://pbpython.com/images/agg_pivot_table.png) 
			  
			  Sometimes you will need to do multiple groupby’s to answer your question. For instance, if we wanted to see a cumulative total of the fares, we can group and aggregate by town and class then group the resulting object and calculate a cumulative sum:
			  
			  fare_group = df.groupby(['embark_town', 'class']).agg({'fare': 'sum'})
			  fare_group.groupby(level=0).cumsum()
			  
			  ![Custom agg functions with cumulative sum](https://proxy-prod.omnivore-image-cache.app/0x0,sqdA-cDoh30T7PWb4DmpQEV7IXf27Y5q9vKgKe0VfIiE/https://pbpython.com/images/agg_cumsum.png) 
			  
			  This may be a little tricky to understand. Here’s a summary of what we are doing:
			  
			  ![Multiple groupby with cumulative sums](https://proxy-prod.omnivore-image-cache.app/0x0,sHPnaJqHRfon-R8aFju-tycaV0g_MIMDS1rPjtbnJJM4/https://pbpython.com/images/multiple-groupby.png) 
			  
			  Here’s another example where we want to summarize daily sales data and convert it to a cumulative daily and quarterly view. Refer to the [Grouper article](https://pbpython.com/pandas-grouper-agg.html) if you are not familiar with using `pd.Grouper()`:
			  
			  In the first example, we want to include a total daily sales as well as cumulative quarter amount:
			  
			  sales = pd.read_excel('https://github.com/chris1610/pbpython/blob/master/data/2018_Sales_Total_v2.xlsx?raw=True')
			  
			  daily_sales = sales.groupby([pd.Grouper(key='date', freq='D')
			                            ]).agg(daily_sales=('ext price',
			                                                'sum')).reset_index()
			  daily_sales['quarter_sales'] = daily_sales.groupby(
			    pd.Grouper(key='date', freq='Q')).agg({'daily_sales': 'cumsum'})
			  
			  To understand this, you need to look at the quarter boundary (end of March through start of April) to get a good sense of what is going on.
			  
			  ![Cumulative total](https://proxy-prod.omnivore-image-cache.app/0x0,sKE64eFJd4LyvJvQmjZJ1gnWfCwnK50cBLKyihT4IrLU/https://pbpython.com/images/cumulative_total.png) 
			  
			  If you want to just get a cumulative quarterly total, you can chain multiple groupby functions.
			  
			  First, group the daily results, then group those results by quarter and use a cumulative sum:
			  
			  sales.groupby([pd.Grouper(key='date', freq='D')
			            ]).agg(daily_sales=('ext price', 'sum')).groupby(
			                pd.Grouper(freq='Q')).agg({
			                    'daily_sales': 'cumsum'
			                }).rename(columns={'daily_sales': 'quarterly_sales'})
			  
			  ![Cumulative quarterly total](https://proxy-prod.omnivore-image-cache.app/0x0,sTrGM_wFpvxstU7E9QjLAWxqe3_8OkyILh26o66xhztA/https://pbpython.com/images/cumulative_quarterly.png) 
			  
			  In this example, I included the named aggregation approach to rename the variable to clarify that it is now daily sales. I then group again and use the cumulative sum to get a running sum for the quarter. Finally, I rename the column to quarterly sales.
			  
			  Admittedly this is a bit tricky to understand. However, if you take it step by step and build out the function and inspect the results at each step, you will start to get the hang of it. Don’t be discouraged!
			  
			  \#\# Flattening Hierarchical Column Indices
			  
			  By default, pandas creates a hierarchical column index on the summary DataFrame. Here is what I am referring to:
			  
			  df.groupby(['embark_town', 'class']).agg({'fare': ['sum', 'mean']}).round(0)
			  
			  ![Hierarchical index](https://proxy-prod.omnivore-image-cache.app/0x0,s1WMUUmxKiGgKi8PHUhrhogFlA3RyLiQHahOYZodfFmc/https://pbpython.com/images/hierarchical_index.png) 
			  
			  At some point in the analysis process you will likely want to “flatten” the columns so that there is a single row of names.
			  
			  I have found that the following approach works best for me. I use the parameter` as_index=False` when grouping, then build a new collapsed column name.
			  
			  Here is the code:
			  
			  multi_df = df.groupby(['embark_town', 'class'],
			                    as_index=False).agg({'fare': ['sum', 'mean']})
			  
			  multi_df.columns = [
			  '_'.join(col).rstrip('_') for col in multi_df.columns.values
			  ]
			  
			  Here is a picture showing what the flattened frame looks like:
			  
			  ![Flatten hierarchical columns](https://proxy-prod.omnivore-image-cache.app/0x0,sCwim9plvCxf9zPH9OH6n3HN72K0bM6Un_4RH2z49Yjk/https://pbpython.com/images/column_flatten.png) 
			  
			  I prefer to use `_` as my separator but you could use other values. Just keep in mind that it will be easier for your subsequent analysis if the resulting column names do not have spaces.
			  
			  \#\# Subtotals
			  
			  One process that is not straightforward with grouping and aggregating in pandas is adding a subtotal. If you want to add subtotals, I recommend the [sidetable](https://github.com/chris1610/sidetable) package. Here is how you can summarize `fares` by `class`, `embark_town` and `sex`with a subtotal at each level as well as a grand total at the bottom:
			  
			  import sidetable
			  df.groupby(['class', 'embark_town', 'sex']).agg({'fare': 'sum'}).stb.subtotal()
			  
			  ![Subtotal](https://proxy-prod.omnivore-image-cache.app/0x0,sA4DXQsCWVpqtsHB8Bd0DHH4SLJlAp9_hZKFqfSpuLEM/https://pbpython.com/images/agg-subtotal.png) 
			  
			  sidetable also allows customization of the subtotal levels and resulting labels. Refer to the package documentation for more examples of how sidetable can summarize your data.
			  
			  \#\# Summary
			  
			  Thanks for reading this article. There is a lot of detail here but that is due to how many different uses there are for grouping and aggregating data with pandas. My hope is that this post becomes a useful resource that you can bookmark and come back to when you get stuck with a challenging problem of your own.
			  
			  If you have other common techniques you use frequently please let me know in the comments. If I get some broadly useful ones, I will include in this post or as an updated article.
			  
			  image credit: [Herman Traub](https://pixabay.com/users/hermann-130146/)
	- How to create stunning visualizations using python from scratch - KDnuggets](https://omnivore.app/me/how-to-create-stunning-visualizations-using-python-from-scratch--18bc48bdbb7)
	  collapsed:: true
	  site:: [KDnuggets](https://www.kdnuggets.com/2021/02/stunning-visualizations-using-python.html)
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- Data science and data analytics can be beautiful things. Not only because of the insights and enhancements to decision-making they can provide, but because of the rich visualizations about the data that can be created. Following this step-by-step guide using the Matplotlib and Seaborn libraries will help you improve the presentation and effective communication of your work.
			  
			  ---
			  
			  
			  **By [Sharan Kumar R](https://twitter.com/rsharankumar), Data Scientist | Author**.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sTOg70HWf94MQrmJ2ZyS3tL616RNQYQ_FBoxJRERA_CY/https://miro.medium.com/max/875/1*mpyrgqwMjfclV2oN1U2VIA.jpeg)
			  
			  _Photo by [Luke Chesser](https://unsplash.com/@lukechesser?utm%5Fsource=unsplash&utm%5Fmedium=referral&utm%5Fcontent=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/dashboard?utm%5Fsource=unsplash&utm%5Fmedium=referral&utm%5Fcontent=creditCopyText)._
			  
			  Visualization is an important skill set for a data scientist. A good visualization can help in clearly communicating insights identified during the analysis, and it is a good technique to better understand the dataset. Our brain is wired in a way that makes it easy for us to extract patterns or trends from visual data as compared to extracting details based on reading or other means.
			  
			  In this article, I will be covering the visualization concept from the basics using Python. Below are the steps to learn visualization from basics,
			  
			  * **Step 1:** Importing data
			  * **Step 2:** Basic visualization using Matplotlib
			  * **Step 3:** More advanced visualizations, still using Matplotlib
			  * **Step 4:** Building quick visualizations for data analysis using Seaborn
			  * **Step 5:** Building interactive charts
			  
			  By the end of this journey, you would be equipped with everything that is required to build a visualization. Though we will be not covering every single visualization that can be built, you will be learning the concepts behind building a chart, and hence it would be easy for you to build any new charts that are not covered in this article.
			  
			  The scripts and the data used in this article can also be found in the git repository [here](https://github.com/rsharankumar/Learn%5FData%5FScience%5Fin%5F100Days). All data used in this article can be found in the “Data” folder within the mentioned git repository, and the scripts are available in the folders ‘Day23, Day 24, and Day25’.
			  
			  \#\#\# Importing Data
			  
			  The first step is to read the required datasets. We can use pandas to read the data. Below is a simple command that can be used to read data from a CSV file,
			  
			  On reading the dataset, it is important to transform it and make it suitable for the visualization we would apply. For example, let’s say we have sales details at the customer level, and if we would want to build a chart that shows the day-wise sales trend, then it is required to group the data and aggregate them at the day level and then use a trend chart.
			  
			  \#\#\# Basic Visualization using Matplotlib
			  
			  Let us start with some basic visualization. It is better to use the code ‘fig,ax=plt.subplots()’, where ‘plt.subplots()’ is a function that will return a tuple with a figure and axes objects assigned to the variables ‘fig’ and ‘ax’ respectively. Without using this as well, you can print a chart, but by using this, you would be able to make changes to figure like you would be able to re-size the charts depending on how it looks and to save the chart as well. Similarly, the ‘ax’ variable here can be used to provide labels to the axes. Below is a simple example where I have passed the data as an array and have print it as a chart directly,
			  
			  In the above code, at first, the required libraries are imported, and then the ‘plt.subplots()’ function is used to generate the figure and the axes objects, and then the data is directly passed as an array to the axes object to print the chart. In the second chart, the axes variable ‘ax’ has taken inputs for labels specific to the x-axis, y-axis, and title.
			  
			  **Trend Charts**
			  
			  Now, let's start using some real data and learn about building interesting charts and about customizing them to make it more intuitive. As explained, in most real-life use cases, the data would require some transformation to make it usable for the charts. Below is an example where I have used the Netflix data but have transformed the data to consolidate the number of movies and Tv shows by year wise. And then, I have used the ‘plt.subplots()’ function, but I have also added few additional details to make the chart more intuitive and self-explanatory.
			  
			  There are a few more customization that can be done to the above chart, like creating a dual-axis. In the above case, there isn’t much difference between the number of movies and TV shows hence the data appears OK, if there has been a huge difference between them then the chart will not be very clear in those cases we can make use of dual-axis so that the attribute with smaller values will also be scaled in line with the other one.
			  
			  **Scatter Plots**
			  
			  We can also make use of the Scatter Plot to bring out any relationship between the variables that we are plotting. This plot helps in bringing the correlation between variables like what happens to one attribute when the other attribute is increasing/decreasing.
			  
			  \#\#\# More advanced visualizations, still using Matplotlib
			  
			  Once you are comfortable with the simple trend-charts we have covered so far, you are ready to move to slightly more advanced charts and functionalities to better customization your visualization
			  
			  **Bar Charts**
			  
			  The Bar Charts help us to compare multiple values at the same time by plotting them side-to-side. There are different kinds of Bar Charts,
			  
			  * Vertical Bar Chart
			  * Horizontal Bar Chart
			  * Stacked Bar Chart
			  
			  Below is an example of a Bar Chart. There are a number of customization added to this plot, including,
			  
			  * Axis labels and title are added
			  * Font size has been provided
			  * Figure size is provided as well (default chart would look much smaller and cluttered)
			  * A function is used to generate and add values to the top of each of the bars to help the viewers get the actual details
			  
			  **Horizontal and Stacked Bar Chart**
			  
			  Vertical Bar charts are most common, but we can also make use of the horizontal bar charts, especially when the data labels have a long name and it is very difficult to print them below a vertical bar. In the case of the stacked bar chart, the bars will be stacked on top of one another within a category. Below is an example of implementing the horizontal and stacked bar charts. The below code also includes customization to the chart color.
			  
			  **Pie and Donut Chart**
			  
			  Pie charts are useful to show the proportion of different categories in the data, and these pie charts can easily be modified to a Donut chart by covering the center part of the pie chart with a circle and re-aligning the text/values to suit the donut chart. Below is a simple example where I have implemented the pie chart and later modified it into a donut chart,
			  
			  **Why is it important to learn Matplotlib?**
			  
			  Matplotlib is a very important visualization library in python because many other visualization libraries in python are dependent on matplotlib. Some of the advantages/benefits of learning matplotib are,
			  
			  * It is easy to learn
			  * It is efficient
			  * It allows a lot of customizations, making it possible to build almost any kind of visual
			  * Libraries like Seaborn are built on top of Matpotlib
			  
			  I have covered only the most essential visualizations in Matplotlib, but the important factor is by practicing these charts, you would have acquired the knowledge for building much more visualization. Matplotlib supports a number of visualization [here](https://matplotlib.org/3.1.0/gallery/index.html) is the link to the gallery of all supported charts.
			  
			  \#\#\# Building quick visualizations for data analysis using Seaborn
			  
			  We have covered a variety of visualization using the Matplotlib library. I am not sure if you have noticed, though matplotlib offers high customization, it involves a lot of coding and hence could be time-consuming, especially when you are working on exploratory analysis and would want to make a few quick plots to understand the data better and make the decisions faster. That’s exactly what is offered by Seaborn library. Here are some benefits of using the seaborn library,
			  
			  * Default themes are still attractive
			  * Simple and quick to build visualizations, especially for data analysis
			  * Its declarative API allows us to just focus on the key elements of the charts
			  
			  There are few downsides, too, like it doesn’t offer much customization, and it could lead to memory issues, especially when we work on large datasets. But still, the benefit outweighs the disadvantages.
			  
			  **Visualizations with just one line code**
			  
			  Below are some simple visualizations that are implemented with just a single code using the seaborn library.
			  
			  As shown in the above snapshot, the visualizations are created with just a single line of code, and they look quite presentable as well. The Seaborn library is widely used in the data analysis phase as we can build charts quickly with ease and with minimum/no effect to make the charts presentable. Visualization is key in the data analysis as they help in bringing out patterns in the data, and the seaborn library fits aptly for the purpose.
			  
			  **Heatmaps**
			  
			  Heatmaps are another interesting visualization that is widely used on time-series data to bring out the seasonalities and other patterns in the dataset. However, to build a heatmap, we need to transform the data into a specific format to support heatmap plotting. Below is a sample code to transform the data to suit the heatmap plot and seaborn library used to build the heatmap,
			  
			  **Pair Plot — my favorite functionality of Seaborn**
			  
			  I consider the pair plot as one of the best features of the seaborn library. It helps in comparison of each attribute in the dataset to every other attribute through visuals and again in a single line of code. Below is a sample code to build pair plots. The use of a pair plot might not be feasible when the dataset we are working on has a large number of columns. In those cases, the pair-plots can be used to analyze the relationship between a specific set of attributes alone.
			  
			  \#\#\# Building interactive charts
			  
			  While working on data science projects, sometimes there would be a requirement to share some visualization with the business teams. Dashboarding tools are widely used for this purpose but let's say there is an interesting pattern that you have noticed while performing data analysis and would like to share with the business user. If they are shared as an image, then there might not be much the business user can do, but if they are shared as an interactive chart, then it gives the business user power to look into the granular details by zooming in or out or use other functionality to interact with the chart. Below is an example where we are creating an HTML file as an output, which includes the visualization that can be shared with any other user and can be simply opened in a web browser.
			  
			  If you are keen to learn about visualizations using Python, then please check out my playlist below. It includes three videos, with a total tutorial length of just over one hour.
			  
			  See [www.youtube.com/watch?list=PLH5lMW7dI2qeI8-85o0eCPDAGANUpUNHm&v=Rdwik2Eh8f0](https://www.youtube.com/watch?list=PLH5lMW7dI2qeI8-85o0eCPDAGANUpUNHm&v=Rdwik2Eh8f0)
			  
			  [Original](https://towardsdatascience.com/how-to-do-visualization-using-python-from-scratch-651304b5ee7a). Reposted with permission.
			  
			  **Bio:** [Sharan Kumar R](https://twitter.com/rsharankumar) is a Data Science professional with over 10 years of experience, and has authored two books in data science, which are available for sale [here](https://www.amazon.com/Sharan-Kumar-Ravindran/e/B015SUYR2S/ref=dp%5Fbyline%5Fcont%5Febooks%5F1). Sharan also hosts a [YouTube channel](https://www.youtube.com/c/DataSciencewithSharan) for teaching and talking about various data science concepts.
			  
			  **Related:**
			  
			  * [These Data Science Skills will be your Superpower](https://www.kdnuggets.com/2020/08/data-science-skills-superpower.html)
			  * [Top 10 Data Visualization Tools for Every Data Scientist](https://www.kdnuggets.com/2020/05/top-10-data-visualization-tools-every-data-scientist.html)
			  * [How to Visualize Data in Python (and R)](https://www.kdnuggets.com/2019/11/visualize-data-python-and-r.html)
	- Part 1. Layout | Dash for Python Documentation | Plotly](https://omnivore.app/me/part-1-layout-dash-for-python-documentation-plotly-18bc48bd974)
	  collapsed:: true
	  site:: [dash.plotly.com](https://dash.plotly.com/layout)
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- [![](https://proxy-prod.omnivore-image-cache.app/0x0,sRW-AO4L4bX7i4PruA5fCZ4jvH6u28BQq-oPkI9_R88E/https://dash.plotly.com/assets/images/plotly_logo_dark.png)](https://plotly.com/)
			  
			  \#\# ![](https://proxy-prod.omnivore-image-cache.app/0x0,sAzAMUKJYNNmCUE42-C54NkF9fKWADfDnvY7ak8li2y0/https://dash.plotly.com/assets/images/language_icons/python_50px.svg) Dash Layout
			  
			  > This is the 1st chapter of the [Dash Fundamentals](https://dash.plotly.com/). The [next chapter](https://dash.plotly.com/basic-callbacks) covers Dash callbacks.
			  
			  This tutorial will walk you through a fundamental aspect of Dash apps, the app `layout`, through six self-contained apps.
			  
			  For production Dash apps, we recommend styling the app `layout` with Dash Enterprise [Design Kit](https://plotly.com/dash/design-kit/?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout).
			  
			  ---
			  
			  Dash apps are composed of two parts. The first part is the "`layout`", which describes what the app looks like. The second part describes the interactivity of the app and will be covered in the [next chapter](https://dash.plotly.com/basic-callbacks).
			  
			  Note: Throughout this documentation, each Python code example can be run either by saving it to an `app.py` file and using `python app.py` or by running it in a Jupyter notebook cell.
			  
			  > If you're using Dash Enterprise's [Data Science Workspaces](https://plotly.com/dash/workspaces/?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout), copy & paste the below code into your Workspace ([see video](https://plotly.com/dash/workspaces/?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout\#screencast)).
			  > 
			  > _[Find out if your company is using Dash Enterprise](https://go.plotly.com/company-lookup?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout)_
			  
			  To get started, create a file named`app.py`, copy the code below into it, and then run it with `python app.py`.
			  
			  ```routeros
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, html, dcc
			  import plotly.express as px
			  import pandas as pd
			  
			  app = Dash(__name__)
			  
			  \# assume you have a "long-form" data frame
			  \# see https://plotly.com/python/px-arguments/ for more options
			  df = pd.DataFrame({
			    "Fruit": ["Apples", "Oranges", "Bananas", "Apples", "Oranges", "Bananas"],
			    "Amount": [4, 1, 2, 2, 4, 5],
			    "City": ["SF", "SF", "SF", "Montreal", "Montreal", "Montreal"]
			  })
			  
			  fig = px.bar(df, x="Fruit", y="Amount", color="City", barmode="group")
			  
			  app.layout = html.Div(children=[
			    html.H1(children='Hello Dash'),
			  
			    html.Div(children='''
			        Dash: A web application framework for your data.
			    '''),
			  
			    dcc.Graph(
			        id='example-graph',
			        figure=fig
			    )
			  ])
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  \#\# Hello Dash
			  
			  Dash: A web application framework for your data.
			  
			  ApplesOrangesBananas012345CitySFMontrealFruitAmount
			  
			  ```vim
			  $ python app.py
			  ...Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)
			  ```
			  
			  Visit <http://127.0.0.1:8050/> in your web browser. You should see an app that looks like the one above.
			  
			  Note:
			  
			  1. The `layout` is composed of a tree of "components" such as`html.Div` and `dcc.Graph`.
			  2. The Dash HTML Components module (`dash.html`)  
			  has a component for every HTML tag. The `html.H1(children='Hello Dash')` component generates a `<h1>Hello Dash</h1>` HTML element in your app.
			  3. Not all components are pure HTML. The Dash Core Components module (`dash.dcc`) contains higher-level components that are interactive and are generated with JavaScript, HTML, and CSS through the React.js library.
			  4. Each component is described entirely through keyword attributes. Dash is _declarative_: you will primarily describe your app through these attributes.
			  5. The `children` property is special. By convention, it's always the first attribute which means that you can omit it: `html.H1(children='Hello Dash')` is the same as `html.H1('Hello Dash')`. It can contain a string, a number, a single component, or a list of components.
			  6. The fonts in your app will look a little bit different than what is displayed here. This app is using a custom CSS stylesheet and Dash Enterprise [Design Kit](https://plotly.com/dash/design-kit/?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout) to modify the default styles of the elements. You can learn more about custom CSS in the[CSS tutorial](https://dash.plotly.com/external-resources).
			  
			  \#\#\# Making Your First Change
			  
			  Dash includes "hot-reloading". This feature is activated by default when you run your app with`app.run(debug=True)`. This means that Dash will automatically refresh your browser when you make a change in your code.
			  
			  Give it a try: change the title "Hello Dash" in your app or change the`x` or the `y` data. Your app should auto-refresh with your change.
			  
			  > Don't like hot-reloading? You can turn this off with`app.run(dev_tools_hot_reload=False)`. Learn more in [Dash Dev Tools documentation](https://dash.plotly.com/devtools)Questions? See the [community forum hot reloading discussion](https://community.plotly.com/t/announcing-hot-reload/14177).
			  
			  **Sign up for Dash Club** → Two free cheat sheets plus updates from Chris Parmer and Adam Schroeder delivered to your inbox every two months. Includes tips and tricks, community apps, and deep dives into the Dash architecture.[Join now](https://go.plotly.com/dash-club?utm%5Fmedium=dash%5Fdocs&utm%5Fcontent=layout).
			  
			  \#\#\# More about HTML Components
			  
			  Dash HTML Components (`dash.html`) contains a component class for every HTML tag as well as keyword arguments for all of the HTML arguments.
			  
			  Let's customize the text in our app by modifying the inline styles of the components. Create a file named `app.py` with the following code:
			  
			  ```routeros
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, dcc, html
			  import plotly.express as px
			  import pandas as pd
			  
			  app = Dash(__name__)
			  
			  colors = {
			    'background': '\#111111',
			    'text': '\#7FDBFF'
			  }
			  
			  \# assume you have a "long-form" data frame
			  \# see https://plotly.com/python/px-arguments/ for more options
			  df = pd.DataFrame({
			    "Fruit": ["Apples", "Oranges", "Bananas", "Apples", "Oranges", "Bananas"],
			    "Amount": [4, 1, 2, 2, 4, 5],
			    "City": ["SF", "SF", "SF", "Montreal", "Montreal", "Montreal"]
			  })
			  
			  fig = px.bar(df, x="Fruit", y="Amount", color="City", barmode="group")
			  
			  fig.update_layout(
			    plot_bgcolor=colors['background'],
			    paper_bgcolor=colors['background'],
			    font_color=colors['text']
			  )
			  
			  app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[
			    html.H1(
			        children='Hello Dash',
			        style={
			            'textAlign': 'center',
			            'color': colors['text']
			        }
			    ),
			  
			    html.Div(children='Dash: A web application framework for your data.', style={
			        'textAlign': 'center',
			        'color': colors['text']
			    }),
			  
			    dcc.Graph(
			        id='example-graph-2',
			        figure=fig
			    )
			  ])
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  \#\# Hello Dash
			  
			  Dash: A web application framework for your data.
			  
			  ApplesOrangesBananas012345CitySFMontrealFruitAmount
			  
			  In this example, we modified the inline styles of the`html.Div`and `html.H1`components with the `style` property.
			  
			  ```stylus
			  html.H1('Hello Dash', style={'textAlign': 'center', 'color': '\#7FDBFF'})
			  ```
			  
			  The above code is rendered in the Dash app as`<h1 style="text-align: center; color: \#7FDBFF">Hello Dash</h1>`.
			  
			  There are a few important differences between the `dash.html`and the HTML attributes:
			  
			  1. The `style` property in HTML is a semicolon-separated string. In Dash, you can just supply a dictionary.
			  2. The keys in the `style` dictionary are [camelCased](https://en.wikipedia.org/wiki/Camel%5Fcase). So, instead of `text-align`, it's `textAlign`.
			  3. The HTML `class` attribute is `className` in Dash.
			  4. The children of the HTML tag is specified through the `children` keyword argument. By convention, this is always the _first_ argument and so it is often omitted.
			  
			  Besides that, all of the available HTML attributes and tags are available to you within your Python context.
			  
			  ---
			  
			  \#\#\# Reusable Components
			  
			  By writing our markup in Python, we can create complex reusable components like tables without switching contexts or languages.
			  
			  Here's a quick example that generates a `Table` from a Pandas dataframe. Create a file named `app.py` with the following code:
			  
			  ```pgsql
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, html
			  import pandas as pd
			  
			  df = pd.read_csv('https://gist.githubusercontent.com/chriddyp/c78bf172206ce24f77d6363a2d754b59/raw/c353e8ef842413cae56ae3920b8fd78468aa4cb2/usa-agricultural-exports-2011.csv')
			  
			  
			  def generate_table(dataframe, max_rows=10):
			    return html.Table([
			        html.Thead(
			            html.Tr([html.Th(col) for col in dataframe.columns])
			        ),
			        html.Tbody([
			            html.Tr([
			                html.Td(dataframe.iloc[i][col]) for col in dataframe.columns
			            ]) for i in range(min(len(dataframe), max_rows))
			        ])
			    ])
			  
			  
			  app = Dash(__name__)
			  
			  app.layout = html.Div([
			    html.H4(children='US Agriculture Exports (2011)'),
			    generate_table(df)
			  ])
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  \#\#\#\# US Agriculture Exports (2011)
			  
			  | Unnamed: 0 | state       | total exports | beef  | pork | poultry | dairy  | fruits fresh | fruits proc | total fruits | veggies fresh | veggies proc | total veggies | corn  | wheat | cotton  |
			  | ---------- | ----------- | ------------- | ----- | ---- | ------- | ------ | ------------ | ----------- | ------------ | ------------- | ------------ | ------------- | ----- | ----- | ------- |
			  | 0          | Alabama     | 1390.63       | 34.4  | 10.6 | 481     | 4.06   | 8            | 17.1        | 25.11        | 5.5           | 8.9          | 14.33         | 34.9  | 70    | 317.61  |
			  | 1          | Alaska      | 13.31         | 0.2   | 0.1  | 0       | 0.19   | 0            | 0           | 0            | 0.6           | 1            | 1.56          | 0     | 0     | 0       |
			  | 2          | Arizona     | 1463.17       | 71.3  | 17.9 | 0       | 105.48 | 19.3         | 41          | 60.27        | 147.5         | 239.4        | 386.91        | 7.3   | 48.7  | 423.95  |
			  | 3          | Arkansas    | 3586.02       | 53.2  | 29.4 | 562.9   | 3.53   | 2.2          | 4.7         | 6.88         | 4.4           | 7.1          | 11.45         | 69.5  | 114.5 | 665.44  |
			  | 4          | California  | 16472.88      | 228.7 | 11.1 | 225.4   | 929.95 | 2791.8       | 5944.6      | 8736.4       | 803.2         | 1303.5       | 2106.79       | 34.6  | 249.3 | 1064.95 |
			  | 5          | Colorado    | 1851.33       | 261.4 | 66   | 14      | 71.94  | 5.7          | 12.2        | 17.99        | 45.1          | 73.2         | 118.27        | 183.2 | 400.5 | 0       |
			  | 6          | Connecticut | 259.62        | 1.1   | 0.1  | 6.9     | 9.49   | 4.2          | 8.9         | 13.1         | 4.3           | 6.9          | 11.16         | 0     | 0     | 0       |
			  | 7          | Delaware    | 282.19        | 0.4   | 0.6  | 114.7   | 2.3    | 0.5          | 1           | 1.53         | 7.6           | 12.4         | 20.03         | 26.9  | 22.9  | 0       |
			  | 8          | Florida     | 3764.09       | 42.6  | 0.9  | 56.9    | 66.31  | 438.2        | 933.1       | 1371.36      | 171.9         | 279          | 450.86        | 3.5   | 1.8   | 78.24   |
			  | 9          | Georgia     | 2860.84       | 31    | 18.9 | 630.4   | 38.38  | 74.6         | 158.9       | 233.51       | 59            | 95.8         | 154.77        | 57.8  | 65.4  | 1154.07 |
			  
			  \#\#\# More about Visualization
			  
			  The Dash Core Components module (`dash.dcc`) includes a component called`Graph`.
			  
			  `Graph` renders interactive data visualizations using the open source[plotly.js](https://github.com/plotly/plotly.js) JavaScript graphing library. Plotly.js supports over 35 chart types and renders charts in both vector-quality SVG and high-performance WebGL.
			  
			  The `figure` argument in the`Graph` component is the same `figure` argument that is used by `plotly.py`, Plotly's open source Python graphing library. Check out the [plotly.py documentation and gallery](https://plotly.com/python)to learn more.
			  
			  Here's an example that creates a scatter plot from a Pandas dataframe. Create a file named `app.py` with the following code:
			  
			  ```routeros
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, dcc, html
			  import plotly.express as px
			  import pandas as pd
			  
			  
			  app = Dash(__name__)
			  
			  df = pd.read_csv('https://gist.githubusercontent.com/chriddyp/5d1ea79569ed194d432e56108a04d188/raw/a9f9e8076b837d541398e999dcbac2b2826a81f8/gdp-life-exp-2007.csv')
			  
			  fig = px.scatter(df, x="gdp per capita", y="life expectancy",
			                 size="population", color="continent", hover_name="country",
			                 log_x=True, size_max=60)
			  
			  app.layout = html.Div([
			    dcc.Graph(
			        id='life-exp-vs-gdp',
			        figure=fig
			    )
			  ])
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  continentAsiaEuropeAfricaAmericasOceaniagdp per capitalife expectancy
			  
			  _These graphs are interactive and responsive.**Hover** over points to see their values,**click** on legend items to toggle traces,**click and drag** to zoom,**hold down shift, and click and drag** to pan._
			  
			  \#\#\# Markdown
			  
			  While Dash exposes HTML through Dash HTML Components (`dash.html`), it can be tedious to write your copy in HTML. For writing blocks of text, you can use the`Markdown` component in Dash Core Components (`dash.dcc`). Create a file named `app.py` with the following code:
			  
			  ```markdown
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, html, dcc
			  
			  app = Dash(__name__)
			  
			  markdown_text = '''
			  \#\#\# Dash and Markdown
			  
			  Dash apps can be written in Markdown.
			  Dash uses the [CommonMark](http://commonmark.org/)
			  specification of Markdown.
			  Check out their [60 Second Markdown Tutorial](http://commonmark.org/help/)
			  if this is your first introduction to Markdown!
			  '''
			  
			  app.layout = html.Div([
			    dcc.Markdown(children=markdown_text)
			  ])
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  \#\#\# Dash and Markdown
			  
			  Dash apps can be written in Markdown. Dash uses the [CommonMark](http://commonmark.org/)specification of Markdown. Check out their [60 Second Markdown Tutorial](http://commonmark.org/help/)if this is your first introduction to Markdown!
			  
			  \#\#\# Core Components
			  
			  Dash Core Components (`dash.dcc`) includes a set of higher-level components like dropdowns, graphs, markdown blocks, and more.
			  
			  Like all Dash components, they are described entirely declaratively. Every option that is configurable is available as a keyword argument of the component.
			  
			  We'll see many of these components throughout the tutorial. You can view all of the available components in the[Dash Core Components overview](https://dash.plotly.com/dash-core-components).
			  
			  Here are a few of the available components. Create a file named `app.py` with the following code:
			  
			  ```routeros
			  \# Run this app with `python app.py` and
			  \# visit http://127.0.0.1:8050/ in your web browser.
			  
			  
			  from dash import Dash, html, dcc
			  
			  app = Dash(__name__)
			  
			  app.layout = html.Div([
			    html.Div(children=[
			        html.Label('Dropdown'),
			        dcc.Dropdown(['New York City', 'Montréal', 'San Francisco'], 'Montréal'),
			  
			        html.Br(),
			        html.Label('Multi-Select Dropdown'),
			        dcc.Dropdown(['New York City', 'Montréal', 'San Francisco'],
			                     ['Montréal', 'San Francisco'],
			                     multi=True),
			  
			        html.Br(),
			        html.Label('Radio Items'),
			        dcc.RadioItems(['New York City', 'Montréal', 'San Francisco'], 'Montréal'),
			    ], style={'padding': 10, 'flex': 1}),
			  
			    html.Div(children=[
			        html.Label('Checkboxes'),
			        dcc.Checklist(['New York City', 'Montréal', 'San Francisco'],
			                      ['Montréal', 'San Francisco']
			        ),
			  
			        html.Br(),
			        html.Label('Text Input'),
			        dcc.Input(value='MTL', type='text'),
			  
			        html.Br(),
			        html.Label('Slider'),
			        dcc.Slider(
			            min=0,
			            max=9,
			            marks={i: f'Label {i}' if i == 1 else str(i) for i in range(1, 6)},
			            value=5,
			        ),
			    ], style={'padding': 10, 'flex': 1})
			  ], style={'display': 'flex', 'flexDirection': 'row'})
			  
			  if __name__ == '__main__':
			    app.run(debug=True)
			  ```
			  
			  Dropdown
			  
			  Multi-Select Dropdown
			  
			  Radio Items
			  
			  New York CityMontréalSan Francisco
			  
			  Checkboxes
			  
			  New York CityMontréalSan Francisco
			  
			  Text Input  
			  Slider
			  
			  \#\#\# Help
			  
			  Dash components are declarative: every configurable aspect of these components is set during instantiation as a keyword argument.
			  
			  Call `help` in your Python console on any of the components to learn more about a component and its available arguments.
			  
			  ```livecodeserver
			  >>> help(dcc.Dropdown)
			  class Dropdown(dash.development.base_component.Component)
			  |  A Dropdown component.
			  |  Dropdown is an interactive dropdown element for selecting one or more
			  |  items.
			  |  The values and labels of the dropdown items are specified in the `options`
			  |  property and the selected item(s) are specified with the `value` property.
			  |
			  |  Use a dropdown when you have many options (more than 5) or when you are
			  |  constrained for space. Otherwise, you can use RadioItems or a Checklist,
			  |  which have the benefit of showing the users all of the items at once.
			  |
			  |  Keyword arguments:
			  |  - id (string; optional)
			  |  - className (string; optional)
			  |  - disabled (boolean; optional): If true, the option is disabled
			  |  - multi (boolean; optional): If true, the user can select multiple values
			  |  - options (list; optional)
			  |  - placeholder (string; optional): The grey, default text shown when no option is selected
			  |  - value (string | list; optional): The value of the input. If `multi` is false (the default)
			  |  then value is just a string that corresponds to the values
			  |  provided in the `options` property. If `multi` is true, then
			  |  multiple values can be selected at once, and `value` is an
			  |  array of items with values corresponding to those in the
			  |  `options` prop.
			  ```
			  
			  \#\#\# Summary
			  
			  The `layout` of a Dash app describes what the app looks like. The `layout` is a hierarchical tree of components.
			  
			  Dash HTML Components (`dash.html`) provides classes for all of the HTML tags and the keyword arguments describe the HTML attributes like `style`, `class`, and `id`. Dash Core Components (`dash.dcc`) generates higher-level components like controls and graphs.
			  
			  For reference, see:
			  
			  * [Dash Core Components overview](https://dash.plotly.com/dash-core-components)
			  * [Dash HTML Components overview](https://dash.plotly.com/dash-html-components)
			  
			  The next part of the Dash Fundamentals covers how to make these apps interactive.[Dash Fundamentals Part 2: Basic Callbacks](https://dash.plotly.com/basic-callbacks)
	- The 4 ways of doing Machine Learning | Non-technical guide](https://omnivore.app/me/the-4-ways-of-doing-machine-learning-non-technical-guide-18bc48bd5d5)
	  collapsed:: true
	  site:: [datarevenue.com](https://www.datarevenue.com/en-blog/the-4-ways-of-doing-machine-learning)
	  author:: byMarkus SchmittMarkus Schmitt
	  date-saved:: [[Nov 12th, 2023]]
		- ### Content
			- What’s the right way for your team to approach Machine Learning?
			  
			  [](https://assets-global.website-files.com/5d3ec351b1eba4332d213004/5fa3ac978235fd892c74429a%5Fguide-to-ml.png)![](https://proxy-prod.omnivore-image-cache.app/0x0,srni78CvIqX5KR4bvE7ebVV46i_uifx0vEwFS6Ebg-H0/https://assets-global.website-files.com/5d3ec351b1eba4332d213004/5fa3ac978235fd892c74429a_guide-to-ml.png)![](https://proxy-prod.omnivore-image-cache.app/0x0,s36JrUVxMLcvf_AzV4AsaURhHG6ezNTek4WS-Jc1IbPA/https://global-uploads.webflow.com/5d3af5ca4e11726adbd659b2/5d3b06736e3cf0299a0dde71_shadow-pattern.svg)
			  
			  Machine learning can be confusing. Everyone uses the term to mean something slightly different, and there’s just so much to keep up with.
			  
			  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s5t-ptBcxDHZD0_DlnH5n9qfrZT52C0QnHT0CxfUtfvY/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5fa3acc48235fd1a2f7442ab_lNjFZyxUX44UoXNidSlVNWmxFtny3Jw9JuXyhDZ31VnrjaCdT_AqKAT6qM4T67yoVNFgKcjPZ-tlydxXY9z_qZ8X9ejy4kww8NXhWBHZ_WZ77LufsZXpPUsKhmb8_pfX8qE0Llan.png)
			  
			  (Source <https://mattturck.com/data2019/>)
			  
			  Before you choose a vendor or a platform to meet your AI needs, it’s important to understand which “layer” of machine learning is best for you. You can build your own AI from scratch, use an off-the-shelf product, or something in between. 
			  
			  In this post, we’ll walk you through the four options and help you identify which is best for you by comparing them to means of transportation.
			  
			  \#\# **The different “layers” of machine learning**
			  
			  ![Two graphs showing APIs, managed platforms, open source frameworks, and research corresponding to public transport, taxis, owning a car, and building a car.](https://proxy-prod.omnivore-image-cache.app/0x0,sURlzp-B_Nfte4P73On4AF6Y-inFqYupVc8DzHkmCB7g/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5fa3acc42a9fd5ab5d228c94_GW2oZ6qU3gLhqRz_bT7mUXeoKXzHcdEtVWDrJy2Zhlo5iSIrVOJ8wUwKxsdStRVYMER-ZsL92q5LePNUQMeEEZ6qSAM5BrW9RdLf0nk2ljk0QDEgsDbzXUzJauwXqFiGooYeXYwq.png)
			  
			  Like different means of transportation, you can use machine learning in different ways.
			  
			  * **Novel research** such as developing new speech-to-text algorithms is like **building a car.**
			  * **Open source frameworks** such as Tensorflow are similar to **owning a car.**
			  * **Managed platforms** such as AWS SageMaker are like **taking a taxi**.
			  * **APIs** such as Google Translate are similar to **taking a flight.**
			  
			  You can build a machine learning solution at different layers. As you go to more specialised layers, more features work straight out of the box, but you get less flexibility.
			  
			  Each layer is built from the previous one: APIs are often built on managed platforms, which are built on open-source frameworks, which are built on research.
			  
			  \#\# **Different problems are best solved by different layers**
			  
			  Which layer is best for you will depend mainly on the problem you’re trying to solve. 
			  
			  Not all the layers are the same size though: for example, it’s actually surprisingly rare for a problem to be cleanly solved by a managed platform.
			  
			  ![A sideways bar graph showing question answering and speech to text in research, automated drug discovery, ML diagnostics, and algorithmic trading in frameworks, churn prediction, demand forecasting, recommender system in platforms, and translation and navigation in APIs.](https://proxy-prod.omnivore-image-cache.app/0x0,stgVvcTlfz41b5k6t3Mqzof5rXuy1IDhLEPqJ9ZILpx0/https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5fa3acc57a7094d6e2705ea3_cDEB9D_rBoddE6hb0zZsZPmy0TrtRysePylDnh_hKLhl537Iy-KSykswJjdkl129vjm-IO9eRXsW3rzKj1eHCrnA4AhN6ZkENrS0JJi9Rh5S8_MWxkhTRsWdUh7_wqbxDEvK8yBs.png)
			  
			  Different problems fit better into different layers. The “managed platform” layer is smaller than many people think.
			  
			  Because the companies behind managed machine learning platforms use aggressive marketing tactics, people often overestimate the value of these platforms. In reality, a managed platform often leaves you with the worst aspects of using off-the-shelf APIs while not fully resolving the pain points that you have to accept when you build a bespoke solution with open source frameworks.
			  
			  Let’s take a look at each layer in depth.
			  
			  \#\# **Doing AI research is like building your own car**
			  
			  If you want to use AI, you might be tempted to hire a team of PhDs. But this would be like hiring a team of mechanical engineers and asking them to build a car for you and then take care of it 24/7.
			  
			  If you’re a racecar driver competing on an international level, this might be reasonable – but otherwise it’s probably not. Similarly, companies like Google have teams of researchers who push AI’s foundations forward by developing new algorithms and architectures.
			  
			  This work is expensive, and it often takes years or decades to create real value. Most companies today don’t need to even consider taking this approach. Unless you’re pushing the leading 1% edge of performance in your field, it’s unlikely that it makes sense to have a dedicated research team.
			  
			  **You might want to do your own AI research if:**
			  
			  * You’re creating a competitor to Siri and you want it to be better than Siri at speech-to-text interpretation.
			  * You’re in a head-to-head race with other investment banks and want to give your trading algorithms an edge.
			  
			  **Sample team structure:**
			  
			  * A team of research engineers: PhDs, postdocs, and professors.
			  * A team of data, machine learning and devops engineers who can translate research results into machine learning solutions.
			  
			  \#\# **Using open source frameworks is like owning your own car**
			  
			  As academic research matures, it evolves into open source frameworks like Tensorflow, PyTorch, and Keras. Thousands of developers around the world then contribute to improving these frameworks for free.
			  
			  Anybody can just download these open source frameworks for free and use them. These are the exact same powerful “engines” that expensive managed platforms like those from Amazon, Google and Microsoft use. But you have the flexibility to build your system the way you want to.
			  
			  But like owning your own car, this comes with some downsides too. If you own a car, you need to learn how to drive or hire a driver. To use a tool like Tensorflow, you need to learn to code or hire a developer. 
			  
			  If you own a car, you also need to worry about things like refueling, replacing tires, and finding a safe garage. If you build your own custom machine learning solutions, you’ll also need to set up a fitting infrastructure and maintain it.
			  
			  **You might want to develop your machine learning solutions with open-source frameworks if:**
			  
			  * You want to improve a workflow that’s very specific to your business.
			  * You have the data and domain knowledge on the problem in-house.
			  * You’re not afraid of taking on complicated software projects.
			  
			  **Sample team structure:**
			  
			  * **Internal:** Teams of DevOps engineers, machine learning engineers, data scientists, project managers, and subject matter experts.
			  
			  **OR** 
			  
			  * **External:** Agencies like [us](https://datarevenue.com/), who build your custom ML solution using open source frameworks.
			  
			  \#\# **Using managed platforms is like taking a taxi**
			  
			  Sometimes you need something quickly, and you’re willing to pay more for it. If you need to make a quick one-off trip, you might take a taxi. But if you take a long-distance taxi every day, it’s likely you’d get better value by buying a car.
			  
			  Similarly, [managed platforms like **AWS Sagemaker, Dataiku, Knime, and Alteryx**](https://datarevenue.com/en-blog/ml-platforms-dataiku-vs-alteryx-vs-sagemaker)allow you to get started more quickly and provide some pre-built components to help you get started faster.
			  
			  But while these platforms can save you time, they’re often far more limited than their marketing teams would like to admit. Plus if you get “locked in” to one of them, it can be expensive. As with a taxi, if you need it all the time, then it’s probably the wrong solution, and you’re likely to hit its limits sooner than you expect.
			  
			  **You might want to use a managed platform if:**
			  
			  * Machine learning isn’t going to be an important part of your business or a source of competitive advantage.
			  * You want to apply machine learning to standard problems that are already well understood and have been solved elsewhere (e.g., recommendation systems, churn analysis, demand forecasting).
			  
			  **Sample team structure:**
			  
			  * At least one machine learning engineer, one developer, and one DevOps specialist.
			  
			  \#\# **Using APIs is like taking a flight**
			  
			  If you just need to get from London to New York regularly, then public transport will probably fit your needs fine. This is the least flexible form of transport, but it does one thing and does it well.
			  
			  Similarly, APIs and web applications like Google Translate are the easiest to plug into your existing infrastructure. They also scale up as much as you need them to and provide a level of quality that’s hard to beat.
			  
			  But they’re also the least flexible. If you have any custom needs, then APIs probably won’t be flexible enough for you. Plus you’ll pay a premium: with a translation API, for example, you’ll probably pay per character translated.
			  
			  **You might want to use an API if:**
			  
			  * You have an e-commerce platform and you want users to see automatic translations of reviews left in other languages.
			  * You run a logistics company and you want automatic route optimization for deliveries.
			  
			  **Sample team structure:**
			  
			  * A single developer to integrate the API into your system.
			  
			  \#\# **Next steps**
			  
			  Once you’ve chosen which “layer” makes the most sense for you, then you can start getting into the details of what [orchestration platform](https://www.datarevenue.com/en-blog/airflow-vs-luigi-vs-argo-vs-mlflow-vs-kubeflow), [tools](https://www.datarevenue.com/machine-learning-software-tools), or [managed platform](https://www.datarevenue.com/en-blog/ml-platforms-dataiku-vs-alteryx-vs-sagemaker) to use.
			  
			  If you’re still unsure, book a [free call](https://www.datarevenue.com/en-contact) with us to discuss what might be right for you!
			  
			  \#\#\# Get Notified of New Articles
			  
			  Leave your email to get our weekly newsletter.
			  
			  Thank you! Your submission has been received!
			  
			  Oops! Something went wrong while submitting the form.
			  
			  \#\# Keep reading
	- 20 productivity tools for the Linux terminal | Opensource.com](https://omnivore.app/me/20-productivity-tools-for-the-linux-terminal-opensource-com-18bc48d1c57)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/20/6/productivity-tools-linux-terminal)
	  author:: Alan Smithee
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[May 28th, 2020]]
	- Z Test Statistics Formula & Python Implementation | by Angel Das | Towards Data Science](https://omnivore.app/me/z-test-statistics-formula-python-implementation-by-angel-das-tow-18bc48d428f)
	  collapsed:: true
	  site:: [Towards Data Science](https://towardsdatascience.com/z-test-statistics-formula-python-implementation-3755d67ba0e7)
	  author:: Angel Das
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Nov 8th, 2022]]
	- ChatGPT API Python Guide - Pierian Training](https://omnivore.app/me/chat-gpt-api-python-guide-pierian-training-18bc48d426a)
	  collapsed:: true
	  site:: [Pierian Training](https://pieriantraining.com/chatgpt-api-python-guide/)
	  author:: Pierian Training
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[May 29th, 2023]]
	- AI Stock Trading Platform | Automated Trading | Pluto](https://omnivore.app/me/ai-stock-trading-platform-automated-trading-pluto-18bc48d3ce4)
	  collapsed:: true
	  site:: [Pluto](https://www.pluto.fi)
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	- Creating a date dimension table in PostgreSQL | by duffn | Medium](https://omnivore.app/me/creating-a-date-dimension-table-in-postgre-sql-by-duffn-medium-18bc48d28f7)
	  collapsed:: true
	  site:: [Medium](https://duffn.medium.com/creating-a-date-dimension-table-in-postgresql-af3f8e2941ac)
	  author:: duffn
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 22nd, 2016]]
	- Exploring an Alternative to Jupyter Notebooks for Python Development - Practical Business Python](https://omnivore.app/me/exploring-an-alternative-to-jupyter-notebooks-for-python-develop-18bc48d241a)
	  collapsed:: true
	  site:: [pbpython.com](https://pbpython.com/notebook-alternative.html)
	  author:: Chris Moffitt
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[May 3rd, 2020]]
	- Useful Queries For PostgreSQL Index Maintenance](https://omnivore.app/me/useful-queries-for-postgre-sql-index-maintenance-18bc48d222e)
	  collapsed:: true
	  site:: [Percona Database Performance Blog](https://www.percona.com/blog/2020/03/31/useful-queries-for-postgresql-index-maintenance/)
	  author:: Ibrar Ahmed
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Mar 31st, 2020]]
	- Refactoring With Tests in Python: a Practical Example | HackerNoon](https://omnivore.app/me/refactoring-with-tests-in-python-a-practical-example-hacker-noon-18bc48d1f12)
	  collapsed:: true
	  site:: [hackernoon.com](https://hackernoon.com/refactoring-with-tests-in-python-a-practical-example-x3oj24jt)
	  author:: The Digital Cat
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 10th, 2020]]
	- 4 Markdown tools for the Linux command line | Opensource.com](https://omnivore.app/me/4-markdown-tools-for-the-linux-command-line-opensource-com-18bc48d0481)
	  collapsed:: true
	  site:: [Opensource.com](https://opensource.com/article/20/3/markdown-apps-linux-command-line)
	  author:: Scott Nesbitt
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Mar 18th, 2020]]
	- Install Synaptic Package Manager on you Chromebook](https://omnivore.app/me/install-synaptic-package-manager-on-you-chromebook-18bc48d03c3)
	  collapsed:: true
	  site:: [Chrome Unboxed - The Latest Chrome OS News](https://chromeunboxed.com/install-synaptic-package-manager-on-you-chromebook/)
	  author:: Gabriel Brangers
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Feb 29th, 2020]]
	- Techniques to Write Better Python Code - MachineLearningMastery.com](https://omnivore.app/me/techniques-to-write-better-python-code-machine-learning-mastery--18bc48d45ba)
	  collapsed:: true
	  site:: [MachineLearningMastery.com](https://machinelearningmastery.com/techniques-to-write-better-python-code/)
	  author:: Adrian Tam
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 29th, 2022]]
	- Exploratory Data Analysis: The Ultimate Workflow | by Arthur Mello | Level Up Coding](https://omnivore.app/me/exploratory-data-analysis-the-ultimate-workflow-by-arthur-mello--18bc48d4394)
	  collapsed:: true
	  site:: [Level Up Coding](https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747)
	  author:: Arthur Mello
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 20th, 2023]]
	- Prioritizing simplicity in your Python code | Opensource.com](https://omnivore.app/me/prioritizing-simplicity-in-your-python-code-opensource-com-18bc48d0328)
	  site:: [Opensource.com](https://opensource.com/article/19/12/zen-python-simplicity-complexity)
	  author:: Moshe Zadka
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Dec 22nd, 2019]]
	  collapsed:: true
	- Understanding Cross Validation in Scikit-Learn with cross_validate - Pierian Training](https://omnivore.app/me/understanding-cross-validation-in-scikit-learn-with-cross-valida-18bc48d47a0)
	  collapsed:: true
	  site:: [Pierian Training](https://pieriantraining.com/understanding-cross-validation-in-scikit-learn-with-cross_validate/)
	  author:: Pierian Training
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	  date-published:: [[Apr 12th, 2023]]
	- 🎢 Introduction to Exploratory Data Analysis | Kaggle](https://omnivore.app/me/introduction-to-exploratory-data-analysis-kaggle-18bc48d47c7)
	  collapsed:: true
	  site:: [kaggle.com](https://www.kaggle.com/code/robikscube/introduction-to-exploratory-data-analysis/notebook)
	  labels:: [[sent to logseq]] [[eda]]
	  date-saved:: [[Nov 12th, 2023]]
	- Powerful Exploratory Data Analysis in just two lines of code - KDnuggets](https://omnivore.app/me/powerful-exploratory-data-analysis-in-just-two-lines-of-code-k-d-18bc48bedbf)
	  site:: [KDnuggets](https://www.kdnuggets.com/2021/02/powerful-exploratory-data-analysis-sweetviz.html)
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 12th, 2023]]
	- Organize your #Omnivore library with labels](https://omnivore.app/me/organize-your-omnivore-library-with-labels)
	  collapsed:: true
	  site:: [Omnivore Blog](https://blog.omnivore.app/p/organize-your-omnivore-library-with)
	  author:: The #Omnivore Team
	  labels:: [[recipes]] [[sent to logseq]]
	  date-saved:: [[Nov 11th, 2023]]
	  date-published:: [[Apr 17th, 2022]]
	- Getting Started with Omnivore](https://omnivore.app/me/getting-started-with-omnivore)
	  collapsed:: true
	  site:: [Omnivore Blog](https://blog.omnivore.app/p/getting-started-with-omnivore)
	  author:: The #Omnivore Team
	  labels:: [[sent to logseq]]
	  date-saved:: [[Nov 11th, 2023]]
	  date-published:: [[Oct 12th, 2021]]